[
  {
    "id": "2006.10270",
    "title": "Multi-branch Attentive Transformer",
    "link": "http://arxiv.org/abs/2006.10270v2",
    "pdf": "http://arxiv.org/pdf/2006.10270v2",
    "summary": "While the multi-branch architecture is one of the key ingredients to the\nsuccess of computer vision tasks, it has not been well investigated in natural\nlanguage processing, especially sequence learning tasks. In this work, we\npropose a simple yet effective variant of Transformer called multi-branch\nattentive Transformer (briefly, MAT), where the attention layer is the average\nof multiple branches and each branch is an independent multi-head attention\nlayer. We leverage two training techniques to regularize the training:\ndrop-branch, which randomly drops individual branches during training, and\nproximal initialization, which uses a pre-trained Transformer model to\ninitialize multiple branches. Experiments on machine translation, code\ngeneration and natural language understanding demonstrate that such a simple\nvariant of Transformer brings significant improvements. Our code is available\nat \\url{https://github.com/HA-Transformer}.",
    "authors": [
      "Yang Fan",
      "Shufang Xie",
      "Yingce Xia",
      "Lijun Wu",
      "Tao Qin",
      "Xiang-Yang Li",
      "Tie-Yan Liu"
    ],
    "updated": "2020-07-26T13:04:49Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1807.11605",
    "title": "Doubly Attentive Transformer Machine Translation",
    "link": "http://arxiv.org/abs/1807.11605v1",
    "pdf": "http://arxiv.org/pdf/1807.11605v1",
    "summary": "In this paper a doubly attentive transformer machine translation model\n(DATNMT) is presented in which a doubly-attentive transformer decoder normally\njoins spatial visual features obtained via pretrained convolutional neural\nnetworks, conquering any gap between image captioning and translation. In this\nframework, the transformer decoder figures out how to take care of\nsource-language words and parts of an image freely by methods for two separate\nattention components in an Enhanced Multi-Head Attention Layer of doubly\nattentive transformer, as it generates words in the target language. We find\nthat the proposed model can effectively exploit not just the scarce multimodal\nmachine translation data, but also large general-domain text-only machine\ntranslation corpora, or image-text image captioning corpora. The experimental\nresults show that the proposed doubly-attentive transformer-decoder performs\nbetter than a single-decoder transformer model, and gives the state-of-the-art\nresults in the English-German multimodal machine translation task.",
    "authors": [
      "Hasan Sait Arslan",
      "Mark Fishel",
      "Gholamreza Anbarjafari"
    ],
    "updated": "2018-07-30T23:13:55Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2405.04515",
    "title": "A Transformer with Stack Attention",
    "link": "http://arxiv.org/abs/2405.04515v2",
    "pdf": "http://arxiv.org/pdf/2405.04515v2",
    "summary": "Natural languages are believed to be (mildly) context-sensitive. Despite\nunderpinning remarkably capable large language models, transformers are unable\nto model many context-free language tasks. In an attempt to address this\nlimitation in the modeling power of transformer-based language models, we\npropose augmenting them with a differentiable, stack-based attention mechanism.\nOur stack-based attention mechanism can be incorporated into any\ntransformer-based language model and adds a level of interpretability to the\nmodel. We show that the addition of our stack-based attention mechanism enables\nthe transformer to model some, but not all, deterministic context-free\nlanguages.",
    "authors": [
      "Jiaoda Li",
      "Jennifer C. White",
      "Mrinmaya Sachan",
      "Ryan Cotterell"
    ],
    "updated": "2024-05-13T18:56:18Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2109.12188",
    "title": "Predicting Attention Sparsity in Transformers",
    "link": "http://arxiv.org/abs/2109.12188v2",
    "pdf": "http://arxiv.org/pdf/2109.12188v2",
    "summary": "Transformers' quadratic complexity with respect to the input sequence length\nhas motivated a body of work on efficient sparse approximations to softmax. An\nalternative path, used by entmax transformers, consists of having built-in\nexact sparse attention; however this approach still requires quadratic\ncomputation. In this paper, we propose Sparsefinder, a simple model trained to\nidentify the sparsity pattern of entmax attention before computing it. We\nexperiment with three variants of our method, based on distances, quantization,\nand clustering, on two tasks: machine translation (attention in the decoder)\nand masked language modeling (encoder-only). Our work provides a new angle to\nstudy model efficiency by doing extensive analysis of the tradeoff between the\nsparsity and recall of the predicted attention graph. This allows for detailed\ncomparison between different models along their Pareto curves, important to\nguide future benchmarks for sparse attention models.",
    "authors": [
      "Marcos Treviso",
      "António Góis",
      "Patrick Fernandes",
      "Erick Fonseca",
      "André F. T. Martins"
    ],
    "updated": "2022-04-21T07:17:11Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2306.13501",
    "title": "Knowledge-Infused Self Attention Transformers",
    "link": "http://arxiv.org/abs/2306.13501v1",
    "pdf": "http://arxiv.org/pdf/2306.13501v1",
    "summary": "Transformer-based language models have achieved impressive success in various\nnatural language processing tasks due to their ability to capture complex\ndependencies and contextual information using self-attention mechanisms.\nHowever, they are not without limitations. These limitations include\nhallucinations, where they produce incorrect outputs with high confidence, and\nalignment issues, where they generate unhelpful and unsafe outputs for human\nusers. These limitations stem from the absence of implicit and missing context\nin the data alone. To address this, researchers have explored augmenting these\nmodels with external knowledge from knowledge graphs to provide the necessary\nadditional context. However, the ad-hoc nature of existing methods makes it\ndifficult to properly analyze the effects of knowledge infusion on the many\nmoving parts or components of a transformer. This paper introduces a systematic\nmethod for infusing knowledge into different components of a transformer-based\nmodel. A modular framework is proposed to identify specific components within\nthe transformer architecture, such as the self-attention mechanism, encoder\nlayers, or the input embedding layer, where knowledge infusion can be applied.\nAdditionally, extensive experiments are conducted on the General Language\nUnderstanding Evaluation (GLUE) benchmark tasks, and the findings are reported.\nThis systematic approach aims to facilitate more principled approaches to\nincorporating knowledge into language model architectures.",
    "authors": [
      "Kaushik Roy",
      "Yuxin Zi",
      "Vignesh Narayanan",
      "Manas Gaur",
      "Amit Sheth"
    ],
    "updated": "2023-06-23T13:55:01Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2004.11886",
    "title": "Lite Transformer with Long-Short Range Attention",
    "link": "http://arxiv.org/abs/2004.11886v1",
    "pdf": "http://arxiv.org/pdf/2004.11886v1",
    "summary": "Transformer has become ubiquitous in natural language processing (e.g.,\nmachine translation, question answering); however, it requires enormous amount\nof computations to achieve high performance, which makes it not suitable for\nmobile applications that are tightly constrained by the hardware resources and\nbattery. In this paper, we present an efficient mobile NLP architecture, Lite\nTransformer to facilitate deploying mobile NLP applications on edge devices.\nThe key primitive is the Long-Short Range Attention (LSRA), where one group of\nheads specializes in the local context modeling (by convolution) while another\ngroup specializes in the long-distance relationship modeling (by attention).\nSuch specialization brings consistent improvement over the vanilla transformer\non three well-established language tasks: machine translation, abstractive\nsummarization, and language modeling. Under constrained resources (500M/100M\nMACs), Lite Transformer outperforms transformer on WMT'14 English-French by\n1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of\ntransformer base model by 2.5x with 0.3 BLEU score degradation. Combining with\npruning and quantization, we further compressed the model size of Lite\nTransformer by 18.2x. For language modeling, Lite Transformer achieves 1.8\nlower perplexity than the transformer at around 500M MACs. Notably, Lite\nTransformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU\nfor the mobile NLP setting without the costly architecture search that requires\nmore than 250 GPU years. Code has been made available at\nhttps://github.com/mit-han-lab/lite-transformer.",
    "authors": [
      "Zhanghao Wu",
      "Zhijian Liu",
      "Ji Lin",
      "Yujun Lin",
      "Song Han"
    ],
    "updated": "2020-04-24T17:52:25Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2410.02703",
    "title": "Selective Attention Improves Transformer",
    "link": "http://arxiv.org/abs/2410.02703v2",
    "pdf": "http://arxiv.org/pdf/2410.02703v2",
    "summary": "Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention consistently improves language modeling and downstream task\nperformance in a variety of model sizes and context lengths. For example,\ntransformers trained with the language modeling objective on C4 with selective\nattention perform language modeling equivalently to standard transformers with\n~2X more heads and parameters in their attention modules. Selective attention\nalso allows decreasing the size of the attention's context buffer, leading to\nmeaningful reductions in the memory and compute requirements during inference.\nFor example, transformers trained on C4 with context sizes of 512, 1,024, and\n2,048 need 16X, 25X, and 47X less memory for their attention module,\nrespectively, when equipped with selective attention, as those without\nselective attention, with the same validation perplexity.",
    "authors": [
      "Yaniv Leviathan",
      "Matan Kalman",
      "Yossi Matias"
    ],
    "updated": "2025-04-24T02:44:54Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id": "2103.13597",
    "title": "Mask Attention Networks: Rethinking and Strengthen Transformer",
    "link": "http://arxiv.org/abs/2103.13597v1",
    "pdf": "http://arxiv.org/pdf/2103.13597v1",
    "summary": "Transformer is an attention-based neural network, which consists of two\nsublayers, namely, Self-Attention Network (SAN) and Feed-Forward Network (FFN).\nExisting research explores to enhance the two sublayers separately to improve\nthe capability of Transformer for text representation. In this paper, we\npresent a novel understanding of SAN and FFN as Mask Attention Networks (MANs)\nand show that they are two special cases of MANs with static mask matrices.\nHowever, their static mask matrices limit the capability for localness modeling\nin text representation learning. We therefore introduce a new layer named\ndynamic mask attention network (DMAN) with a learnable mask matrix which is\nable to model localness adaptively. To incorporate advantages of DMAN, SAN, and\nFFN, we propose a sequential layered structure to combine the three types of\nlayers. Extensive experiments on various tasks, including neural machine\ntranslation and text summarization demonstrate that our model outperforms the\noriginal Transformer.",
    "authors": [
      "Zhihao Fan",
      "Yeyun Gong",
      "Dayiheng Liu",
      "Zhongyu Wei",
      "Siyuan Wang",
      "Jian Jiao",
      "Nan Duan",
      "Ruofei Zhang",
      "Xuanjing Huang"
    ],
    "updated": "2021-03-25T04:07:44Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2103.13799",
    "title": "Bertinho: Galician BERT Representations",
    "link": "http://arxiv.org/abs/2103.13799v1",
    "pdf": "http://arxiv.org/pdf/2103.13799v1",
    "summary": "This paper presents a monolingual BERT model for Galician. We follow the\nrecent trend that shows that it is feasible to build robust monolingual BERT\nmodels even for relatively low-resource languages, while performing better than\nthe well-known official multilingual BERT (mBERT). More particularly, we\nrelease two monolingual Galician BERT models, built using 6 and 12 transformer\nlayers, respectively; trained with limited resources (~45 million tokens on a\nsingle GPU of 24GB). We then provide an exhaustive evaluation on a number of\ntasks such as POS-tagging, dependency parsing and named entity recognition. For\nthis purpose, all these tasks are cast in a pure sequence labeling setup in\norder to run BERT without the need to include any additional layers on top of\nit (we only use an output classification layer to map the contextualized\nrepresentations into the predicted label). The experiments show that our\nmodels, especially the 12-layer one, outperform the results of mBERT in most\ntasks.",
    "authors": [
      "David Vilares",
      "Marcos Garcia",
      "Carlos Gómez-Rodríguez"
    ],
    "updated": "2021-03-25T12:51:34Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1901.05287",
    "title": "Assessing BERT's Syntactic Abilities",
    "link": "http://arxiv.org/abs/1901.05287v1",
    "pdf": "http://arxiv.org/pdf/1901.05287v1",
    "summary": "I assess the extent to which the recently introduced BERT model captures\nEnglish syntactic phenomena, using (1) naturally-occurring subject-verb\nagreement stimuli; (2) \"coloreless green ideas\" subject-verb agreement stimuli,\nin which content words in natural sentences are randomly replaced with words\nsharing the same part-of-speech and inflection; and (3) manually crafted\nstimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT\nmodel performs remarkably well on all cases.",
    "authors": [
      "Yoav Goldberg"
    ],
    "updated": "2019-01-16T14:01:15Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2004.09984",
    "title": "BERT-ATTACK: Adversarial Attack Against BERT Using BERT",
    "link": "http://arxiv.org/abs/2004.09984v3",
    "pdf": "http://arxiv.org/pdf/2004.09984v3",
    "summary": "Adversarial attacks for discrete data (such as texts) have been proved\nsignificantly more challenging than continuous data (such as images) since it\nis difficult to generate adversarial samples with gradient-based methods.\nCurrent successful attack methods for texts usually adopt heuristic replacement\nstrategies on the character or word level, which remains challenging to find\nthe optimal solution in the massive space of possible combinations of\nreplacements while preserving semantic consistency and language fluency. In\nthis paper, we propose \\textbf{BERT-Attack}, a high-quality and effective\nmethod to generate adversarial samples using pre-trained masked language models\nexemplified by BERT. We turn BERT against its fine-tuned models and other deep\nneural models in downstream tasks so that we can successfully mislead the\ntarget models to predict incorrectly. Our method outperforms state-of-the-art\nattack strategies in both success rate and perturb percentage, while the\ngenerated adversarial samples are fluent and semantically preserved. Also, the\ncost of calculation is low, thus possible for large-scale generations. The code\nis available at https://github.com/LinyangLee/BERT-Attack.",
    "authors": [
      "Linyang Li",
      "Ruotian Ma",
      "Qipeng Guo",
      "Xiangyang Xue",
      "Xipeng Qiu"
    ],
    "updated": "2020-10-02T03:08:04Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1904.08398",
    "title": "DocBERT: BERT for Document Classification",
    "link": "http://arxiv.org/abs/1904.08398v3",
    "pdf": "http://arxiv.org/pdf/1904.08398v3",
    "summary": "We present, to our knowledge, the first application of BERT to document\nclassification. A few characteristics of the task might lead one to think that\nBERT is not the most appropriate model: syntactic structures matter less for\ncontent categories, documents can often be longer than typical BERT input, and\ndocuments often have multiple labels. Nevertheless, we show that a\nstraightforward classification model using BERT is able to achieve the state of\nthe art across four popular datasets. To address the computational expense\nassociated with BERT inference, we distill knowledge from BERT-large to small\nbidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x\nfewer parameters. The primary contribution of our paper is improved baselines\nthat can provide the foundation for future work.",
    "authors": [
      "Ashutosh Adhikari",
      "Achyudh Ram",
      "Raphael Tang",
      "Jimmy Lin"
    ],
    "updated": "2019-08-22T05:09:47Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2010.00454",
    "title": "Evaluating Multilingual BERT for Estonian",
    "link": "http://arxiv.org/abs/2010.00454v2",
    "pdf": "http://arxiv.org/pdf/2010.00454v2",
    "summary": "Recently, large pre-trained language models, such as BERT, have reached\nstate-of-the-art performance in many natural language processing tasks, but for\nmany languages, including Estonian, BERT models are not yet available. However,\nthere exist several multilingual BERT models that can handle multiple languages\nsimultaneously and that have been trained also on Estonian data. In this paper,\nwe evaluate four multilingual models -- multilingual BERT, multilingual\ndistilled BERT, XLM and XLM-RoBERTa -- on several NLP tasks including POS and\nmorphological tagging, NER and text classification. Our aim is to establish a\ncomparison between these multilingual BERT models and the existing baseline\nneural models for these tasks. Our results show that multilingual BERT models\ncan generalise well on different Estonian NLP tasks outperforming all baselines\nmodels for POS and morphological tagging and text classification, and reaching\nthe comparable level with the best baseline for NER, with XLM-RoBERTa achieving\nthe highest results compared with other multilingual models.",
    "authors": [
      "Claudia Kittask",
      "Kirill Milintsevich",
      "Kairit Sirts"
    ],
    "updated": "2021-01-08T10:00:42Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1910.14296",
    "title": "LIMIT-BERT : Linguistic Informed Multi-Task BERT",
    "link": "http://arxiv.org/abs/1910.14296v2",
    "pdf": "http://arxiv.org/pdf/1910.14296v2",
    "summary": "In this paper, we present a Linguistic Informed Multi-Task BERT (LIMIT-BERT)\nfor learning language representations across multiple linguistic tasks by\nMulti-Task Learning (MTL). LIMIT-BERT includes five key linguistic syntax and\nsemantics tasks: Part-Of-Speech (POS) tags, constituent and dependency\nsyntactic parsing, span and dependency semantic role labeling (SRL). Besides,\nLIMIT-BERT adopts linguistics mask strategy: Syntactic and Semantic Phrase\nMasking which mask all of the tokens corresponding to a syntactic/semantic\nphrase. Different from recent Multi-Task Deep Neural Networks (MT-DNN) (Liu et\nal., 2019), our LIMIT-BERT is linguistically motivated and learning in a\nsemi-supervised method which provides large amounts of linguistic-task data as\nsame as BERT learning corpus. As a result, LIMIT-BERT not only improves\nlinguistic tasks performance but also benefits from a regularization effect and\nlinguistic information that leads to more general representations to help adapt\nto new tasks and domains. LIMIT-BERT obtains new state-of-the-art or\ncompetitive results on both span and dependency semantic parsing on Propbank\nbenchmarks and both dependency and constituent syntactic parsing on Penn\nTreebank.",
    "authors": [
      "Junru Zhou",
      "Zhuosheng Zhang",
      "Hai Zhao",
      "Shuailiang Zhang"
    ],
    "updated": "2020-10-06T03:11:55Z",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id": "1912.09582",
    "title": "BERTje: A Dutch BERT Model",
    "link": "http://arxiv.org/abs/1912.09582v1",
    "pdf": "http://arxiv.org/pdf/1912.09582v1",
    "summary": "The transformer-based pre-trained language model BERT has helped to improve\nstate-of-the-art performance on many natural language processing (NLP) tasks.\nUsing the same architecture and parameters, we developed and evaluated a\nmonolingual Dutch BERT model called BERTje. Compared to the multilingual BERT\nmodel, which includes Dutch but is only based on Wikipedia text, BERTje is\nbased on a large and diverse dataset of 2.4 billion tokens. BERTje consistently\noutperforms the equally-sized multilingual BERT model on downstream NLP tasks\n(part-of-speech tagging, named-entity recognition, semantic role labeling, and\nsentiment analysis). Our pre-trained Dutch BERT model is made available at\nhttps://github.com/wietsedv/bertje.",
    "authors": [
      "Wietse de Vries",
      "Andreas van Cranenburgh",
      "Arianna Bisazza",
      "Tommaso Caselli",
      "Gertjan van Noord",
      "Malvina Nissim"
    ],
    "updated": "2019-12-19T22:59:26Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2203.06390",
    "title": "BiBERT: Accurate Fully Binarized BERT",
    "link": "http://arxiv.org/abs/2203.06390v1",
    "pdf": "http://arxiv.org/pdf/2203.06390v1",
    "summary": "The large pre-trained BERT has achieved remarkable performance on Natural\nLanguage Processing (NLP) tasks but is also computation and memory expensive.\nAs one of the powerful compression approaches, binarization extremely reduces\nthe computation and memory consumption by utilizing 1-bit parameters and\nbitwise operations. Unfortunately, the full binarization of BERT (i.e., 1-bit\nweight, embedding, and activation) usually suffer a significant performance\ndrop, and there is rare study addressing this problem. In this paper, with the\ntheoretical justification and empirical analysis, we identify that the severe\nperformance drop can be mainly attributed to the information degradation and\noptimization direction mismatch respectively in the forward and backward\npropagation, and propose BiBERT, an accurate fully binarized BERT, to eliminate\nthe performance bottlenecks. Specifically, BiBERT introduces an efficient\nBi-Attention structure for maximizing representation information statistically\nand a Direction-Matching Distillation (DMD) scheme to optimize the full\nbinarized BERT accurately. Extensive experiments show that BiBERT outperforms\nboth the straightforward baseline and existing state-of-the-art quantized BERTs\nwith ultra-low bit activations by convincing margins on the NLP benchmark. As\nthe first fully binarized BERT, our method yields impressive 56.3 times and\n31.2 times saving on FLOPs and model size, demonstrating the vast advantages\nand potential of the fully binarized BERT model in real-world\nresource-constrained scenarios.",
    "authors": [
      "Haotong Qin",
      "Yifu Ding",
      "Mingyuan Zhang",
      "Qinghua Yan",
      "Aishan Liu",
      "Qingqing Dang",
      "Ziwei Liu",
      "Xianglong Liu"
    ],
    "updated": "2022-03-12T09:46:13Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2409.00096",
    "title": "Non-instructional Fine-tuning: Enabling Instruction-Following\n  Capabilities in Pre-trained Language Models without Instruction-Following\n  Data",
    "link": "http://arxiv.org/abs/2409.00096v1",
    "pdf": "http://arxiv.org/pdf/2409.00096v1",
    "summary": "Instruction fine-tuning is crucial for today's large language models (LLMs)\nto learn to follow instructions and align with human preferences.\nConventionally, supervised data, including the instruction and the correct\nresponse, is required for instruction fine-tuning. To obtain such data, some\nresearchers prompted well-trained models like GPT-4 to generate instructions\nand correct responses. In this paper, we propose a novel approach that uses the\nfirst half of a random text from OpenWebText as the instruction and\nGPT-3.5-turbo or GPT-4-turbo to complete the text as the response. Despite the\ndata being \"non-instructional\", we found that pre-trained LLMs fine-tuned on\nthis data can gain instruction-following capabilities. This observation is\nverified by fine-tuning several well-known pre-trained LLMs (e.g., LLaMA-2-7B,\nLLaMA-3-8B, LLaMA-3-70B, Mistral-7B-v0.1). The \"non-instructional data\" also\nimproved some models that underwent supervised fine-tuning and human preference\nalignment. Our LLaMA-3-70B-Instruct fine-tuned through \"non-instructional data\"\nis comparable with LLaMA-3.1-70B-Instruct on the Arena Hard leaderboard. We\nanalyzed the \"non-instructional data\" and ensured it is devoid of content\nrelated to instruction fine-tuning. Our findings will inspire further\ninvestigation into how to develop instruction-following capabilities without\nexplicit instruction-related data.",
    "authors": [
      "Juncheng Xie",
      "Shensian Syu",
      "Hung-yi Lee"
    ],
    "updated": "2024-08-27T01:21:53Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id": "2410.10739",
    "title": "Balancing Continuous Pre-Training and Instruction Fine-Tuning:\n  Optimizing Instruction-Following in LLMs",
    "link": "http://arxiv.org/abs/2410.10739v1",
    "pdf": "http://arxiv.org/pdf/2410.10739v1",
    "summary": "Large Language Models (LLMs) for public use require continuous pre-training\nto remain up-to-date with the latest data. The models also need to be\nfine-tuned with specific instructions to maintain their ability to follow\ninstructions accurately. Typically, LLMs are released in two versions: the Base\nLLM, pre-trained on diverse data, and the instruction-refined LLM, additionally\ntrained with specific instructions for better instruction following. The\nquestion arises as to which model should undergo continuous pre-training to\nmaintain its instruction-following abilities while also staying current with\nthe latest data. In this study, we delve into the intricate relationship\nbetween continuous pre-training and instruction fine-tuning of the LLMs and\ninvestigate the impact of continuous pre-training on the instruction following\nabilities of both the base and its instruction finetuned model. Further, the\ninstruction fine-tuning process is computationally intense and requires a\nsubstantial number of hand-annotated examples for the model to learn\neffectively. This study aims to find the most compute-efficient strategy to\ngain up-to-date knowledge and instruction-following capabilities without\nrequiring any instruction data and fine-tuning. We empirically prove our\nfindings on the LLaMa 3, 3.1 and Qwen 2, 2.5 family of base and instruction\nmodels, providing a comprehensive exploration of our hypotheses across varying\nsizes of pre-training data corpus and different LLMs settings.",
    "authors": [
      "Ishan Jindal",
      "Chandana Badrinath",
      "Pranjal Bharti",
      "Lakkidi Vinay",
      "Sachin Dev Sharma"
    ],
    "updated": "2024-10-14T17:20:30Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2501.17703",
    "title": "Critique Fine-Tuning: Learning to Critique is More Effective than\n  Learning to Imitate",
    "link": "http://arxiv.org/abs/2501.17703v4",
    "pdf": "http://arxiv.org/pdf/2501.17703v4",
    "summary": "Supervised Fine-Tuning (SFT) is commonly used to train language models to\nimitate annotated responses for given instructions. In this paper, we propose\nCritique Fine-Tuning (CFT), a method more effective than SFT for reasoning\ntasks. Instead of simply imitating correct responses, CFT trains models to\ncritique noisy responses, inspired by human learning processes that emphasize\ncritical thinking, deeper analysis, and nuanced understanding - traits often\noverlooked by standard SFT. To validate the effectiveness of CFT, we construct\nmultiple critique datasets (e.g., WebInstruct, MetaMath, NuminaMath), where\nGPT-4o serves as the teacher to generate critiques in the form of ([query;\nnoisy response], critique). Experiments on these datasets demonstrate that CFT\nconsistently outperforms SFT by 4-10% across six mathematical reasoning\nbenchmarks, and is effective across different base models including Qwen2.5,\nQwen2.5-Math, and DeepSeek-Math. Notably, our model Qwen2.5-Math-CFT only\nrequires 1 hour of training on 8 x H100 over the 50K examples, yet matches or\noutperforms strong competitors like Qwen2.5-Math-Instruct on most benchmarks,\nwhich use over 2M samples. Moreover, it matches the performance of SimpleRL,\nwhich is a DeepSeek-r1 replication trained with 140 x more compute. Experiments\non IF_Eval and MT-Bench further demonstrate that CFT can significantly enhance\nthe model's general generation and instruction-following capabilities,\noutperforming the Qwen2.5-Math-Instruct by a large margin. Ablation studies\nshow that CFT is robust to noisy response sources and teacher critique models.\nThese findings highlight that CFT offers a more effective alternative to\nadvance the reasoning of language models.",
    "authors": [
      "Yubo Wang",
      "Xiang Yue",
      "Wenhu Chen"
    ],
    "updated": "2025-03-29T15:21:55Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2412.12865",
    "title": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over\n  Aligned Large Language Models",
    "link": "http://arxiv.org/abs/2412.12865v1",
    "pdf": "http://arxiv.org/pdf/2412.12865v1",
    "summary": "Alignment, endowing a pre-trained Large language model (LLM) with the ability\nto follow instructions, is crucial for its real-world applications.\nConventional supervised fine-tuning (SFT) methods formalize it as causal\nlanguage modeling typically with a cross-entropy objective, requiring a large\namount of high-quality instruction-response pairs. However, the quality of\nwidely used SFT datasets can not be guaranteed due to the high cost and\nintensive labor for the creation and maintenance in practice. To overcome the\nlimitations associated with the quality of SFT datasets, we introduce a novel\n\\textbf{p}reference-\\textbf{o}riented supervised \\textbf{f}ine-\\textbf{t}uning\napproach, namely PoFT. The intuition is to boost SFT by imposing a particular\npreference: \\textit{favoring the target model over aligned LLMs on the same SFT\ndata.} This preference encourages the target model to predict a higher\nlikelihood than that predicted by the aligned LLMs, incorporating assessment\ninformation on data quality (i.e., predicted likelihood by the aligned LLMs)\ninto the training process. Extensive experiments are conducted, and the results\nvalidate the effectiveness of the proposed method. PoFT achieves stable and\nconsistent improvements over the SFT baselines across different training\ndatasets and base models. Moreover, we prove that PoFT can be integrated with\nexisting SFT data filtering methods to achieve better performance, and further\nimproved by following preference optimization procedures, such as DPO.",
    "authors": [
      "Yuchen Fan",
      "Yuzhong Hong",
      "Qiushi Wang",
      "Junwei Bao",
      "Hongfei Jiang",
      "Yang Song"
    ],
    "updated": "2024-12-17T12:49:14Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2503.11197",
    "title": "Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering",
    "link": "http://arxiv.org/abs/2503.11197v4",
    "pdf": "http://arxiv.org/pdf/2503.11197v4",
    "summary": "Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa.",
    "authors": [
      "Gang Li",
      "Jizhong Liu",
      "Heinrich Dinkel",
      "Yadong Niu",
      "Junbo Zhang",
      "Jian Luan"
    ],
    "updated": "2025-05-14T02:12:43Z",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ]
  },
  {
    "id": "2501.11463",
    "title": "Curiosity-Driven Reinforcement Learning from Human Feedback",
    "link": "http://arxiv.org/abs/2501.11463v2",
    "pdf": "http://arxiv.org/pdf/2501.11463v2",
    "summary": "Reinforcement learning from human feedback (RLHF) has proven effective in\naligning large language models (LLMs) with human preferences, but often at the\ncost of reduced output diversity. This trade-off between diversity and\nalignment quality remains a significant challenge. Drawing inspiration from\ncuriosity-driven exploration in reinforcement learning, we introduce\ncuriosity-driven RLHF (CD-RLHF), a framework that incorporates intrinsic\nrewards for novel states, alongside traditional sparse extrinsic rewards, to\noptimize both output diversity and alignment quality. We demonstrate the\neffectiveness of CD-RLHF through extensive experiments on a range of tasks,\nincluding text summarization and instruction following. Our approach achieves\nsignificant gains in diversity on multiple diversity-oriented metrics while\nmaintaining alignment with human preferences comparable to standard RLHF. We\nmake our code publicly available at https://github.com/ernie-research/CD-RLHF.",
    "authors": [
      "Haoran Sun",
      "Yekun Chai",
      "Shuohuan Wang",
      "Yu Sun",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "updated": "2025-05-31T16:08:55Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2401.13586",
    "title": "Instruction Fine-Tuning: Does Prompt Loss Matter?",
    "link": "http://arxiv.org/abs/2401.13586v4",
    "pdf": "http://arxiv.org/pdf/2401.13586v4",
    "summary": "We present a novel study analyzing the effects of various prompt loss token\nweights (PLW) for supervised instruction fine-tuning (SIFT). While\nprompt-masking (PLW = 0) is common for SIFT, some fine-tuning APIs support\nfractional PLWs and suggest that using a small non-zero PLW can help stabilize\nlearning when fine-tuning on short-completion data. However, there has never\nbeen a study confirming this claim, and OpenAI, a major cloud-based SIFT\nprovider, recently removed this parameter from their fine-tuning API. We found\nthat performance of models fine-tuned on short-completion data had a\nstatistically-significant negative quadratic relationship with PLW. Using small\nvalues (0.01 - 0.5) of PLW produced better results on multiple-choice and\nshort-generation benchmarks (outperforming models fine-tuned on long-completion\ndata) while large values (~ 1.0) of PLW produced better results on\nlong-generation benchmarks. We explained this effect and verified its\nimportance through additional experiments. This research serves as a warning to\nAPI providers about the importance of providing a PLW parameter for SIFT.",
    "authors": [
      "Mathew Huerta-Enochian",
      "Seung Yong Ko"
    ],
    "updated": "2024-10-14T01:16:38Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id": "2509.00309",
    "title": "Balanced Actor Initialization: Stable RLHF Training of\n  Distillation-Based Reasoning Models",
    "link": "http://arxiv.org/abs/2509.00309v1",
    "pdf": "http://arxiv.org/pdf/2509.00309v1",
    "summary": "The development of alignment and reasoning capabilities in large language\nmodels has seen remarkable progress through two paradigms: instruction tuning\nand reinforcement learning from human feedback (RLHF) alignment paradigm, and\ndistillation-based reasoning fine-tuning paradigm. While both approaches prove\neffective independently, the third paradigm of applying RLHF to\ndistillation-trained models presents significant challenges. Our investigation\nreveals two critical phenomena that emerge in this paradigm: Sequence Length\nCollapse, where language generation dramatically reduces during early RLHF\ntraining, and the Reward Hockey Stick Curve, featuring severe reward score\ndrops followed by gradual recovery. These instabilities fundamentally\ncompromise the model's alignment and reasoning capabilities. To address these\nchallenges, we propose Balanced Actor Initialization (BAI), a two-stage\nweighted model merging approach. BAI first merges instruction-following and\ndistillation-based reasoning fine-tuned models, then further combines this\nintermediate model with the pretrained model to preserve foundational\nknowledge. Through comprehensive experiments across diverse benchmarks and\ndetailed analysis of training experiments, we demonstrate that BAI resolves\nSequence Length Collapse, mitigates the Reward Hockey Stick Curve, and enables\ncontinuous sequence length improvement during training. Additionally, our\nanalysis reveals that balanced merging ratios achieve optimal trade-offs\nbetween training stability and reasoning capability preservation. Our work\nprovides the effective solution for stable training in this third paradigm,\nenabling more capable reasoning models that combine distillation efficiency\nwith RLHF alignment.",
    "authors": [
      "Chen Zheng",
      "Yiyuan Ma",
      "Yuan Yang",
      "Deyi Liu",
      "Jing Liu",
      "Zuquan Song",
      "Yuxin Song",
      "Cheng Ren",
      "Hang Zhu",
      "Xin Liu",
      "Yiyuan Ma",
      "Siyuan Qiao",
      "Xun Zhou",
      "Liang Xiang",
      "Yonghui Wu"
    ],
    "updated": "2025-08-30T01:53:25Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2509.11106",
    "title": "Fluid Language Model Benchmarking",
    "link": "http://arxiv.org/abs/2509.11106v1",
    "pdf": "http://arxiv.org/pdf/2509.11106v1",
    "summary": "Language model (LM) benchmarking faces several challenges: comprehensive\nevaluations are costly, benchmarks often fail to measure the intended\ncapabilities, and evaluation quality can degrade due to labeling errors and\nbenchmark saturation. Although various strategies have been proposed to\nmitigate these issues, they tend to address individual aspects in isolation,\nneglecting broader questions about overall evaluation quality. Here, we\nintroduce Fluid Benchmarking, a new evaluation approach that advances LM\nbenchmarking across multiple dimensions. Inspired by psychometrics, Fluid\nBenchmarking is based on the insight that the relative value of benchmark items\ndepends on an LM's capability level, suggesting that evaluation should adapt to\neach LM. Methodologically, Fluid Benchmarking estimates an item response model\nbased on existing LM evaluation results and uses the inferred quantities to\nselect evaluation items dynamically, similar to computerized adaptive testing\nin education. In our experiments, we compare Fluid Benchmarking against the\ncommon practice of random item sampling as well as more sophisticated\nbaselines, including alternative methods grounded in item response theory. We\nexamine four dimensions -- efficiency, validity, variance, and saturation --\nand find that Fluid Benchmarking achieves superior performance in all of them\n(e.g., higher validity and less variance on MMLU with fifty times fewer items).\nOur analysis shows that the two components of Fluid Benchmarking have distinct\neffects: item response theory, used to map performance into a latent ability\nspace, increases validity, while dynamic item selection reduces variance.\nOverall, our results suggest that LM benchmarking can be substantially improved\nby moving beyond static evaluation.",
    "authors": [
      "Valentin Hofmann",
      "David Heineman",
      "Ian Magnusson",
      "Kyle Lo",
      "Jesse Dodge",
      "Maarten Sap",
      "Pang Wei Koh",
      "Chun Wang",
      "Hannaneh Hajishirzi",
      "Noah A. Smith"
    ],
    "updated": "2025-09-14T05:49:42Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id": "2112.01894",
    "title": "The Catalan Language CLUB",
    "link": "http://arxiv.org/abs/2112.01894v1",
    "pdf": "http://arxiv.org/pdf/2112.01894v1",
    "summary": "The Catalan Language Understanding Benchmark (CLUB) encompasses various\ndatasets representative of different NLU tasks that enable accurate evaluations\nof language models, following the General Language Understanding Evaluation\n(GLUE) example. It is part of AINA and PlanTL, two public funding initiatives\nto empower the Catalan language in the Artificial Intelligence era.",
    "authors": [
      "Carlos Rodriguez-Penagos",
      "Carme Armentano-Oller",
      "Marta Villegas",
      "Maite Melero",
      "Aitor Gonzalez",
      "Ona de Gibert Bonet",
      "Casimiro Carrino Pio"
    ],
    "updated": "2021-12-03T13:15:17Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "91F20",
      "I.2.7"
    ]
  },
  {
    "id": "2308.11696",
    "title": "Efficient Benchmarking of Language Models",
    "link": "http://arxiv.org/abs/2308.11696v5",
    "pdf": "http://arxiv.org/pdf/2308.11696v5",
    "summary": "The increasing versatility of language models (LMs) has given rise to a new\nclass of benchmarks that comprehensively assess a broad range of capabilities.\nSuch benchmarks are associated with massive computational costs, extending to\nthousands of GPU hours per model. However, the efficiency aspect of these\nevaluation efforts had raised little discussion in the literature. In this\nwork, we present the problem of Efficient Benchmarking, namely, intelligently\nreducing the computation costs of LM evaluation without compromising\nreliability. Using the HELM benchmark as a test case, we investigate how\ndifferent benchmark design choices affect the computation-reliability\ntrade-off. We propose to evaluate the reliability of such decisions, by using a\nnew measure -- Decision Impact on Reliability, DIoR for short. We find, for\nexample, that a benchmark leader may change by merely removing a low-ranked\nmodel from the benchmark, and observe that a correct benchmark ranking can be\nobtained by considering only a fraction of the evaluation examples. Based on\nour findings, we outline a set of concrete recommendations for efficient\nbenchmark design and utilization practices. To take a step further, we use our\nfindings to propose an evaluation algorithm, that, when applied to the HELM\nbenchmark, leads to dramatic cost savings with minimal loss of benchmark\nreliability, often reducing computation by x100 or more.",
    "authors": [
      "Yotam Perlitz",
      "Elron Bandel",
      "Ariel Gera",
      "Ofir Arviv",
      "Liat Ein-Dor",
      "Eyal Shnarch",
      "Noam Slonim",
      "Michal Shmueli-Scheuer",
      "Leshem Choshen"
    ],
    "updated": "2024-04-01T17:34:34Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ]
  },
  {
    "id": "2506.20249",
    "title": "Language Modeling by Language Models",
    "link": "http://arxiv.org/abs/2506.20249v1",
    "pdf": "http://arxiv.org/pdf/2506.20249v1",
    "summary": "Can we leverage LLMs to model the process of discovering novel language model\n(LM) architectures? Inspired by real research, we propose a multi-agent LLM\napproach that simulates the conventional stages of research, from ideation and\nliterature search (proposal stage) to design implementation (code generation),\ngenerative pre-training, and downstream evaluation (verification). Using ideas\nfrom scaling laws, our system, Genesys, employs a Ladder of Scales approach;\nnew designs are proposed, adversarially reviewed, implemented, and selectively\nverified at increasingly larger model scales (14M$\\sim$350M parameters) with a\nnarrowing budget (the number of models we can train at each scale). To help\nmake discovery efficient and factorizable, Genesys uses a novel genetic\nprogramming backbone, which we show has empirical advantages over commonly used\ndirect prompt generation workflows (e.g., $\\sim$86\\% percentage point\nimprovement in successful design generation, a key bottleneck). We report\nexperiments involving 1,162 newly discovered designs (1,062 fully verified\nthrough pre-training) and find the best designs to be highly competitive with\nknown architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common\nbenchmarks). We couple these results with comprehensive system-level ablations\nand formal results, which give broader insights into the design of effective\nautonomous discovery systems.",
    "authors": [
      "Junyan Cheng",
      "Peter Clark",
      "Kyle Richardson"
    ],
    "updated": "2025-06-25T08:46:10Z",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ]
  },
  {
    "id": "2406.08702",
    "title": "VLind-Bench: Measuring Language Priors in Large Vision-Language Models",
    "link": "http://arxiv.org/abs/2406.08702v4",
    "pdf": "http://arxiv.org/pdf/2406.08702v4",
    "summary": "Large Vision-Language Models (LVLMs) have demonstrated outstanding\nperformance across various multimodal tasks. However, they suffer from a\nproblem known as language prior, where responses are generated based solely on\ntextual patterns while disregarding image information. Addressing the issue of\nlanguage prior is crucial, as it can lead to undesirable biases or\nhallucinations when dealing with images that are out of training distribution.\nDespite its importance, current methods for accurately measuring language\npriors in LVLMs are poorly studied. Although existing benchmarks based on\ncounterfactual or out-of-distribution images can partially be used to measure\nlanguage priors, they fail to disentangle language priors from other\nconfounding factors. To this end, we propose a new benchmark called\nVLind-Bench, which is the first benchmark specifically designed to measure the\nlanguage priors, or blindness, of LVLMs. It not only includes tests on\ncounterfactual images to assess language priors but also involves a series of\ntests to evaluate more basic capabilities such as commonsense knowledge, visual\nperception, and commonsense biases. For each instance in our benchmark, we\nensure that all these basic tests are passed before evaluating the language\npriors, thereby minimizing the influence of other factors on the assessment.\nThe evaluation and analysis of recent LVLMs in our benchmark reveal that almost\nall models exhibit a significant reliance on language priors, presenting a\nstrong challenge in the field.",
    "authors": [
      "Kang-il Lee",
      "Minbeom Kim",
      "Seunghyun Yoon",
      "Minsung Kim",
      "Dongryeol Lee",
      "Hyukhun Koh",
      "Kyomin Jung"
    ],
    "updated": "2025-02-08T23:14:12Z",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ]
  },
  {
    "id": "2109.15254",
    "title": "SlovakBERT: Slovak Masked Language Model",
    "link": "http://arxiv.org/abs/2109.15254v2",
    "pdf": "http://arxiv.org/pdf/2109.15254v2",
    "summary": "We introduce a new Slovak masked language model called SlovakBERT. This is to\nour best knowledge the first paper discussing Slovak transformers-based\nlanguage models. We evaluate our model on several NLP tasks and achieve\nstate-of-the-art results. This evaluation is likewise the first attempt to\nestablish a benchmark for Slovak language models. We publish the masked\nlanguage model, as well as the fine-tuned models for part-of-speech tagging,\nsentiment analysis and semantic textual similarity.",
    "authors": [
      "Matúš Pikuliak",
      "Štefan Grivalský",
      "Martin Konôpka",
      "Miroslav Blšták",
      "Martin Tamajka",
      "Viktor Bachratý",
      "Marián Šimko",
      "Pavol Balážik",
      "Michal Trnka",
      "Filip Uhlárik"
    ],
    "updated": "2022-10-29T19:41:06Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2503.23730",
    "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language",
    "link": "http://arxiv.org/abs/2503.23730v1",
    "pdf": "http://arxiv.org/pdf/2503.23730v1",
    "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA",
    "authors": [
      "Yoonshik Kim",
      "Jaeyoon Jung"
    ],
    "updated": "2025-03-31T05:04:25Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id": "2410.08928",
    "title": "Towards Multilingual LLM Evaluation for European Languages",
    "link": "http://arxiv.org/abs/2410.08928v2",
    "pdf": "http://arxiv.org/pdf/2410.08928v2",
    "summary": "The rise of Large Language Models (LLMs) has revolutionized natural language\nprocessing across numerous languages and tasks. However, evaluating LLM\nperformance in a consistent and meaningful way across multiple European\nlanguages remains challenging, especially due to the scarcity of\nlanguage-parallel multilingual benchmarks. We introduce a multilingual\nevaluation approach tailored for European languages. We employ translated\nversions of five widely-used benchmarks to assess the capabilities of 40 LLMs\nacross 21 European languages. Our contributions include examining the\neffectiveness of translated benchmarks, assessing the impact of different\ntranslation services, and offering a multilingual evaluation framework for LLMs\nthat includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC,\nEU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly\navailable to encourage further research in multilingual LLM evaluation.",
    "authors": [
      "Klaudia Thellmann",
      "Bernhard Stadler",
      "Michael Fromm",
      "Jasper Schulze Buschhoff",
      "Alex Jude",
      "Fabio Barth",
      "Johannes Leveling",
      "Nicolas Flores-Herr",
      "Joachim Köhler",
      "René Jäkel",
      "Mehdi Ali"
    ],
    "updated": "2024-10-17T17:58:53Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id": "1810.03444",
    "title": "Phrase-Based Attentions",
    "link": "http://arxiv.org/abs/1810.03444v1",
    "pdf": "http://arxiv.org/pdf/1810.03444v1",
    "summary": "Most state-of-the-art neural machine translation systems, despite being\ndifferent in architectural skeletons (e.g. recurrence, convolutional), share an\nindispensable feature: the Attention. However, most existing attention methods\nare token-based and ignore the importance of phrasal alignments, the key\ningredient for the success of phrase-based statistical machine translation. In\nthis paper, we propose novel phrase-based attention methods to model n-grams of\ntokens as attention entities. We incorporate our phrase-based attentions into\nthe recently proposed Transformer network, and demonstrate that our approach\nyields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for\nGerman-to-English translation tasks on WMT newstest2014 using WMT'16 training\ndata.",
    "authors": [
      "Phi Xuan Nguyen",
      "Shafiq Joty"
    ],
    "updated": "2018-09-30T16:28:40Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2507.07694",
    "title": "SAS: Simulated Attention Score",
    "link": "http://arxiv.org/abs/2507.07694v1",
    "pdf": "http://arxiv.org/pdf/2507.07694v1",
    "summary": "The attention mechanism is a core component of the Transformer architecture.\nVarious methods have been developed to compute attention scores, including\nmulti-head attention (MHA), multi-query attention, group-query attention and so\non. We further analyze the MHA and observe that its performance improves as the\nnumber of attention heads increases, provided the hidden size per head remains\nsufficiently large. Therefore, increasing both the head count and hidden size\nper head with minimal parameter overhead can lead to significant performance\ngains at a low cost. Motivated by this insight, we introduce Simulated\nAttention Score (SAS), which maintains a compact model size while simulating a\nlarger number of attention heads and hidden feature dimension per head. This is\nachieved by projecting a low-dimensional head representation into a\nhigher-dimensional space, effectively increasing attention capacity without\nincreasing parameter count. Beyond the head representations, we further extend\nthe simulation approach to feature dimension of the key and query embeddings,\nenhancing expressiveness by mimicking the behavior of a larger model while\npreserving the original model size. To control the parameter cost, we also\npropose Parameter-Efficient Attention Aggregation (PEAA). Comprehensive\nexperiments on a variety of datasets and tasks demonstrate the effectiveness of\nthe proposed SAS method, achieving significant improvements over different\nattention variants.",
    "authors": [
      "Chuanyang Zheng",
      "Jiankai Sun",
      "Yihang Gao",
      "Yuehao Wang",
      "Peihao Wang",
      "Jing Xiong",
      "Liliang Ren",
      "Hao Cheng",
      "Janardhan Kulkarni",
      "Yelong Shen",
      "Atlas Wang",
      "Mac Schwager",
      "Anderson Schneider",
      "Xiaodong Liu",
      "Jianfeng Gao"
    ],
    "updated": "2025-07-10T12:16:16Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1909.12406",
    "title": "Monotonic Multihead Attention",
    "link": "http://arxiv.org/abs/1909.12406v1",
    "pdf": "http://arxiv.org/pdf/1909.12406v1",
    "summary": "Simultaneous machine translation models start generating a target sequence\nbefore they have encoded or read the source sequence. Recent approaches for\nthis task either apply a fixed policy on a state-of-the art Transformer model,\nor a learnable monotonic attention on a weaker recurrent neural network-based\nstructure. In this paper, we propose a new attention mechanism, Monotonic\nMultihead Attention (MMA), which extends the monotonic attention mechanism to\nmultihead attention. We also introduce two novel and interpretable approaches\nfor latency control that are specifically designed for multiple attentions\nheads. We apply MMA to the simultaneous machine translation task and\ndemonstrate better latency-quality tradeoffs compared to MILk, the previous\nstate-of-the-art approach. We also analyze how the latency controls affect the\nattention span and we motivate the introduction of our model by analyzing the\neffect of the number of decoder layers and heads on quality and latency.",
    "authors": [
      "Xutai Ma",
      "Juan Pino",
      "James Cross",
      "Liezl Puzon",
      "Jiatao Gu"
    ],
    "updated": "2019-09-26T21:32:50Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1906.03731",
    "title": "Is Attention Interpretable?",
    "link": "http://arxiv.org/abs/1906.03731v1",
    "pdf": "http://arxiv.org/pdf/1906.03731v1",
    "summary": "Attention mechanisms have recently boosted performance on a range of NLP\ntasks. Because attention layers explicitly weight input components'\nrepresentations, it is also often assumed that attention can be used to\nidentify information that models found important (e.g., specific contextualized\nword tokens). We test whether that assumption holds by manipulating attention\nweights in already-trained text classification models and analyzing the\nresulting differences in their predictions. While we observe some ways in which\nhigher attention weights correlate with greater impact on model predictions, we\nalso find many ways in which this does not hold, i.e., where gradient-based\nrankings of attention weights better predict their effects than their\nmagnitudes. We conclude that while attention noisily predicts input components'\noverall importance to a model, it is by no means a fail-safe indicator.",
    "authors": [
      "Sofia Serrano",
      "Noah A. Smith"
    ],
    "updated": "2019-06-09T22:46:12Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2504.00927",
    "title": "Multi-Token Attention",
    "link": "http://arxiv.org/abs/2504.00927v2",
    "pdf": "http://arxiv.org/pdf/2504.00927v2",
    "summary": "Soft attention is a critical mechanism powering LLMs to locate relevant parts\nwithin a given context. However, individual attention weights are determined by\nthe similarity of only a single query and key token vector. This \"single token\nattention\" bottlenecks the amount of information used in distinguishing a\nrelevant part from the rest of the context. To address this issue, we propose a\nnew attention method, Multi-Token Attention (MTA), which allows LLMs to\ncondition their attention weights on multiple query and key vectors\nsimultaneously. This is achieved by applying convolution operations over\nqueries, keys and heads, allowing nearby queries and keys to affect each\nother's attention weights for more precise attention. As a result, our method\ncan locate relevant context using richer, more nuanced information that can\nexceed a single vector's capacity. Through extensive evaluations, we\ndemonstrate that MTA achieves enhanced performance on a range of popular\nbenchmarks. Notably, it outperforms Transformer baseline models on standard\nlanguage modeling tasks, and on tasks that require searching for information\nwithin long contexts, where our method's ability to leverage richer information\nproves particularly beneficial.",
    "authors": [
      "Olga Golovneva",
      "Tianlu Wang",
      "Jason Weston",
      "Sainbayar Sukhbaatar"
    ],
    "updated": "2025-07-11T15:49:30Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1710.03743",
    "title": "Confidence through Attention",
    "link": "http://arxiv.org/abs/1710.03743v1",
    "pdf": "http://arxiv.org/pdf/1710.03743v1",
    "summary": "Attention distributions of the generated translations are a useful bi-product\nof attention-based recurrent neural network translation models and can be\ntreated as soft alignments between the input and output tokens. In this work,\nwe use attention distributions as a confidence metric for output translations.\nWe present two strategies of using the attention distributions: filtering out\nbad translations from a large back-translated corpus, and selecting the best\ntranslation in a hybrid setup of two different translation systems. While\nmanual evaluation indicated only a weak correlation between our confidence\nscore and human judgments, the use-cases showed improvements of up to 2.22 BLEU\npoints for filtering and 0.99 points for hybrid translation, tested on\nEnglish<->German and English<->Latvian translation.",
    "authors": [
      "Matīss Rikters",
      "Mark Fishel"
    ],
    "updated": "2017-10-10T17:47:41Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1710.03348",
    "title": "What does Attention in Neural Machine Translation Pay Attention to?",
    "link": "http://arxiv.org/abs/1710.03348v1",
    "pdf": "http://arxiv.org/pdf/1710.03348v1",
    "summary": "Attention in neural machine translation provides the possibility to encode\nrelevant parts of the source sentence at each translation step. As a result,\nattention is considered to be an alignment model as well. However, there is no\nwork that specifically studies attention and provides analysis of what is being\nlearned by attention models. Thus, the question still remains that how\nattention is similar or different from the traditional alignment. In this\npaper, we provide detailed analysis of attention and compare it to traditional\nalignment. We answer the question of whether attention is only capable of\nmodelling translational equivalent or it captures more information. We show\nthat attention is different from alignment in some cases and is capturing\nuseful information other than alignments.",
    "authors": [
      "Hamidreza Ghader",
      "Christof Monz"
    ],
    "updated": "2017-10-09T23:21:34Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1810.13320",
    "title": "Convolutional Self-Attention Network",
    "link": "http://arxiv.org/abs/1810.13320v2",
    "pdf": "http://arxiv.org/pdf/1810.13320v2",
    "summary": "Self-attention network (SAN) has recently attracted increasing interest due\nto its fully parallelized computation and flexibility in modeling dependencies.\nIt can be further enhanced with multi-headed attention mechanism by allowing\nthe model to jointly attend to information from different representation\nsubspaces at different positions (Vaswani et al., 2017). In this work, we\npropose a novel convolutional self-attention network (CSAN), which offers SAN\nthe abilities to 1) capture neighboring dependencies, and 2) model the\ninteraction between multiple attention heads. Experimental results on WMT14\nEnglish-to-German translation task demonstrate that the proposed approach\noutperforms both the strong Transformer baseline and other existing works on\nenhancing the locality of SAN. Comparing with previous work, our model does not\nintroduce any new parameters.",
    "authors": [
      "Baosong Yang",
      "Longyue Wang",
      "Derek F. Wong",
      "Lidia S. Chao",
      "Zhaopeng Tu"
    ],
    "updated": "2019-04-08T09:15:30Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2010.11973",
    "title": "Rediscovering the Slavic Continuum in Representations Emerging from\n  Neural Models of Spoken Language Identification",
    "link": "http://arxiv.org/abs/2010.11973v1",
    "pdf": "http://arxiv.org/pdf/2010.11973v1",
    "summary": "Deep neural networks have been employed for various spoken language\nrecognition tasks, including tasks that are multilingual by definition such as\nspoken language identification. In this paper, we present a neural model for\nSlavic language identification in speech signals and analyze its emergent\nrepresentations to investigate whether they reflect objective measures of\nlanguage relatedness and/or non-linguists' perception of language similarity.\nWhile our analysis shows that the language representation space indeed captures\nlanguage relatedness to a great extent, we find perceptual confusability\nbetween languages in our study to be the best predictor of the language\nrepresentation similarity.",
    "authors": [
      "Badr M. Abdullah",
      "Jacek Kudera",
      "Tania Avgustinova",
      "Bernd Möbius",
      "Dietrich Klakow"
    ],
    "updated": "2020-10-22T18:18:19Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2205.10964",
    "title": "The Geometry of Multilingual Language Model Representations",
    "link": "http://arxiv.org/abs/2205.10964v2",
    "pdf": "http://arxiv.org/pdf/2205.10964v2",
    "summary": "We assess how multilingual language models maintain a shared multilingual\nrepresentation space while still encoding language-sensitive information in\neach language. Using XLM-R as a case study, we show that languages occupy\nsimilar linear subspaces after mean-centering, evaluated based on causal\neffects on language modeling performance and direct comparisons between\nsubspaces for 88 languages. The subspace means differ along language-sensitive\naxes that are relatively stable throughout middle layers, and these axes encode\ninformation such as token vocabularies. Shifting representations by language\nmeans is sufficient to induce token predictions in different languages.\nHowever, we also identify stable language-neutral axes that encode information\nsuch as token positions and part-of-speech. We visualize representations\nprojected onto language-sensitive and language-neutral axes, identifying\nlanguage family and part-of-speech clusters, along with spirals, toruses, and\ncurves representing token position information. These results demonstrate that\nmultilingual language models encode information along orthogonal\nlanguage-sensitive and language-neutral axes, allowing the models to extract a\nvariety of features for downstream tasks and cross-lingual transfer learning.",
    "authors": [
      "Tyler A. Chang",
      "Zhuowen Tu",
      "Benjamin K. Bergen"
    ],
    "updated": "2022-10-21T23:10:27Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1707.06519",
    "title": "Language Transfer of Audio Word2Vec: Learning Audio Segment\n  Representations without Target Language Data",
    "link": "http://arxiv.org/abs/1707.06519v1",
    "pdf": "http://arxiv.org/pdf/1707.06519v1",
    "summary": "Audio Word2Vec offers vector representations of fixed dimensionality for\nvariable-length audio segments using Sequence-to-sequence Autoencoder (SA).\nThese vector representations are shown to describe the sequential phonetic\nstructures of the audio segments to a good degree, with real world applications\nsuch as query-by-example Spoken Term Detection (STD). This paper examines the\ncapability of language transfer of Audio Word2Vec. We train SA from one\nlanguage (source language) and use it to extract the vector representation of\nthe audio segments of another language (target language). We found that SA can\nstill catch phonetic structure from the audio segments of the target language\nif the source and target languages are similar. In query-by-example STD, we\nobtain the vector representations from the SA learned from a large amount of\nsource language data, and found them surpass the representations from naive\nencoder and SA directly learned from a small amount of target language data.\nThe result shows that it is possible to learn Audio Word2Vec model from\nhigh-resource languages and use it on low-resource languages. This further\nexpands the usability of Audio Word2Vec.",
    "authors": [
      "Chia-Hao Shen",
      "Janet Y. Sung",
      "Hung-Yi Lee"
    ],
    "updated": "2017-07-19T10:54:00Z",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id": "2504.11492",
    "title": "Language and Knowledge Representation: A Stratified Approach",
    "link": "http://arxiv.org/abs/2504.11492v1",
    "pdf": "http://arxiv.org/pdf/2504.11492v1",
    "summary": "The thesis proposes the problem of representation heterogeneity to emphasize\nthe fact that heterogeneity is an intrinsic property of any representation,\nwherein, different observers encode different representations of the same\ntarget reality in a stratified manner using different concepts, language and\nknowledge (as well as data). The thesis then advances a top-down solution\napproach to the above stratified problem of representation heterogeneity in\nterms of several solution components, namely: (i) a representation formalism\nstratified into concept level, language level, knowledge level and data level\nto accommodate representation heterogeneity, (ii) a top-down language\nrepresentation using Universal Knowledge Core (UKC), UKC namespaces and domain\nlanguages to tackle the conceptual and language level heterogeneity, (iii) a\ntop-down knowledge representation using the notions of language teleontology\nand knowledge teleontology to tackle the knowledge level heterogeneity, (iv)\nthe usage and further development of the existing LiveKnowledge catalog for\nenforcing iterative reuse and sharing of language and knowledge\nrepresentations, and, (v) the kTelos methodology integrating the solution\ncomponents above to iteratively generate the language and knowledge\nrepresentations absolving representation heterogeneity. The thesis also\nincludes proof-of-concepts of the language and knowledge representations\ndeveloped for two international research projects - DataScientia (data\ncatalogs) and JIDEP (materials modelling). Finally, the thesis concludes with\nfuture lines of research.",
    "authors": [
      "Mayukh Bagchi"
    ],
    "updated": "2025-04-14T20:18:10Z",
    "categories": [
      "cs.DB",
      "cs.CL",
      "cs.DL"
    ]
  },
  {
    "id": "2406.08092",
    "title": "Languages Transferred Within the Encoder: On Representation Transfer in\n  Zero-Shot Multilingual Translation",
    "link": "http://arxiv.org/abs/2406.08092v2",
    "pdf": "http://arxiv.org/pdf/2406.08092v2",
    "summary": "Understanding representation transfer in multilingual neural machine\ntranslation (MNMT) can reveal the reason for the zero-shot translation\ndeficiency. In this work, we systematically analyze the representational issue\nof MNMT models. We first introduce the identity pair, translating a sentence to\nitself, to address the lack of the base measure in multilingual investigations,\nas the identity pair can reflect the representation of a language within the\nmodel. Then, we demonstrate that the encoder transfers the source language to\nthe representational subspace of the target language instead of the\nlanguage-agnostic state. Thus, the zero-shot translation deficiency arises\nbecause the representation of a translation is entangled with other languages\nand not transferred to the target language effectively. Based on our findings,\nwe propose two methods: 1) low-rank language-specific embedding at the encoder,\nand 2) language-specific contrastive learning of the representation at the\ndecoder. The experimental results on Europarl-15, TED-19, and OPUS-100 datasets\nshow that our methods substantially enhance the performance of zero-shot\ntranslations without sacrifices in supervised directions by improving language\ntransfer capacity, thereby providing practical evidence to support our\nconclusions. Codes are available at https://github.com/zhiqu22/ZeroTrans.",
    "authors": [
      "Zhi Qu",
      "Chenchen Ding",
      "Taro Watanabe"
    ],
    "updated": "2025-04-08T03:39:51Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2409.18199",
    "title": "LangSAMP: Language-Script Aware Multilingual Pretraining",
    "link": "http://arxiv.org/abs/2409.18199v2",
    "pdf": "http://arxiv.org/pdf/2409.18199v2",
    "summary": "Recent multilingual pretrained language models (mPLMs) often avoid using\nlanguage embeddings -- learnable vectors assigned to individual languages.\nHowever, this places a significant burden on token representations to encode\nall language-specific information, which may hinder language neutrality. To\naddress this limitation, we propose Language-Script Aware Multilingual\nPretraining (LangSAMP), a method that incorporates both language and script\nembeddings to enhance representation learning. Specifically, we integrate these\nembeddings into the output of the Transformer blocks before passing the final\nrepresentations to the language modeling head for prediction. We apply LangSAMP\nto the continual pretraining of XLM-R on a highly multilingual corpus covering\nmore than 500 languages. The resulting model consistently outperforms the\nbaseline in zero-shot crosslingual transfer across diverse downstream tasks.\nExtensive analysis reveals that language and script embeddings capture\nlanguage- and script-specific nuances, which benefits more language-neutral\nrepresentations, proven by improved pairwise cosine similarity. In our case\nstudy, we also show that language and script embeddings can be used to select\nbetter source languages for crosslingual transfer. We make our code and models\npublicly available at https://github.com/cisnlp/LangSAMP.",
    "authors": [
      "Yihong Liu",
      "Haotian Ye",
      "Chunlan Ma",
      "Mingyang Wang",
      "Hinrich Schütze"
    ],
    "updated": "2025-05-21T18:52:11Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1902.09697",
    "title": "Polyglot Contextual Representations Improve Crosslingual Transfer",
    "link": "http://arxiv.org/abs/1902.09697v2",
    "pdf": "http://arxiv.org/pdf/1902.09697v2",
    "summary": "We introduce Rosita, a method to produce multilingual contextual word\nrepresentations by training a single language model on text from multiple\nlanguages. Our method combines the advantages of contextual word\nrepresentations with those of multilingual representation learning. We produce\nlanguage models from dissimilar language pairs (English/Arabic and\nEnglish/Chinese) and use them in dependency parsing, semantic role labeling,\nand named entity recognition, with comparisons to monolingual and\nnon-contextual variants. Our results provide further evidence for the benefits\nof polyglot learning, in which representations are shared across multiple\nlanguages.",
    "authors": [
      "Phoebe Mulcaire",
      "Jungo Kasai",
      "Noah A. Smith"
    ],
    "updated": "2019-03-18T21:00:13Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2404.12444",
    "title": "mOthello: When Do Cross-Lingual Representation Alignment and\n  Cross-Lingual Transfer Emerge in Multilingual Models?",
    "link": "http://arxiv.org/abs/2404.12444v1",
    "pdf": "http://arxiv.org/pdf/2404.12444v1",
    "summary": "Many pretrained multilingual models exhibit cross-lingual transfer ability,\nwhich is often attributed to a learned language-neutral representation during\npretraining. However, it remains unclear what factors contribute to the\nlearning of a language-neutral representation, and whether the learned\nlanguage-neutral representation suffices to facilitate cross-lingual transfer.\nWe propose a synthetic task, Multilingual Othello (mOthello), as a testbed to\ndelve into these two questions. We find that: (1) models trained with naive\nmultilingual pretraining fail to learn a language-neutral representation across\nall input languages; (2) the introduction of \"anchor tokens\" (i.e., lexical\nitems that are identical across languages) helps cross-lingual representation\nalignment; and (3) the learning of a language-neutral representation alone is\nnot sufficient to facilitate cross-lingual transfer. Based on our findings, we\npropose a novel approach - multilingual pretraining with unified output space -\nthat both induces the learning of language-neutral representation and\nfacilitates cross-lingual transfer.",
    "authors": [
      "Tianze Hua",
      "Tian Yun",
      "Ellie Pavlick"
    ],
    "updated": "2024-04-18T18:03:08Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  }
]