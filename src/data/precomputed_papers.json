[
  {
    "id": "2405.04515",
    "title": "A Transformer with Stack Attention",
    "link": "http://arxiv.org/abs/2405.04515v2",
    "pdf": "http://arxiv.org/pdf/2405.04515v2",
    "summary": "Natural languages are believed to be (mildly) context-sensitive. Despite\nunderpinning remarkably capable large language models, transformers are unable\nto model many context-free language tasks. In an attempt to address this\nlimitation in the modeling power of transformer-based language models, we\npropose augmenting them with a differentiable, stack-based attention mechanism.\nOur stack-based attention mechanism can be incorporated into any\ntransformer-based language model and adds a level of interpretability to the\nmodel. We show that the addition of our stack-based attention mechanism enables\nthe transformer to model some, but not all, deterministic context-free\nlanguages.",
    "authors": [
      "Jiaoda Li",
      "Jennifer C. White",
      "Mrinmaya Sachan",
      "Ryan Cotterell"
    ],
    "updated": "2024-05-13T18:56:18Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2410.15578",
    "title": "Generalized Probabilistic Attention Mechanism in Transformers",
    "link": "http://arxiv.org/abs/2410.15578v1",
    "pdf": "http://arxiv.org/pdf/2410.15578v1",
    "summary": "The Transformer architecture has become widely adopted due to its\ndemonstrated success, attributed to the attention mechanism at its core.\nDespite these successes, the attention mechanism of Transformers is associated\nwith two well-known issues: rank-collapse and gradient vanishing. In this\npaper, we present a theoretical analysis that it is inherently difficult to\naddress both issues simultaneously in the conventional attention mechanism. To\nhandle these issues, we introduce a novel class of attention mechanism,\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\ndual-attention implementation within the Transformer architecture. Unlike\nconventional attention mechanisms, GPAM allows for negative attention scores\nwhile preserving a fixed total sum. We provide theoretical evidence that the\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\nrank-collapse and gradient vanishing issues which are difficult to resolve\nsimultaneously with the conventional attention mechanisms. Furthermore, we\nempirically validate this theoretical evidence, demonstrating the superiority\nof daGPAM compared to other alternative attention mechanisms that were proposed\nto address the same issues. Additionally, we demonstrate the practical benefits\nof GPAM in natural language processing tasks, such as language modeling and\nneural machine translation.",
    "authors": [
      "DongNyeong Heo",
      "Heeyoul Choi"
    ],
    "updated": "2024-10-21T01:55:52Z",
    "categories": [
      "cs.LG",
      "cs.CL"
    ]
  },
  {
    "id": "2410.02703",
    "title": "Selective Attention Improves Transformer",
    "link": "http://arxiv.org/abs/2410.02703v2",
    "pdf": "http://arxiv.org/pdf/2410.02703v2",
    "summary": "Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention consistently improves language modeling and downstream task\nperformance in a variety of model sizes and context lengths. For example,\ntransformers trained with the language modeling objective on C4 with selective\nattention perform language modeling equivalently to standard transformers with\n~2X more heads and parameters in their attention modules. Selective attention\nalso allows decreasing the size of the attention's context buffer, leading to\nmeaningful reductions in the memory and compute requirements during inference.\nFor example, transformers trained on C4 with context sizes of 512, 1,024, and\n2,048 need 16X, 25X, and 47X less memory for their attention module,\nrespectively, when equipped with selective attention, as those without\nselective attention, with the same validation perplexity.",
    "authors": [
      "Yaniv Leviathan",
      "Matan Kalman",
      "Yossi Matias"
    ],
    "updated": "2025-04-24T02:44:54Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id": "2209.15176",
    "title": "Adaptive Sparse and Monotonic Attention for Transformer-based Automatic\n  Speech Recognition",
    "link": "http://arxiv.org/abs/2209.15176v1",
    "pdf": "http://arxiv.org/pdf/2209.15176v1",
    "summary": "The Transformer architecture model, based on self-attention and multi-head\nattention, has achieved remarkable success in offline end-to-end Automatic\nSpeech Recognition (ASR). However, self-attention and multi-head attention\ncannot be easily applied for streaming or online ASR. For self-attention in\nTransformer ASR, the softmax normalization function-based attention mechanism\nmakes it impossible to highlight important speech information. For multi-head\nattention in Transformer ASR, it is not easy to model monotonic alignments in\ndifferent heads. To overcome these two limits, we integrate sparse attention\nand monotonic attention into Transformer-based ASR. The sparse mechanism\nintroduces a learned sparsity scheme to enable each self-attention structure to\nfit the corresponding head better. The monotonic attention deploys\nregularization to prune redundant heads for the multi-head attention structure.\nThe experiments show that our method can effectively improve the attention\nmechanism on widely used benchmarks of speech recognition.",
    "authors": [
      "Chendong Zhao",
      "Jianzong Wang",
      "Wen qi Wei",
      "Xiaoyang Qu",
      "Haoqian Wang",
      "Jing Xiao"
    ],
    "updated": "2022-09-30T01:55:57Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2108.01778",
    "title": "Armour: Generalizable Compact Self-Attention for Vision Transformers",
    "link": "http://arxiv.org/abs/2108.01778v1",
    "pdf": "http://arxiv.org/pdf/2108.01778v1",
    "summary": "Attention-based transformer networks have demonstrated promising potential as\ntheir applications extend from natural language processing to vision. However,\ndespite the recent improvements, such as sub-quadratic attention approximation\nand various training enhancements, the compact vision transformers to date\nusing the regular attention still fall short in comparison with its convnet\ncounterparts, in terms of \\textit{accuracy,} \\textit{model size}, \\textit{and}\n\\textit{throughput}. This paper introduces a compact self-attention mechanism\nthat is fundamental and highly generalizable. The proposed method reduces\nredundancy and improves efficiency on top of the existing attention\noptimizations. We show its drop-in applicability for both the regular attention\nmechanism and some most recent variants in vision transformers. As a result, we\nproduced smaller and faster models with the same or better accuracies.",
    "authors": [
      "Lingchuan Meng"
    ],
    "updated": "2021-08-03T22:33:58Z",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "1906.09777",
    "title": "A Tensorized Transformer for Language Modeling",
    "link": "http://arxiv.org/abs/1906.09777v3",
    "pdf": "http://arxiv.org/pdf/1906.09777v3",
    "summary": "Latest development of neural models has connected the encoder and decoder\nthrough a self-attention mechanism. In particular, Transformer, which is solely\nbased on self-attention, has led to breakthroughs in Natural Language\nProcessing (NLP) tasks. However, the multi-head attention mechanism, as a key\ncomponent of Transformer, limits the effective deployment of the model to a\nresource-limited setting. In this paper, based on the ideas of tensor\ndecomposition and parameters sharing, we propose a novel self-attention model\n(namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We\ntest and verify the proposed attention method on three language modeling tasks\n(i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task\n(i.e., WMT-2016 English-German). Multi-linear attention can not only largely\ncompress the model parameters but also obtain performance improvements,\ncompared with a number of language modeling approaches, such as Transformer,\nTransformer-XL, and Transformer with tensor train decomposition.",
    "authors": [
      "Xindian Ma",
      "Peng Zhang",
      "Shuai Zhang",
      "Nan Duan",
      "Yuexian Hou",
      "Dawei Song",
      "Ming Zhou"
    ],
    "updated": "2019-11-06T13:53:14Z",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id": "1907.06607",
    "title": "Agglomerative Attention",
    "link": "http://arxiv.org/abs/1907.06607v1",
    "pdf": "http://arxiv.org/pdf/1907.06607v1",
    "summary": "Neural networks using transformer-based architectures have recently\ndemonstrated great power and flexibility in modeling sequences of many types.\nOne of the core components of transformer networks is the attention layer,\nwhich allows contextual information to be exchanged among sequence elements.\nWhile many of the prevalent network structures thus far have utilized full\nattention -- which operates on all pairs of sequence elements -- the quadratic\nscaling of this attention mechanism significantly constrains the size of models\nthat can be trained. In this work, we present an attention model that has only\nlinear requirements in memory and computation time. We show that, despite the\nsimpler attention model, networks using this attention mechanism can attain\ncomparable performance to full attention networks on language modeling tasks.",
    "authors": [
      "Matthew Spellings"
    ],
    "updated": "2019-07-15T17:11:05Z",
    "categories": [
      "cs.LG",
      "stat.ML"
    ]
  },
  {
    "id": "2009.07053",
    "title": "Attention Flows: Analyzing and Comparing Attention Mechanisms in\n  Language Models",
    "link": "http://arxiv.org/abs/2009.07053v1",
    "pdf": "http://arxiv.org/pdf/2009.07053v1",
    "summary": "Advances in language modeling have led to the development of deep\nattention-based models that are performant across a wide variety of natural\nlanguage processing (NLP) problems. These language models are typified by a\npre-training process on large unlabeled text corpora and subsequently\nfine-tuned for specific tasks. Although considerable work has been devoted to\nunderstanding the attention mechanisms of pre-trained models, it is less\nunderstood how a model's attention mechanisms change when trained for a target\nNLP task. In this paper, we propose a visual analytics approach to\nunderstanding fine-tuning in attention-based language models. Our\nvisualization, Attention Flows, is designed to support users in querying,\ntracing, and comparing attention within layers, across layers, and amongst\nattention heads in Transformer-based language models. To help users gain\ninsight on how a classification decision is made, our design is centered on\ndepicting classification-based attention at the deepest layer and how attention\nfrom prior layers flows throughout words in the input. Attention Flows supports\nthe analysis of a single model, as well as the visual comparison between\npre-trained and fine-tuned models via their similarities and differences. We\nuse Attention Flows to study attention mechanisms in various sentence\nunderstanding tasks and highlight how attention evolves to address the nuances\nof solving these tasks.",
    "authors": [
      "Joseph F DeRose",
      "Jiayao Wang",
      "Matthew Berger"
    ],
    "updated": "2020-09-03T19:56:30Z",
    "categories": [
      "cs.HC",
      "cs.CL"
    ]
  },
  {
    "id": "2204.04477",
    "title": "FoundationLayerNorm: Scaling BERT and GPT to 1,000 Layers",
    "link": "http://arxiv.org/abs/2204.04477v1",
    "pdf": "http://arxiv.org/pdf/2204.04477v1",
    "summary": "The mainstream BERT/GPT model contains only 10 to 20 layers, and there is\nlittle literature to discuss the training of deep BERT/GPT. This paper proposes\na simple yet effective method to stabilize BERT and GPT training. We\nsuccessfully scale up BERT and GPT to 1,000 layers, which is an order of\nmagnitude deeper than previous BERT and GPT. The proposed method\nFoundationLayerNormalization enables efficient training of deep neural networks\nand is validated at the 1000-layer scale.",
    "authors": [
      "Dezhou Shen"
    ],
    "updated": "2022-04-09T14:03:28Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ]
  },
  {
    "id": "2108.07789",
    "title": "Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition",
    "link": "http://arxiv.org/abs/2108.07789v2",
    "pdf": "http://arxiv.org/pdf/2108.07789v2",
    "summary": "Language models (LMs) pre-trained on massive amounts of text, in particular\nbidirectional encoder representations from Transformers (BERT), generative\npre-training (GPT), and GPT-2, have become a key technology for many natural\nlanguage processing tasks. In this paper, we present results using fine-tuned\nGPT, GPT-2, and their combination for automatic speech recognition (ASR).\nUnlike unidirectional LM GPT and GPT-2, BERT is bidirectional whose direct\nproduct of the output probabilities is no longer a valid language prior\nprobability. A conversion method is proposed to compute the correct language\nprior probability based on bidirectional LM outputs in a mathematically exact\nway. Experimental results on the widely used AMI and Switchboard ASR tasks\nshowed that the combination of the fine-tuned GPT and GPT-2 outperformed the\ncombination of three neural LMs with different architectures trained from\nscratch on the in-domain text by up to a 12% relative word error rate reduction\n(WERR). Furthermore, on the AMI corpus, the proposed conversion for language\nprior probabilities enables BERT to obtain an extra 3% relative WERR, and the\ncombination of BERT, GPT and GPT-2 results in further improvements.",
    "authors": [
      "Xianrui Zheng",
      "Chao Zhang",
      "Philip C. Woodland"
    ],
    "updated": "2021-10-01T14:19:39Z",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ]
  },
  {
    "id": "2401.14040",
    "title": "(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection",
    "link": "http://arxiv.org/abs/2401.14040v3",
    "pdf": "http://arxiv.org/pdf/2401.14040v3",
    "summary": "In the universe of Natural Language Processing, Transformer-based language\nmodels like BERT and (Chat)GPT have emerged as lexical superheroes with great\npower to solve open research problems. In this paper, we specifically focus on\nthe temporal problem of semantic change, and evaluate their ability to solve\ntwo diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and\nHistoWiC. In particular, we investigate the potential of a novel, off-the-shelf\ntechnology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a\nfamily of models that currently stand as the state-of-the-art for modeling\nsemantic change. Our experiments represent the first attempt to assess the use\nof (Chat)GPT for studying semantic change. Our results indicate that ChatGPT\nperforms significantly worse than the foundational GPT version. Furthermore,\nour results demonstrate that (Chat)GPT achieves slightly lower performance than\nBERT in detecting long-term changes but performs significantly worse in\ndetecting short-term changes.",
    "authors": [
      "Francesco Periti",
      "Haim Dubossarsky",
      "Nina Tahmasebi"
    ],
    "updated": "2024-04-29T07:07:45Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2408.16749",
    "title": "Assessing Large Language Models for Online Extremism Research:\n  Identification, Explanation, and New Knowledge",
    "link": "http://arxiv.org/abs/2408.16749v1",
    "pdf": "http://arxiv.org/pdf/2408.16749v1",
    "summary": "The United States has experienced a significant increase in violent\nextremism, prompting the need for automated tools to detect and limit the\nspread of extremist ideology online. This study evaluates the performance of\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformers (GPT) in detecting and classifying online domestic\nextremist posts. We collected social media posts containing \"far-right\" and\n\"far-left\" ideological keywords and manually labeled them as extremist or\nnon-extremist. Extremist posts were further classified into one or more of five\ncontributing elements of extremism based on a working definitional framework.\nThe BERT model's performance was evaluated based on training data size and\nknowledge transfer between categories. We also compared the performance of GPT\n3.5 and GPT 4 models using different prompts: na\\\"ive, layperson-definition,\nrole-playing, and professional-definition. Results showed that the best\nperforming GPT models outperformed the best performing BERT models, with more\ndetailed prompts generally yielding better results. However, overly complex\nprompts may impair performance. Different versions of GPT have unique\nsensitives to what they consider extremist. GPT 3.5 performed better at\nclassifying far-left extremist posts, while GPT 4 performed better at\nclassifying far-right extremist posts. Large language models, represented by\nGPT models, hold significant potential for online extremism classification\ntasks, surpassing traditional BERT models in a zero-shot setting. Future\nresearch should explore human-computer interactions in optimizing GPT models\nfor extremist detection and classification tasks to develop more efficient\n(e.g., quicker, less effort) and effective (e.g., fewer errors or mistakes)\nmethods for identifying extremist content.",
    "authors": [
      "Beidi Dong",
      "Jin R. Lee",
      "Ziwei Zhu",
      "Balassubramanian Srinivasan"
    ],
    "updated": "2024-08-29T17:43:03Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2405.12990",
    "title": "BERT vs GPT for financial engineering",
    "link": "http://arxiv.org/abs/2405.12990v1",
    "pdf": "http://arxiv.org/pdf/2405.12990v1",
    "summary": "The paper benchmarks several Transformer models [4], to show how these models\ncan judge sentiment from a news event. This signal can then be used for\ndownstream modelling and signal identification for commodity trading. We find\nthat fine-tuned BERT models outperform fine-tuned or vanilla GPT models on this\ntask. Transformer models have revolutionized the field of natural language\nprocessing (NLP) in recent years, achieving state-of-the-art results on various\ntasks such as machine translation, text summarization, question answering, and\nnatural language generation. Among the most prominent transformer models are\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-trained Transformer (GPT), which differ in their architectures and\nobjectives.\n  A CopBERT model training data and process overview is provided. The CopBERT\nmodel outperforms similar domain specific BERT trained models such as FinBERT.\nThe below confusion matrices show the performance on CopBERT & CopGPT\nrespectively. We see a ~10 percent increase in f1_score when compare CopBERT vs\nGPT4 and 16 percent increase vs CopGPT. Whilst GPT4 is dominant It highlights\nthe importance of considering alternatives to GPT models for financial\nengineering tasks, given risks of hallucinations, and challenges with\ninterpretability. We unsurprisingly see the larger LLMs outperform the BERT\nmodels, with predictive power. In summary BERT is partially the new XGboost,\nwhat it lacks in predictive power it provides with higher levels of\ninterpretability. Concluding that BERT models might not be the next XGboost\n[2], but represent an interesting alternative for financial engineering tasks,\nthat require a blend of interpretability and accuracy.",
    "authors": [
      "Edward Sharkey",
      "Philip Treleaven"
    ],
    "updated": "2024-04-24T11:30:04Z",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id": "1909.02209",
    "title": "Semantics-aware BERT for Language Understanding",
    "link": "http://arxiv.org/abs/1909.02209v3",
    "pdf": "http://arxiv.org/pdf/1909.02209v3",
    "summary": "The latest work on language representations carefully integrates\ncontextualized features into language model training, which enables a series of\nsuccess especially in various machine reading comprehension and natural\nlanguage inference tasks. However, the existing language representation models\nincluding ELMo, GPT and BERT only exploit plain context-sensitive features such\nas character or word embeddings. They rarely consider incorporating structured\nsemantic information which can provide rich semantics for language\nrepresentation. To promote natural language understanding, we propose to\nincorporate explicit contextual semantics from pre-trained semantic role\nlabeling, and introduce an improved language representation model,\nSemantics-aware BERT (SemBERT), which is capable of explicitly absorbing\ncontextual semantics over a BERT backbone. SemBERT keeps the convenient\nusability of its BERT precursor in a light fine-tuning way without substantial\ntask-specific modifications. Compared with BERT, semantics-aware BERT is as\nsimple in concept but more powerful. It obtains new state-of-the-art or\nsubstantially improves results on ten reading comprehension and language\ninference tasks.",
    "authors": [
      "Zhuosheng Zhang",
      "Yuwei Wu",
      "Hai Zhao",
      "Zuchao Li",
      "Shuailiang Zhang",
      "Xi Zhou",
      "Xiang Zhou"
    ],
    "updated": "2020-02-04T09:43:22Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2410.24159",
    "title": "GPT or BERT: why not both?",
    "link": "http://arxiv.org/abs/2410.24159v2",
    "pdf": "http://arxiv.org/pdf/2410.24159v2",
    "summary": "We present a simple way to merge masked language modeling with causal\nlanguage modeling. This hybrid training objective results in a model that\ncombines the strengths of both modeling paradigms within a single transformer\nstack: GPT-BERT can be transparently used like any standard causal or masked\nlanguage model. We test the pretraining process that enables this flexible\nbehavior on the BabyLM Challenge 2024. The results show that the hybrid\npretraining outperforms masked-only or causal-only models. We openly release\nthe models, training corpora and code.",
    "authors": [
      "Lucas Georges Gabriel Charpentier",
      "David Samuel"
    ],
    "updated": "2024-12-29T18:00:05Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2411.05050",
    "title": "Selecting Between BERT and GPT for Text Classification in Political\n  Science Research",
    "link": "http://arxiv.org/abs/2411.05050v1",
    "pdf": "http://arxiv.org/pdf/2411.05050v1",
    "summary": "Political scientists often grapple with data scarcity in text classification.\nRecently, fine-tuned BERT models and their variants have gained traction as\neffective solutions to address this issue. In this study, we investigate the\npotential of GPT-based models combined with prompt engineering as a viable\nalternative. We conduct a series of experiments across various classification\ntasks, differing in the number of classes and complexity, to evaluate the\neffectiveness of BERT-based versus GPT-based models in low-data scenarios. Our\nfindings indicate that while zero-shot and few-shot learning with GPT models\nprovide reasonable performance and are well-suited for early-stage research\nexploration, they generally fall short - or, at best, match - the performance\nof BERT fine-tuning, particularly as the training set reaches a substantial\nsize (e.g., 1,000 samples). We conclude by comparing these approaches in terms\nof performance, ease of use, and cost, providing practical guidance for\nresearchers facing data limitations. Our results are particularly relevant for\nthose engaged in quantitative text analysis in low-resource settings or with\nlimited labeled data.",
    "authors": [
      "Yu Wang",
      "Wen Qu",
      "Xin Ye"
    ],
    "updated": "2024-11-07T07:29:39Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2409.00096",
    "title": "Non-instructional Fine-tuning: Enabling Instruction-Following\n  Capabilities in Pre-trained Language Models without Instruction-Following\n  Data",
    "link": "http://arxiv.org/abs/2409.00096v1",
    "pdf": "http://arxiv.org/pdf/2409.00096v1",
    "summary": "Instruction fine-tuning is crucial for today's large language models (LLMs)\nto learn to follow instructions and align with human preferences.\nConventionally, supervised data, including the instruction and the correct\nresponse, is required for instruction fine-tuning. To obtain such data, some\nresearchers prompted well-trained models like GPT-4 to generate instructions\nand correct responses. In this paper, we propose a novel approach that uses the\nfirst half of a random text from OpenWebText as the instruction and\nGPT-3.5-turbo or GPT-4-turbo to complete the text as the response. Despite the\ndata being \"non-instructional\", we found that pre-trained LLMs fine-tuned on\nthis data can gain instruction-following capabilities. This observation is\nverified by fine-tuning several well-known pre-trained LLMs (e.g., LLaMA-2-7B,\nLLaMA-3-8B, LLaMA-3-70B, Mistral-7B-v0.1). The \"non-instructional data\" also\nimproved some models that underwent supervised fine-tuning and human preference\nalignment. Our LLaMA-3-70B-Instruct fine-tuned through \"non-instructional data\"\nis comparable with LLaMA-3.1-70B-Instruct on the Arena Hard leaderboard. We\nanalyzed the \"non-instructional data\" and ensured it is devoid of content\nrelated to instruction fine-tuning. Our findings will inspire further\ninvestigation into how to develop instruction-following capabilities without\nexplicit instruction-related data.",
    "authors": [
      "Juncheng Xie",
      "Shensian Syu",
      "Hung-yi Lee"
    ],
    "updated": "2024-08-27T01:21:53Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id": "2410.10739",
    "title": "Balancing Continuous Pre-Training and Instruction Fine-Tuning:\n  Optimizing Instruction-Following in LLMs",
    "link": "http://arxiv.org/abs/2410.10739v1",
    "pdf": "http://arxiv.org/pdf/2410.10739v1",
    "summary": "Large Language Models (LLMs) for public use require continuous pre-training\nto remain up-to-date with the latest data. The models also need to be\nfine-tuned with specific instructions to maintain their ability to follow\ninstructions accurately. Typically, LLMs are released in two versions: the Base\nLLM, pre-trained on diverse data, and the instruction-refined LLM, additionally\ntrained with specific instructions for better instruction following. The\nquestion arises as to which model should undergo continuous pre-training to\nmaintain its instruction-following abilities while also staying current with\nthe latest data. In this study, we delve into the intricate relationship\nbetween continuous pre-training and instruction fine-tuning of the LLMs and\ninvestigate the impact of continuous pre-training on the instruction following\nabilities of both the base and its instruction finetuned model. Further, the\ninstruction fine-tuning process is computationally intense and requires a\nsubstantial number of hand-annotated examples for the model to learn\neffectively. This study aims to find the most compute-efficient strategy to\ngain up-to-date knowledge and instruction-following capabilities without\nrequiring any instruction data and fine-tuning. We empirically prove our\nfindings on the LLaMa 3, 3.1 and Qwen 2, 2.5 family of base and instruction\nmodels, providing a comprehensive exploration of our hypotheses across varying\nsizes of pre-training data corpus and different LLMs settings.",
    "authors": [
      "Ishan Jindal",
      "Chandana Badrinath",
      "Pranjal Bharti",
      "Lakkidi Vinay",
      "Sachin Dev Sharma"
    ],
    "updated": "2024-10-14T17:20:30Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2501.11463",
    "title": "Curiosity-Driven Reinforcement Learning from Human Feedback",
    "link": "http://arxiv.org/abs/2501.11463v2",
    "pdf": "http://arxiv.org/pdf/2501.11463v2",
    "summary": "Reinforcement learning from human feedback (RLHF) has proven effective in\naligning large language models (LLMs) with human preferences, but often at the\ncost of reduced output diversity. This trade-off between diversity and\nalignment quality remains a significant challenge. Drawing inspiration from\ncuriosity-driven exploration in reinforcement learning, we introduce\ncuriosity-driven RLHF (CD-RLHF), a framework that incorporates intrinsic\nrewards for novel states, alongside traditional sparse extrinsic rewards, to\noptimize both output diversity and alignment quality. We demonstrate the\neffectiveness of CD-RLHF through extensive experiments on a range of tasks,\nincluding text summarization and instruction following. Our approach achieves\nsignificant gains in diversity on multiple diversity-oriented metrics while\nmaintaining alignment with human preferences comparable to standard RLHF. We\nmake our code publicly available at https://github.com/ernie-research/CD-RLHF.",
    "authors": [
      "Haoran Sun",
      "Yekun Chai",
      "Shuohuan Wang",
      "Yu Sun",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "updated": "2025-05-31T16:08:55Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2509.00309",
    "title": "Balanced Actor Initialization: Stable RLHF Training of\n  Distillation-Based Reasoning Models",
    "link": "http://arxiv.org/abs/2509.00309v1",
    "pdf": "http://arxiv.org/pdf/2509.00309v1",
    "summary": "The development of alignment and reasoning capabilities in large language\nmodels has seen remarkable progress through two paradigms: instruction tuning\nand reinforcement learning from human feedback (RLHF) alignment paradigm, and\ndistillation-based reasoning fine-tuning paradigm. While both approaches prove\neffective independently, the third paradigm of applying RLHF to\ndistillation-trained models presents significant challenges. Our investigation\nreveals two critical phenomena that emerge in this paradigm: Sequence Length\nCollapse, where language generation dramatically reduces during early RLHF\ntraining, and the Reward Hockey Stick Curve, featuring severe reward score\ndrops followed by gradual recovery. These instabilities fundamentally\ncompromise the model's alignment and reasoning capabilities. To address these\nchallenges, we propose Balanced Actor Initialization (BAI), a two-stage\nweighted model merging approach. BAI first merges instruction-following and\ndistillation-based reasoning fine-tuned models, then further combines this\nintermediate model with the pretrained model to preserve foundational\nknowledge. Through comprehensive experiments across diverse benchmarks and\ndetailed analysis of training experiments, we demonstrate that BAI resolves\nSequence Length Collapse, mitigates the Reward Hockey Stick Curve, and enables\ncontinuous sequence length improvement during training. Additionally, our\nanalysis reveals that balanced merging ratios achieve optimal trade-offs\nbetween training stability and reasoning capability preservation. Our work\nprovides the effective solution for stable training in this third paradigm,\nenabling more capable reasoning models that combine distillation efficiency\nwith RLHF alignment.",
    "authors": [
      "Chen Zheng",
      "Yiyuan Ma",
      "Yuan Yang",
      "Deyi Liu",
      "Jing Liu",
      "Zuquan Song",
      "Yuxin Song",
      "Cheng Ren",
      "Hang Zhu",
      "Xin Liu",
      "Yiyuan Ma",
      "Siyuan Qiao",
      "Xun Zhou",
      "Liang Xiang",
      "Yonghui Wu"
    ],
    "updated": "2025-08-30T01:53:25Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2410.18252",
    "title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language\n  Models",
    "link": "http://arxiv.org/abs/2410.18252v3",
    "pdf": "http://arxiv.org/pdf/2410.18252v3",
    "summary": "The dominant paradigm for RLHF is online and on-policy RL: synchronously\ngenerating from the large language model (LLM) policy, labelling with a reward\nmodel, and learning using feedback on the LLM's own outputs. While performant,\nthis paradigm is computationally inefficient. Inspired by classical deep RL\nliterature, we propose separating generation and learning in RLHF. This enables\nasynchronous generation of new samples while simultaneously training on old\nsamples, leading to faster training and more compute-optimal scaling. However,\nasynchronous training relies on an underexplored regime, online but off-policy\nRLHF: learning on samples from previous iterations of our model which give a\nworse training signal. We tackle the fundamental challenge in this regime: how\nmuch off-policyness can we tolerate for asynchronous training to speed up\nlearning but maintain performance? Among several RLHF algorithms we test,\nonline DPO is found to be most robust to off-policy data, and robustness\nincreases with the scale of the policy model. We study further compute\noptimizations for asynchronous RLHF but find that they come at a performance\ncost, giving rise to a trade-off. We verify the scalability of asynchronous\nRLHF by training a general-purpose chatbot from LLaMA 3.1 8B on an\ninstruction-following task ~40% faster than a synchronous run while matching\nfinal performance. Finally, we extend our results to math and reasoning to\ndemonstrate asynchronous RL can finetune Rho 1B on GSM8k ~70% faster while\nmatching synchronous accuracy.",
    "authors": [
      "Michael Noukhovitch",
      "Shengyi Huang",
      "Sophie Xhonneux",
      "Arian Hosseini",
      "Rishabh Agarwal",
      "Aaron Courville"
    ],
    "updated": "2025-04-26T08:33:32Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id": "2310.06452",
    "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity",
    "link": "http://arxiv.org/abs/2310.06452v3",
    "pdf": "http://arxiv.org/pdf/2310.06452v3",
    "summary": "Large language models (LLMs) fine-tuned with reinforcement learning from\nhuman feedback (RLHF) have been used in some of the most widely deployed AI\nmodels to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has\nbeen significant work developing these methods, our understanding of the\nbenefits and downsides of each stage in RLHF is still limited. To fill this\ngap, we present an extensive analysis of how each stage of the process (i.e.\nsupervised fine-tuning (SFT), reward modelling, and RLHF) affects two key\nproperties: out-of-distribution (OOD) generalisation and output diversity. OOD\ngeneralisation is crucial given the wide range of real-world scenarios in which\nthese models are being used, while output diversity refers to the model's\nability to generate varied outputs and is important for a variety of use cases.\nWe perform our analysis across two base models on both summarisation and\ninstruction following tasks, the latter being highly relevant for current LLM\nuse cases. We find that RLHF generalises better than SFT to new inputs,\nparticularly as the distribution shift between train and test becomes larger.\nHowever, RLHF significantly reduces output diversity compared to SFT across a\nvariety of measures, implying a tradeoff in current LLM fine-tuning methods\nbetween generalisation and diversity. Our results provide guidance on which\nfine-tuning method should be used depending on the application, and show that\nmore research is needed to improve the tradeoff between generalisation and\ndiversity.",
    "authors": [
      "Robert Kirk",
      "Ishita Mediratta",
      "Christoforos Nalmpantis",
      "Jelena Luketina",
      "Eric Hambro",
      "Edward Grefenstette",
      "Roberta Raileanu"
    ],
    "updated": "2024-02-19T14:39:07Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id": "2406.13542",
    "title": "Self-play with Execution Feedback: Improving Instruction-following\n  Capabilities of Large Language Models",
    "link": "http://arxiv.org/abs/2406.13542v3",
    "pdf": "http://arxiv.org/pdf/2406.13542v3",
    "summary": "One core capability of large language models (LLMs) is to follow natural\nlanguage instructions. However, the issue of automatically constructing\nhigh-quality training data to enhance the complex instruction-following\nabilities of LLMs without manual annotation remains unresolved. In this paper,\nwe introduce AutoIF, the first scalable and reliable method for automatically\ngenerating instruction-following training data. AutoIF transforms the\nvalidation of instruction-following data quality into code verification,\nrequiring LLMs to generate instructions, the corresponding code to check the\ncorrectness of the instruction responses, and unit test samples to verify the\ncode's correctness. Then, execution feedback-based rejection sampling can\ngenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from\nHuman Feedback (RLHF) training. AutoIF achieves significant improvements across\nthree training algorithms, SFT, Offline DPO, and Online DPO, when applied to\nthe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and\nstrong-to-weak distillation settings. Our code is publicly available at\nhttps://github.com/QwenLM/AutoIF.",
    "authors": [
      "Guanting Dong",
      "Keming Lu",
      "Chengpeng Li",
      "Tingyu Xia",
      "Bowen Yu",
      "Chang Zhou",
      "Jingren Zhou"
    ],
    "updated": "2024-07-18T09:00:23Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ]
  },
  {
    "id": "2403.08694",
    "title": "TeaMs-RL: Teaching LLMs to Generate Better Instruction Datasets via\n  Reinforcement Learning",
    "link": "http://arxiv.org/abs/2403.08694v4",
    "pdf": "http://arxiv.org/pdf/2403.08694v4",
    "summary": "The development of Large Language Models (LLMs) often confronts challenges\nstemming from the heavy reliance on human annotators in the reinforcement\nlearning with human feedback (RLHF) framework, or the frequent and costly\nexternal queries tied to the self-instruct paradigm. In this work, we pivot to\nReinforcement Learning (RL) -- but with a twist. Diverging from the typical\nRLHF, which refines LLMs following instruction data training, we use RL to\ndirectly generate the foundational instruction dataset that alone suffices for\nfine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and\nrules, prioritizing the diversification of training datasets. It facilitates\nthe generation of high-quality data without excessive reliance on external\nadvanced models, paving the way for a single fine-tuning step and negating the\nneed for subsequent RLHF stages. Our findings highlight key advantages of our\napproach: reduced need for human involvement and fewer model queries (only\n5.73% of the strong baseline's total), along with enhanced capabilities of LLMs\nin crafting and comprehending complex instructions compared to strong\nbaselines, and substantially improved model privacy protection. Code is\navailable at the link: https://github.com/SafeRL-Lab/TeaMs-RL",
    "authors": [
      "Shangding Gu",
      "Alois Knoll",
      "Ming Jin"
    ],
    "updated": "2025-03-01T19:25:49Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2501.06480",
    "title": "Flash Window Attention: speedup the attention computation for Swin\n  Transformer",
    "link": "http://arxiv.org/abs/2501.06480v2",
    "pdf": "http://arxiv.org/pdf/2501.06480v2",
    "summary": "To address the high resolution of image pixels, the Swin Transformer\nintroduces window attention. This mechanism divides an image into\nnon-overlapping windows and restricts attention computation to within each\nwindow, significantly enhancing computational efficiency. To further optimize\nthis process, one might consider replacing standard attention with flash\nattention, which has proven to be more efficient in language models. However, a\ndirect substitution is ineffective. Flash attention is designed for long\nsequences, whereas window attention deals with shorter sequences but must\nhandle numerous of them in parallel. In this report, we present an optimized\nsolution called Flash Window Attention, tailored specifically for window\nattention. Flash Window Attention improves attention computation efficiency by\nup to 300% and enhances end-to-end runtime efficiency by up to 30%. Our code is\navailable online.",
    "authors": [
      "Zhendong Zhang"
    ],
    "updated": "2025-01-14T04:16:54Z",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2503.14376",
    "title": "Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM\n  Kernels",
    "link": "http://arxiv.org/abs/2503.14376v2",
    "pdf": "http://arxiv.org/pdf/2503.14376v2",
    "summary": "Linear RNNs with gating recently demonstrated competitive performance\ncompared to Transformers in language modeling. Although their linear compute\nscaling in sequence length offers theoretical runtime advantages over\nTransformers, realizing these benefits in practice requires optimized custom\nkernels, as Transformers rely on the highly efficient Flash Attention kernels\n(Dao, 2024). Leveraging the chunkwise-parallel formulation of linear RNNs,\nFlash Linear Attention (FLA) (Yang & Zhang, 2024) shows that linear RNN kernels\nare faster than Flash Attention, by parallelizing over chunks of the input\nsequence. However, since the chunk size of FLA is limited, many intermediate\nstates must be materialized in GPU memory. This leads to low arithmetic\nintensity and causes high memory consumption and IO cost, especially for\nlong-context pre-training. In this work, we present Tiled Flash Linear\nAttention (TFLA), a novel kernel algorithm for linear RNNs, that enables\narbitrary large chunk sizes and high arithmetic intensity by introducing an\nadditional level of sequence parallelization within each chunk. First, we apply\nTFLA to the xLSTM with matrix memory, the mLSTM (Beck et al., 2024). Second, we\npropose an mLSTM variant with sigmoid input gate and reduced computation for\neven faster kernel runtimes at equal language modeling performance. In our\nspeed benchmarks, we show that our new mLSTM kernels based on TFLA outperform\nhighly optimized Flash Attention, Linear Attention and Mamba kernels, setting a\nnew state of the art for efficient long-context sequence modeling primitives.",
    "authors": [
      "Maximilian Beck",
      "Korbinian PÃ¶ppel",
      "Phillip Lippe",
      "Sepp Hochreiter"
    ],
    "updated": "2025-05-10T08:07:13Z",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id": "2409.15097",
    "title": "Efficiently Dispatching Flash Attention For Partially Filled Attention\n  Masks",
    "link": "http://arxiv.org/abs/2409.15097v2",
    "pdf": "http://arxiv.org/pdf/2409.15097v2",
    "summary": "Transformers are widely used across various applications, many of which yield\nsparse or partially filled attention matrices. Examples include attention masks\ndesigned to reduce the quadratic complexity of attention, sequence packing\ntechniques, and recent innovations like tree masking for fast validation in\nMEDUSA. Despite the inherent sparsity in these matrices, the state-of-the-art\nalgorithm Flash Attention still processes them with quadratic complexity as\nthough they were dense. In this paper, we introduce Binary Block Masking, a\nhighly efficient modification that enhances Flash Attention by making it\nmask-aware. We further propose two optimizations: one tailored for masks with\ncontiguous non-zero patterns and another for extremely sparse masks. Our\nexperiments on attention masks derived from real-world scenarios demonstrate up\nto a 9x runtime improvement. The implementation will be publicly released to\nfoster further research and application.",
    "authors": [
      "Agniv Sharma",
      "Jonas Geiping"
    ],
    "updated": "2024-09-24T12:56:13Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id": "2409.10489",
    "title": "Flash STU: Fast Spectral Transform Units",
    "link": "http://arxiv.org/abs/2409.10489v5",
    "pdf": "http://arxiv.org/pdf/2409.10489v5",
    "summary": "Recent advances in state-space model architectures have shown great promise\nfor efficient sequence modeling, but challenges remain in balancing\ncomputational efficiency with model expressiveness. We propose the Flash STU\narchitecture, a hybrid model that interleaves spectral state space model layers\nwith sliding window attention, enabling scalability to billions of parameters\nfor language modeling while maintaining a near-linear time complexity. We\nevaluate the Flash STU and its variants on diverse sequence prediction tasks,\nincluding linear dynamical systems, robotics control, and language modeling. We\nfind that, given a fixed parameter budget, the Flash STU architecture\nconsistently outperforms the Transformer and other leading state-space models\nsuch as S4 and Mamba-2.",
    "authors": [
      "Y. Isabel Liu",
      "Windsor Nguyen",
      "Yagiz Devre",
      "Evan Dogariu",
      "Anirudha Majumdar",
      "Elad Hazan"
    ],
    "updated": "2025-09-06T14:21:01Z",
    "categories": [
      "cs.LG",
      "cs.AI"
    ]
  },
  {
    "id": "2503.06671",
    "title": "Emulating Self-attention with Convolution for Efficient Image\n  Super-Resolution",
    "link": "http://arxiv.org/abs/2503.06671v2",
    "pdf": "http://arxiv.org/pdf/2503.06671v2",
    "summary": "In this paper, we tackle the high computational overhead of Transformers for\nefficient image super-resolution~(SR). Motivated by the observations of\nself-attention's inter-layer repetition, we introduce a convolutionized\nself-attention module named Convolutional Attention~(ConvAttn) that emulates\nself-attention's long-range modeling capability and instance-dependent\nweighting with a single shared large kernel and dynamic kernels. By utilizing\nthe ConvAttn module, we significantly reduce the reliance on self-attention and\nits involved memory-bound operations while maintaining the representational\ncapability of Transformers. Furthermore, we overcome the challenge of\nintegrating flash attention into the lightweight SR regime, effectively\nmitigating self-attention's inherent memory bottleneck. We scale up the window\nsize to 32$\\times$32 with flash attention rather than proposing an intricate\nself-attention module, significantly improving PSNR by 0.31dB on\nUrban100$\\times$2 while reducing latency and memory usage by 16$\\times$ and\n12.2$\\times$. Building on these approaches, our proposed network, termed\nEmulating Self-attention with Convolution~(ESC), notably improves PSNR by 0.27\ndB on Urban100$\\times$4 compared to HiT-SRF, reducing the latency and memory\nusage by 3.7$\\times$ and 6.2$\\times$, respectively. Extensive experiments\ndemonstrate that our ESC maintains the ability for long-range modeling, data\nscalability, and the representational power of Transformers despite most\nself-attention being replaced by the ConvAttn module.",
    "authors": [
      "Dongheon Lee",
      "Seokju Yun",
      "Youngmin Ro"
    ],
    "updated": "2025-06-30T07:49:21Z",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2507.16676",
    "title": "Custom Algorithm-based Fault Tolerance for Attention Layers in\n  Transformers",
    "link": "http://arxiv.org/abs/2507.16676v1",
    "pdf": "http://arxiv.org/pdf/2507.16676v1",
    "summary": "Transformers and large language models (LLMs), powered by the attention\nmechanism, have transformed numerous AI applications, driving the need for\nspecialized hardware accelerators. A major challenge in these accelerators is\nefficiently detecting errors caused by random hardware faults. Traditional\nalgorithm-based fault tolerance (ABFT) techniques verify individual matrix\nmultiplications but fall short in handling the full attention mechanism,\nparticularly due to intermediate softmax normalization. This work proposes\nFlash-ABFT, a novel method that computes an online checksum across the entire\nthree-matrix product of query, key and value matrices, of an attention layer,\nincluding the softmax operation, with a single check. This approach\nsignificantly reduces overhead by eliminating redundant checks while\nmaintaining high fault-detection accuracy. Experimental results demonstrate\nthat Flash-ABFT incurs only 5.3% hardware area overhead and less than 1.9%\nenergy overhead, making it a cost-effective and robust solution for error\ndetection in attention accelerators.",
    "authors": [
      "Vasileios Titopoulos",
      "Kosmas Alexandridis",
      "Giorgos Dimitrakopoulos"
    ],
    "updated": "2025-07-22T15:11:13Z",
    "categories": [
      "cs.LG",
      "cs.AR"
    ]
  },
  {
    "id": "2412.02344",
    "title": "UniForm: A Reuse Attention Mechanism Optimized for Efficient Vision\n  Transformers on Edge Devices",
    "link": "http://arxiv.org/abs/2412.02344v1",
    "pdf": "http://arxiv.org/pdf/2412.02344v1",
    "summary": "Transformer-based architectures have demonstrated remarkable success across\nvarious domains, but their deployment on edge devices remains challenging due\nto high memory and computational demands. In this paper, we introduce a novel\nReuse Attention mechanism, tailored for efficient memory access and\ncomputational optimization, enabling seamless operation on resource-constrained\nplatforms without compromising performance. Unlike traditional multi-head\nattention (MHA), which redundantly computes separate attention matrices for\neach head, Reuse Attention consolidates these computations into a shared\nattention matrix, significantly reducing memory overhead and computational\ncomplexity. Comprehensive experiments on ImageNet-1K and downstream tasks show\nthat the proposed UniForm models leveraging Reuse Attention achieve\nstate-of-the-art imagenet classification accuracy while outperforming existing\nattention mechanisms, such as Linear Attention and Flash Attention, in\ninference speed and memory scalability. Notably, UniForm-l achieves a 76.7%\nTop-1 accuracy on ImageNet-1K with 21.8ms inference time on edge devices like\nthe Jetson AGX Orin, representing up to a 5x speedup over competing benchmark\nmethods. These results demonstrate the versatility of Reuse Attention across\nhigh-performance GPUs and edge platforms, paving the way for broader real-time\napplications",
    "authors": [
      "Seul-Ki Yeom",
      "Tae-Ho Kim"
    ],
    "updated": "2024-12-03T10:04:15Z",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2403.18276",
    "title": "RankMamba: Benchmarking Mamba's Document Ranking Performance in the Era\n  of Transformers",
    "link": "http://arxiv.org/abs/2403.18276v2",
    "pdf": "http://arxiv.org/pdf/2403.18276v2",
    "summary": "Transformer structure has achieved great success in multiple applied machine\nlearning communities, such as natural language processing (NLP), computer\nvision (CV) and information retrieval (IR). Transformer architecture's core\nmechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$\ntime complexity in inference. Many works have been proposed to improve the\nattention mechanism's scalability, such as Flash Attention and Multi-query\nAttention. A different line of work aims to design new mechanisms to replace\nattention. Recently, a notable model structure -- Mamba, which is based on\nstate space models, has achieved transformer-equivalent performance in multiple\nsequence modeling tasks.\n  In this work, we examine \\mamba's efficacy through the lens of a classical IR\ntask -- document ranking. A reranker model takes a query and a document as\ninput, and predicts a scalar relevance score. This task demands the language\nmodel's ability to comprehend lengthy contextual inputs and to capture the\ninteraction between query and document tokens. We find that (1) Mamba models\nachieve competitive performance compared to transformer-based models with the\nsame training recipe; (2) but also have a lower training throughput in\ncomparison to efficient transformer implementations such as flash attention. We\nhope this study can serve as a starting point to explore Mamba models in other\nclassical IR tasks. Our code implementation and trained checkpoints are made\npublic to facilitate reproducibility\n(https://github.com/zhichaoxu-shufe/RankMamba).",
    "authors": [
      "Zhichao Xu"
    ],
    "updated": "2024-04-07T06:44:28Z",
    "categories": [
      "cs.IR",
      "cs.CL"
    ]
  },
  {
    "id": "2503.23730",
    "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large\n  Vision-Language Models in the Korean Language",
    "link": "http://arxiv.org/abs/2503.23730v1",
    "pdf": "http://arxiv.org/pdf/2503.23730v1",
    "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA",
    "authors": [
      "Yoonshik Kim",
      "Jaeyoon Jung"
    ],
    "updated": "2025-03-31T05:04:25Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ]
  },
  {
    "id": "2504.09979",
    "title": "Resampling Benchmark for Efficient Comprehensive Evaluation of Large\n  Vision-Language Models",
    "link": "http://arxiv.org/abs/2504.09979v1",
    "pdf": "http://arxiv.org/pdf/2504.09979v1",
    "summary": "We propose an efficient evaluation protocol for large vision-language models\n(VLMs). Given their broad knowledge and reasoning capabilities, multiple\nbenchmarks are needed for comprehensive assessment, making evaluation\ncomputationally expensive. To improve efficiency, we construct a subset that\nyields results comparable to full benchmark evaluations. Our benchmark\nclassification experiments reveal that no single benchmark fully covers all\nchallenges. We then introduce a subset construction method using farthest point\nsampling (FPS). Our experiments show that FPS-based benchmarks maintain a\nstrong correlation (> 0.96) with full evaluations while using only ~1\\% of the\ndata. Additionally, applying FPS to an existing benchmark improves correlation\nwith overall evaluation results, suggesting its potential to reduce unintended\ndataset biases.",
    "authors": [
      "Teppei Suzuki",
      "Keisuke Ozawa"
    ],
    "updated": "2025-04-14T08:43:00Z",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2309.08448",
    "title": "Advancing the Evaluation of Traditional Chinese Language Models: Towards\n  a Comprehensive Benchmark Suite",
    "link": "http://arxiv.org/abs/2309.08448v2",
    "pdf": "http://arxiv.org/pdf/2309.08448v2",
    "summary": "The evaluation of large language models is an essential task in the field of\nlanguage understanding and generation. As language models continue to advance,\nthe need for effective benchmarks to assess their performance has become\nimperative. In the context of Traditional Chinese, there is a scarcity of\ncomprehensive and diverse benchmarks to evaluate the capabilities of language\nmodels, despite the existence of certain benchmarks such as DRCD, TTQA, CMDQA,\nand FGC dataset. To address this gap, we propose a novel set of benchmarks that\nleverage existing English datasets and are tailored to evaluate language models\nin Traditional Chinese. These benchmarks encompass a wide range of tasks,\nincluding contextual question-answering, summarization, classification, and\ntable understanding. The proposed benchmarks offer a comprehensive evaluation\nframework, enabling the assessment of language models' capabilities across\ndifferent tasks. In this paper, we evaluate the performance of GPT-3.5,\nTaiwan-LLaMa-v1.0, and Model 7-C, our proprietary model, on these benchmarks.\nThe evaluation results highlight that our model, Model 7-C, achieves\nperformance comparable to GPT-3.5 with respect to a part of the evaluated\ncapabilities. In an effort to advance the evaluation of language models in\nTraditional Chinese and stimulate further research in this field, we have\nopen-sourced our benchmark and opened the model for trial.",
    "authors": [
      "Chan-Jan Hsu",
      "Chang-Le Liu",
      "Feng-Ting Liao",
      "Po-Chun Hsu",
      "Yi-Chang Chen",
      "Da-shan Shiu"
    ],
    "updated": "2023-10-02T15:22:42Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2411.19244",
    "title": "Consolidating and Developing Benchmarking Datasets for the Nepali\n  Natural Language Understanding Tasks",
    "link": "http://arxiv.org/abs/2411.19244v2",
    "pdf": "http://arxiv.org/pdf/2411.19244v2",
    "summary": "The Nepali language has distinct linguistic features, especially its complex\nscript (Devanagari script), morphology, and various dialects,which pose a\nunique challenge for Natural Language Understanding (NLU) tasks. While the\nNepali Language Understanding Evaluation (Nep-gLUE) benchmark provides a\nfoundation for evaluating models, it remains limited in scope, covering four\ntasks. This restricts their utility for comprehensive assessments of Natural\nLanguage Processing (NLP) models. To address this limitation, we introduce\ntwelve new datasets, creating a new benchmark, the Nepali /Language\nUnderstanding Evaluation (NLUE) benchmark for evaluating the performance of\nmodels across a diverse set of Natural Language Understanding (NLU) tasks. The\nadded tasks include Single-Sentence Classification, Similarity and Paraphrase\nTasks, Natural Language Inference (NLI), and General Masked Evaluation Task\n(GMET). Through extensive experiments, we demonstrate that existing top models\nstruggle with the added complexity of these tasks. We also find that the best\nmultilingual model outperforms the best monolingual models across most tasks,\nhighlighting the need for more robust solutions tailored to the Nepali\nlanguage. This expanded benchmark sets a new standard for evaluating,\ncomparing, and advancing models, contributing significantly to the broader goal\nof advancing NLP research for low-resource languages.",
    "authors": [
      "Jinu Nyachhyon",
      "Mridul Sharma",
      "Prajwal Thapa",
      "Bal Krishna Bal"
    ],
    "updated": "2025-08-19T10:54:02Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2208.08227",
    "title": "MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural\n  Code Generation",
    "link": "http://arxiv.org/abs/2208.08227v4",
    "pdf": "http://arxiv.org/pdf/2208.08227v4",
    "summary": "Large language models have demonstrated the ability to generate both natural\nlanguage and programming language text. Such models open up the possibility of\nmulti-language code generation: could code generation models generalize\nknowledge from one language to another? Although contemporary code generation\nmodels can generate semantically correct Python code, little is known about\ntheir abilities with other languages. We propose MultiPL-E, a system for\ntranslating unit test-driven code generation benchmarks to new languages. We\ncreate the first massively multilingual code generation benchmark by using\nMultiPL-E to translate two popular Python code generation benchmarks to 18\nadditional programming languages.\n  We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18\nlanguages that encompass a range of programming paradigms and popularity. Using\nthese new parallel benchmarks, we evaluate the multi-language performance of\nthree state-of-the-art code generation models: Codex, CodeGen, and InCoder. We\nfind that Codex matches or even exceeds its performance on Python for several\nother languages. The range of programming languages represented in MultiPL-E\nallow us to explore the impact of language frequency and language features on\nmodel performance. Finally, the MultiPL-E approach of compiling code generation\nbenchmarks to new programming languages is both scalable and extensible, making\nit straightforward to evaluate new models, benchmarks, and languages.",
    "authors": [
      "Federico Cassano",
      "John Gouwar",
      "Daniel Nguyen",
      "Sydney Nguyen",
      "Luna Phipps-Costin",
      "Donald Pinckney",
      "Ming-Ho Yee",
      "Yangtian Zi",
      "Carolyn Jane Anderson",
      "Molly Q Feldman",
      "Arjun Guha",
      "Michael Greenberg",
      "Abhinav Jangda"
    ],
    "updated": "2022-12-19T10:30:12Z",
    "categories": [
      "cs.LG",
      "cs.PL"
    ]
  },
  {
    "id": "2404.05337",
    "title": "Towards Objectively Benchmarking Social Intelligence for Language Agents\n  at Action Level",
    "link": "http://arxiv.org/abs/2404.05337v1",
    "pdf": "http://arxiv.org/pdf/2404.05337v1",
    "summary": "Prominent large language models have exhibited human-level performance in\nmany domains, even enabling the derived agents to simulate human and social\ninteractions. While practical works have substantiated the practicability of\ngrounding language agents in sandbox simulation or embodied simulators, current\nsocial intelligence benchmarks either stay at the language level or use\nsubjective metrics. In pursuit of a more realistic and objective evaluation, we\nintroduce the Social Tasks in Sandbox Simulation (STSS) benchmark, which\nassesses language agents \\textbf{objectively} at the \\textbf{action level} by\nscrutinizing the goal achievements within the multi-agent simulation.\nAdditionally, we sample conversation scenarios to build a language-level\nbenchmark to provide an economically prudent preliminary evaluation and align\nwith prevailing benchmarks. To gauge the significance of agent architecture, we\nimplement a target-driven planning (TDP) module as an adjunct to the existing\nagent. Our evaluative findings highlight that the STSS benchmark is challenging\nfor state-of-the-art language agents. Furthermore, it effectively discriminates\nbetween distinct language agents, suggesting its usefulness as a benchmark for\nevaluating both language models and agent architectures.",
    "authors": [
      "Chenxu Wang",
      "Bin Dai",
      "Huaping Liu",
      "Baoyuan Wang"
    ],
    "updated": "2024-04-08T09:25:32Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2506.12148",
    "title": "Hatevolution: What Static Benchmarks Don't Tell Us",
    "link": "http://arxiv.org/abs/2506.12148v1",
    "pdf": "http://arxiv.org/pdf/2506.12148v1",
    "summary": "Language changes over time, including in the hate speech domain, which\nevolves quickly following social dynamics and cultural shifts. While NLP\nresearch has investigated the impact of language evolution on model training\nand has proposed several solutions for it, its impact on model benchmarking\nremains under-explored. Yet, hate speech benchmarks play a crucial role to\nensure model safety. In this paper, we empirically evaluate the robustness of\n20 language models across two evolving hate speech experiments, and we show the\ntemporal misalignment between static and time-sensitive evaluations. Our\nfindings call for time-sensitive linguistic benchmarks in order to correctly\nand reliably evaluate language models in the hate speech domain.",
    "authors": [
      "Chiara Di Bonaventura",
      "Barbara McGillivray",
      "Yulan He",
      "Albert MeroÃ±o-PeÃ±uela"
    ],
    "updated": "2025-06-13T18:08:19Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2410.16186",
    "title": "Contamination Report for Multilingual Benchmarks",
    "link": "http://arxiv.org/abs/2410.16186v1",
    "pdf": "http://arxiv.org/pdf/2410.16186v1",
    "summary": "Benchmark contamination refers to the presence of test datasets in Large\nLanguage Model (LLM) pre-training or post-training data. Contamination can lead\nto inflated scores on benchmarks, compromising evaluation results and making it\ndifficult to determine the capabilities of models. In this work, we study the\ncontamination of popular multilingual benchmarks in LLMs that support multiple\nlanguages. We use the Black Box test to determine whether $7$ frequently used\nmultilingual benchmarks are contaminated in $7$ popular open and closed LLMs\nand find that almost all models show signs of being contaminated with almost\nall the benchmarks we test. Our findings can help the community determine the\nbest set of benchmarks to use for multilingual evaluation.",
    "authors": [
      "Sanchit Ahuja",
      "Varun Gumma",
      "Sunayana Sitaram"
    ],
    "updated": "2024-10-21T16:49:35Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1606.02012",
    "title": "Can neural machine translation do simultaneous translation?",
    "link": "http://arxiv.org/abs/1606.02012v1",
    "pdf": "http://arxiv.org/pdf/1606.02012v1",
    "summary": "We investigate the potential of attention-based neural machine translation in\nsimultaneous translation. We introduce a novel decoding algorithm, called\nsimultaneous greedy decoding, that allows an existing neural machine\ntranslation model to begin translating before a full source sentence is\nreceived. This approach is unique from previous works on simultaneous\ntranslation in that segmentation and translation are done jointly to maximize\nthe translation quality and that translating each segment is strongly\nconditioned on all the previous segments. This paper presents a first step\ntoward building a full simultaneous translation system based on neural machine\ntranslation.",
    "authors": [
      "Kyunghyun Cho",
      "Masha Esipova"
    ],
    "updated": "2016-06-07T03:38:46Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1710.03743",
    "title": "Confidence through Attention",
    "link": "http://arxiv.org/abs/1710.03743v1",
    "pdf": "http://arxiv.org/pdf/1710.03743v1",
    "summary": "Attention distributions of the generated translations are a useful bi-product\nof attention-based recurrent neural network translation models and can be\ntreated as soft alignments between the input and output tokens. In this work,\nwe use attention distributions as a confidence metric for output translations.\nWe present two strategies of using the attention distributions: filtering out\nbad translations from a large back-translated corpus, and selecting the best\ntranslation in a hybrid setup of two different translation systems. While\nmanual evaluation indicated only a weak correlation between our confidence\nscore and human judgments, the use-cases showed improvements of up to 2.22 BLEU\npoints for filtering and 0.99 points for hybrid translation, tested on\nEnglish<->German and English<->Latvian translation.",
    "authors": [
      "MatÄ«ss Rikters",
      "Mark Fishel"
    ],
    "updated": "2017-10-10T17:47:41Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1702.03525",
    "title": "Learning to Parse and Translate Improves Neural Machine Translation",
    "link": "http://arxiv.org/abs/1702.03525v2",
    "pdf": "http://arxiv.org/pdf/1702.03525v2",
    "summary": "There has been relatively little attention to incorporating linguistic prior\nto neural machine translation. Much of the previous work was further\nconstrained to considering linguistic prior on the source side. In this paper,\nwe propose a hybrid model, called NMT+RNNG, that learns to parse and translate\nby combining the recurrent neural network grammar into the attention-based\nneural machine translation. Our approach encourages the neural machine\ntranslation model to incorporate linguistic prior during training, and lets it\ntranslate on its own afterward. Extensive experiments with four language pairs\nshow the effectiveness of the proposed NMT+RNNG.",
    "authors": [
      "Akiko Eriguchi",
      "Yoshimasa Tsuruoka",
      "Kyunghyun Cho"
    ],
    "updated": "2017-04-23T16:52:03Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2110.00435",
    "title": "Attention based Sequence to Sequence Learning for Machine Translation of\n  Low Resourced Indic Languages -- A case of Sanskrit to Hindi",
    "link": "http://arxiv.org/abs/2110.00435v1",
    "pdf": "http://arxiv.org/pdf/2110.00435v1",
    "summary": "Deep Learning techniques are powerful in mimicking humans in a particular set\nof problems. They have achieved a remarkable performance in complex learning\ntasks. Deep learning inspired Neural Machine Translation (NMT) is a proficient\ntechnique that outperforms traditional machine translation. Performing\nmachine-aided translation on Indic languages has always been a challenging task\nconsidering their rich and diverse grammar. The neural machine translation has\nshown quality results compared to the traditional machine translation\napproaches. The fully automatic machine translation becomes problematic when it\ncomes to low-resourced languages, especially with Sanskrit. This paper presents\nattention mechanism based neural machine translation by selectively focusing on\na particular part of language sentences during translation. The work shows the\nconstruction of Sanskrit to Hindi bilingual parallel corpus with nearly 10K\nsamples and having 178,000 tokens. The neural translation model equipped with\nan attention mechanism has been trained on Sanskrit to Hindi parallel corpus.\nThe approach has shown the significance of attention mechanisms to overcome\nlong-term dependencies, primarily associated with low resources Indic\nlanguages. The paper shows the attention plots on testing data to demonstrate\nthe alignment between source and translated words. For the evaluation of the\ntranslated sentences, manual score based human evaluation and automatic\nevaluation metric based techniques have been adopted. The attention mechanism\nbased neural translation has achieved 88% accuracy in human evaluation and a\nBLEU score of 0.92 on Sanskrit to Hindi translation.",
    "authors": [
      "Vishvajit Bakarola",
      "Jitendra Nasriwala"
    ],
    "updated": "2021-09-07T04:55:48Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "1709.07809",
    "title": "Neural Machine Translation",
    "link": "http://arxiv.org/abs/1709.07809v1",
    "pdf": "http://arxiv.org/pdf/1709.07809v1",
    "summary": "Draft of textbook chapter on neural machine translation. a comprehensive\ntreatment of the topic, ranging from introduction to neural networks,\ncomputation graphs, description of the currently dominant attentional\nsequence-to-sequence model, recent refinements, alternative architectures and\nchallenges. Written as chapter for the textbook Statistical Machine\nTranslation. Used in the JHU Fall 2017 class on machine translation.",
    "authors": [
      "Philipp Koehn"
    ],
    "updated": "2017-09-22T15:28:24Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1610.05011",
    "title": "Interactive Attention for Neural Machine Translation",
    "link": "http://arxiv.org/abs/1610.05011v1",
    "pdf": "http://arxiv.org/pdf/1610.05011v1",
    "summary": "Conventional attention-based Neural Machine Translation (NMT) conducts\ndynamic alignment in generating the target sentence. By repeatedly reading the\nrepresentation of source sentence, which keeps fixed after generated by the\nencoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced\nstate-of-the-art NMT. In this paper, we propose a new attention mechanism,\ncalled INTERACTIVE ATTENTION, which models the interaction between the decoder\nand the representation of source sentence during translation by both reading\nand writing operations. INTERACTIVE ATTENTION can keep track of the interaction\nhistory and therefore improve the translation performance. Experiments on NIST\nChinese-English translation task show that INTERACTIVE ATTENTION can achieve\nsignificant improvements over both the previous attention-based NMT baseline\nand some state-of-the-art variants of attention-based NMT (i.e., coverage\nmodels (Tu et al., 2016)). And neural machine translator with our INTERACTIVE\nATTENTION can outperform the open source attention-based NMT system Groundhog\nby 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU\npoints averagely on multiple test sets.",
    "authors": [
      "Fandong Meng",
      "Zhengdong Lu",
      "Hang Li",
      "Qun Liu"
    ],
    "updated": "2016-10-17T08:33:20Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1606.04164",
    "title": "Zero-Resource Translation with Multi-Lingual Neural Machine Translation",
    "link": "http://arxiv.org/abs/1606.04164v1",
    "pdf": "http://arxiv.org/pdf/1606.04164v1",
    "summary": "In this paper, we propose a novel finetuning algorithm for the recently\nintroduced multi-way, mulitlingual neural machine translate that enables\nzero-resource machine translation. When used together with novel many-to-one\ntranslation strategies, we empirically show that this finetuning algorithm\nallows the multi-way, multilingual model to translate a zero-resource language\npair (1) as well as a single-pair neural translation model trained with up to\n1M direct parallel sentences of the same language pair and (2) better than\npivot-based translation strategy, while keeping only one additional copy of\nattention-related parameters.",
    "authors": [
      "Orhan Firat",
      "Baskaran Sankaran",
      "Yaser Al-Onaizan",
      "Fatos T. Yarman Vural",
      "Kyunghyun Cho"
    ],
    "updated": "2016-06-13T22:40:33Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1708.05943",
    "title": "Neural Machine Translation with Extended Context",
    "link": "http://arxiv.org/abs/1708.05943v1",
    "pdf": "http://arxiv.org/pdf/1708.05943v1",
    "summary": "We investigate the use of extended context in attention-based neural machine\ntranslation. We base our experiments on translated movie subtitles and discuss\nthe effect of increasing the segments beyond single translation units. We study\nthe use of extended source language context as well as bilingual context\nextensions. The models learn to distinguish between information from different\nsegments and are surprisingly robust with respect to translation quality. In\nthis pilot study, we observe interesting cross-sentential attention patterns\nthat improve textual coherence in translation at least in some selected cases.",
    "authors": [
      "JÃ¶rg Tiedemann",
      "Yves Scherrer"
    ],
    "updated": "2017-08-20T09:31:49Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2010.11973",
    "title": "Rediscovering the Slavic Continuum in Representations Emerging from\n  Neural Models of Spoken Language Identification",
    "link": "http://arxiv.org/abs/2010.11973v1",
    "pdf": "http://arxiv.org/pdf/2010.11973v1",
    "summary": "Deep neural networks have been employed for various spoken language\nrecognition tasks, including tasks that are multilingual by definition such as\nspoken language identification. In this paper, we present a neural model for\nSlavic language identification in speech signals and analyze its emergent\nrepresentations to investigate whether they reflect objective measures of\nlanguage relatedness and/or non-linguists' perception of language similarity.\nWhile our analysis shows that the language representation space indeed captures\nlanguage relatedness to a great extent, we find perceptual confusability\nbetween languages in our study to be the best predictor of the language\nrepresentation similarity.",
    "authors": [
      "Badr M. Abdullah",
      "Jacek Kudera",
      "Tania Avgustinova",
      "Bernd MÃ¶bius",
      "Dietrich Klakow"
    ],
    "updated": "2020-10-22T18:18:19Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2205.10964",
    "title": "The Geometry of Multilingual Language Model Representations",
    "link": "http://arxiv.org/abs/2205.10964v2",
    "pdf": "http://arxiv.org/pdf/2205.10964v2",
    "summary": "We assess how multilingual language models maintain a shared multilingual\nrepresentation space while still encoding language-sensitive information in\neach language. Using XLM-R as a case study, we show that languages occupy\nsimilar linear subspaces after mean-centering, evaluated based on causal\neffects on language modeling performance and direct comparisons between\nsubspaces for 88 languages. The subspace means differ along language-sensitive\naxes that are relatively stable throughout middle layers, and these axes encode\ninformation such as token vocabularies. Shifting representations by language\nmeans is sufficient to induce token predictions in different languages.\nHowever, we also identify stable language-neutral axes that encode information\nsuch as token positions and part-of-speech. We visualize representations\nprojected onto language-sensitive and language-neutral axes, identifying\nlanguage family and part-of-speech clusters, along with spirals, toruses, and\ncurves representing token position information. These results demonstrate that\nmultilingual language models encode information along orthogonal\nlanguage-sensitive and language-neutral axes, allowing the models to extract a\nvariety of features for downstream tasks and cross-lingual transfer learning.",
    "authors": [
      "Tyler A. Chang",
      "Zhuowen Tu",
      "Benjamin K. Bergen"
    ],
    "updated": "2022-10-21T23:10:27Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1707.06519",
    "title": "Language Transfer of Audio Word2Vec: Learning Audio Segment\n  Representations without Target Language Data",
    "link": "http://arxiv.org/abs/1707.06519v1",
    "pdf": "http://arxiv.org/pdf/1707.06519v1",
    "summary": "Audio Word2Vec offers vector representations of fixed dimensionality for\nvariable-length audio segments using Sequence-to-sequence Autoencoder (SA).\nThese vector representations are shown to describe the sequential phonetic\nstructures of the audio segments to a good degree, with real world applications\nsuch as query-by-example Spoken Term Detection (STD). This paper examines the\ncapability of language transfer of Audio Word2Vec. We train SA from one\nlanguage (source language) and use it to extract the vector representation of\nthe audio segments of another language (target language). We found that SA can\nstill catch phonetic structure from the audio segments of the target language\nif the source and target languages are similar. In query-by-example STD, we\nobtain the vector representations from the SA learned from a large amount of\nsource language data, and found them surpass the representations from naive\nencoder and SA directly learned from a small amount of target language data.\nThe result shows that it is possible to learn Audio Word2Vec model from\nhigh-resource languages and use it on low-resource languages. This further\nexpands the usability of Audio Word2Vec.",
    "authors": [
      "Chia-Hao Shen",
      "Janet Y. Sung",
      "Hung-Yi Lee"
    ],
    "updated": "2017-07-19T10:54:00Z",
    "categories": [
      "cs.CL",
      "cs.LG"
    ]
  },
  {
    "id": "2504.11492",
    "title": "Language and Knowledge Representation: A Stratified Approach",
    "link": "http://arxiv.org/abs/2504.11492v1",
    "pdf": "http://arxiv.org/pdf/2504.11492v1",
    "summary": "The thesis proposes the problem of representation heterogeneity to emphasize\nthe fact that heterogeneity is an intrinsic property of any representation,\nwherein, different observers encode different representations of the same\ntarget reality in a stratified manner using different concepts, language and\nknowledge (as well as data). The thesis then advances a top-down solution\napproach to the above stratified problem of representation heterogeneity in\nterms of several solution components, namely: (i) a representation formalism\nstratified into concept level, language level, knowledge level and data level\nto accommodate representation heterogeneity, (ii) a top-down language\nrepresentation using Universal Knowledge Core (UKC), UKC namespaces and domain\nlanguages to tackle the conceptual and language level heterogeneity, (iii) a\ntop-down knowledge representation using the notions of language teleontology\nand knowledge teleontology to tackle the knowledge level heterogeneity, (iv)\nthe usage and further development of the existing LiveKnowledge catalog for\nenforcing iterative reuse and sharing of language and knowledge\nrepresentations, and, (v) the kTelos methodology integrating the solution\ncomponents above to iteratively generate the language and knowledge\nrepresentations absolving representation heterogeneity. The thesis also\nincludes proof-of-concepts of the language and knowledge representations\ndeveloped for two international research projects - DataScientia (data\ncatalogs) and JIDEP (materials modelling). Finally, the thesis concludes with\nfuture lines of research.",
    "authors": [
      "Mayukh Bagchi"
    ],
    "updated": "2025-04-14T20:18:10Z",
    "categories": [
      "cs.DB",
      "cs.CL",
      "cs.DL"
    ]
  },
  {
    "id": "2406.08092",
    "title": "Languages Transferred Within the Encoder: On Representation Transfer in\n  Zero-Shot Multilingual Translation",
    "link": "http://arxiv.org/abs/2406.08092v2",
    "pdf": "http://arxiv.org/pdf/2406.08092v2",
    "summary": "Understanding representation transfer in multilingual neural machine\ntranslation (MNMT) can reveal the reason for the zero-shot translation\ndeficiency. In this work, we systematically analyze the representational issue\nof MNMT models. We first introduce the identity pair, translating a sentence to\nitself, to address the lack of the base measure in multilingual investigations,\nas the identity pair can reflect the representation of a language within the\nmodel. Then, we demonstrate that the encoder transfers the source language to\nthe representational subspace of the target language instead of the\nlanguage-agnostic state. Thus, the zero-shot translation deficiency arises\nbecause the representation of a translation is entangled with other languages\nand not transferred to the target language effectively. Based on our findings,\nwe propose two methods: 1) low-rank language-specific embedding at the encoder,\nand 2) language-specific contrastive learning of the representation at the\ndecoder. The experimental results on Europarl-15, TED-19, and OPUS-100 datasets\nshow that our methods substantially enhance the performance of zero-shot\ntranslations without sacrifices in supervised directions by improving language\ntransfer capacity, thereby providing practical evidence to support our\nconclusions. Codes are available at https://github.com/zhiqu22/ZeroTrans.",
    "authors": [
      "Zhi Qu",
      "Chenchen Ding",
      "Taro Watanabe"
    ],
    "updated": "2025-04-08T03:39:51Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2409.18199",
    "title": "LangSAMP: Language-Script Aware Multilingual Pretraining",
    "link": "http://arxiv.org/abs/2409.18199v2",
    "pdf": "http://arxiv.org/pdf/2409.18199v2",
    "summary": "Recent multilingual pretrained language models (mPLMs) often avoid using\nlanguage embeddings -- learnable vectors assigned to individual languages.\nHowever, this places a significant burden on token representations to encode\nall language-specific information, which may hinder language neutrality. To\naddress this limitation, we propose Language-Script Aware Multilingual\nPretraining (LangSAMP), a method that incorporates both language and script\nembeddings to enhance representation learning. Specifically, we integrate these\nembeddings into the output of the Transformer blocks before passing the final\nrepresentations to the language modeling head for prediction. We apply LangSAMP\nto the continual pretraining of XLM-R on a highly multilingual corpus covering\nmore than 500 languages. The resulting model consistently outperforms the\nbaseline in zero-shot crosslingual transfer across diverse downstream tasks.\nExtensive analysis reveals that language and script embeddings capture\nlanguage- and script-specific nuances, which benefits more language-neutral\nrepresentations, proven by improved pairwise cosine similarity. In our case\nstudy, we also show that language and script embeddings can be used to select\nbetter source languages for crosslingual transfer. We make our code and models\npublicly available at https://github.com/cisnlp/LangSAMP.",
    "authors": [
      "Yihong Liu",
      "Haotian Ye",
      "Chunlan Ma",
      "Mingyang Wang",
      "Hinrich SchÃ¼tze"
    ],
    "updated": "2025-05-21T18:52:11Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "1902.09697",
    "title": "Polyglot Contextual Representations Improve Crosslingual Transfer",
    "link": "http://arxiv.org/abs/1902.09697v2",
    "pdf": "http://arxiv.org/pdf/1902.09697v2",
    "summary": "We introduce Rosita, a method to produce multilingual contextual word\nrepresentations by training a single language model on text from multiple\nlanguages. Our method combines the advantages of contextual word\nrepresentations with those of multilingual representation learning. We produce\nlanguage models from dissimilar language pairs (English/Arabic and\nEnglish/Chinese) and use them in dependency parsing, semantic role labeling,\nand named entity recognition, with comparisons to monolingual and\nnon-contextual variants. Our results provide further evidence for the benefits\nof polyglot learning, in which representations are shared across multiple\nlanguages.",
    "authors": [
      "Phoebe Mulcaire",
      "Jungo Kasai",
      "Noah A. Smith"
    ],
    "updated": "2019-03-18T21:00:13Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2404.12444",
    "title": "mOthello: When Do Cross-Lingual Representation Alignment and\n  Cross-Lingual Transfer Emerge in Multilingual Models?",
    "link": "http://arxiv.org/abs/2404.12444v1",
    "pdf": "http://arxiv.org/pdf/2404.12444v1",
    "summary": "Many pretrained multilingual models exhibit cross-lingual transfer ability,\nwhich is often attributed to a learned language-neutral representation during\npretraining. However, it remains unclear what factors contribute to the\nlearning of a language-neutral representation, and whether the learned\nlanguage-neutral representation suffices to facilitate cross-lingual transfer.\nWe propose a synthetic task, Multilingual Othello (mOthello), as a testbed to\ndelve into these two questions. We find that: (1) models trained with naive\nmultilingual pretraining fail to learn a language-neutral representation across\nall input languages; (2) the introduction of \"anchor tokens\" (i.e., lexical\nitems that are identical across languages) helps cross-lingual representation\nalignment; and (3) the learning of a language-neutral representation alone is\nnot sufficient to facilitate cross-lingual transfer. Based on our findings, we\npropose a novel approach - multilingual pretraining with unified output space -\nthat both induces the learning of language-neutral representation and\nfacilitates cross-lingual transfer.",
    "authors": [
      "Tianze Hua",
      "Tian Yun",
      "Ellie Pavlick"
    ],
    "updated": "2024-04-18T18:03:08Z",
    "categories": [
      "cs.CL",
      "cs.AI"
    ]
  },
  {
    "id": "2111.01673",
    "title": "Relational Self-Attention: What's Missing in Attention for Video\n  Understanding",
    "link": "http://arxiv.org/abs/2111.01673v1",
    "pdf": "http://arxiv.org/pdf/2111.01673v1",
    "summary": "Convolution has been arguably the most important feature transform for modern\nneural networks, leading to the advance of deep learning. Recent emergence of\nTransformer networks, which replace convolution layers with self-attention\nblocks, has revealed the limitation of stationary convolution kernels and\nopened the door to the era of dynamic feature transforms. The existing dynamic\ntransforms, including self-attention, however, are all limited for video\nunderstanding where correspondence relations in space and time, i.e., motion\ninformation, are crucial for effective representation. In this work, we\nintroduce a relational feature transform, dubbed the relational self-attention\n(RSA), that leverages rich structures of spatio-temporal relations in videos by\ndynamically generating relational kernels and aggregating relational contexts.\nOur experiments and ablation studies show that the RSA network substantially\noutperforms convolution and self-attention counterparts, achieving the state of\nthe art on the standard motion-centric benchmarks for video action recognition,\nsuch as Something-Something-V1 & V2, Diving48, and FineGym.",
    "authors": [
      "Manjin Kim",
      "Heeseung Kwon",
      "Chunyu Wang",
      "Suha Kwak",
      "Minsu Cho"
    ],
    "updated": "2021-11-02T15:36:11Z",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2502.10927",
    "title": "The underlying structures of self-attention: symmetry, directionality,\n  and emergent dynamics in Transformer training",
    "link": "http://arxiv.org/abs/2502.10927v2",
    "pdf": "http://arxiv.org/pdf/2502.10927v2",
    "summary": "Self-attention is essential to Transformer architectures, yet how information\nis embedded in the self-attention matrices and how different objective\nfunctions impact this process remains unclear. We present a mathematical\nframework to analyze self-attention matrices by deriving the structures\ngoverning their weight updates. Using this framework, we demonstrate that\nbidirectional training induces symmetry in the weight matrices, while\nautoregressive training results in directionality and column dominance. Our\ntheoretical findings are validated across multiple Transformer models -\nincluding ModernBERT, GPT, LLaMA3, and Mistral - and input modalities like\ntext, vision, and audio. Finally, we apply these insights by showing that\nsymmetric initialization improves the performance of encoder-only models on\nlanguage tasks. This mathematical analysis offers a novel theoretical\nperspective on how information is embedded through self-attention, thereby\nimproving the interpretability of Transformer models.",
    "authors": [
      "Matteo Saponati",
      "Pascal Sager",
      "Pau Vilimelis Aceituno",
      "Thilo Stadelmann",
      "Benjamin Grewe"
    ],
    "updated": "2025-06-03T10:08:01Z",
    "categories": [
      "cs.LG"
    ]
  },
  {
    "id": "2501.18322",
    "title": "A Unified Perspective on the Dynamics of Deep Transformers",
    "link": "http://arxiv.org/abs/2501.18322v1",
    "pdf": "http://arxiv.org/pdf/2501.18322v1",
    "summary": "Transformers, which are state-of-the-art in most machine learning tasks,\nrepresent the data as sequences of vectors called tokens. This representation\nis then exploited by the attention function, which learns dependencies between\ntokens and is key to the success of Transformers. However, the iterative\napplication of attention across layers induces complex dynamics that remain to\nbe fully understood. To analyze these dynamics, we identify each input sequence\nwith a probability measure and model its evolution as a Vlasov equation called\nTransformer PDE, whose velocity field is non-linear in the probability measure.\nOur first set of contributions focuses on compactly supported initial data. We\nshow the Transformer PDE is well-posed and is the mean-field limit of an\ninteracting particle system, thus generalizing and extending previous analysis\nto several variants of self-attention: multi-head attention, L2 attention,\nSinkhorn attention, Sigmoid attention, and masked attention--leveraging a\nconditional Wasserstein framework. In a second set of contributions, we are the\nfirst to study non-compactly supported initial conditions, by focusing on\nGaussian initial data. Again for different types of attention, we show that the\nTransformer PDE preserves the space of Gaussian measures, which allows us to\nanalyze the Gaussian case theoretically and numerically to identify typical\nbehaviors. This Gaussian analysis captures the evolution of data anisotropy\nthrough a deep Transformer. In particular, we highlight a clustering phenomenon\nthat parallels previous results in the non-normalized discrete case.",
    "authors": [
      "ValÃ©rie Castin",
      "Pierre Ablin",
      "JosÃ© Antonio Carrillo",
      "Gabriel PeyrÃ©"
    ],
    "updated": "2025-01-30T13:04:54Z",
    "categories": [
      "cs.LG",
      "math.AP",
      "35Q68 (Primary) 68T07, 35B40 (Secondary)"
    ]
  },
  {
    "id": "1905.09418",
    "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy\n  Lifting, the Rest Can Be Pruned",
    "link": "http://arxiv.org/abs/1905.09418v2",
    "pdf": "http://arxiv.org/pdf/1905.09418v2",
    "summary": "Multi-head self-attention is a key component of the Transformer, a\nstate-of-the-art architecture for neural machine translation. In this work we\nevaluate the contribution made by individual attention heads in the encoder to\nthe overall performance of the model and analyze the roles played by them. We\nfind that the most important and confident heads play consistent and often\nlinguistically-interpretable roles. When pruning heads using a method based on\nstochastic gates and a differentiable relaxation of the L0 penalty, we observe\nthat specialized heads are last to be pruned. Our novel pruning method removes\nthe vast majority of heads without seriously affecting performance. For\nexample, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads\nresults in a drop of only 0.15 BLEU.",
    "authors": [
      "Elena Voita",
      "David Talbot",
      "Fedor Moiseev",
      "Rico Sennrich",
      "Ivan Titov"
    ],
    "updated": "2019-06-07T14:00:58Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2106.05505",
    "title": "Convolutions and Self-Attention: Re-interpreting Relative Positions in\n  Pre-trained Language Models",
    "link": "http://arxiv.org/abs/2106.05505v1",
    "pdf": "http://arxiv.org/pdf/2106.05505v1",
    "summary": "In this paper, we detail the relationship between convolutions and\nself-attention in natural language tasks. We show that relative position\nembeddings in self-attention layers are equivalent to recently-proposed dynamic\nlightweight convolutions, and we consider multiple new ways of integrating\nconvolutions into Transformer self-attention. Specifically, we propose\ncomposite attention, which unites previous relative position embedding methods\nunder a convolutional framework. We conduct experiments by training BERT with\ncomposite attention, finding that convolutions consistently improve performance\non multiple downstream tasks, replacing absolute position embeddings. To inform\nfuture work, we present results comparing lightweight convolutions, dynamic\nconvolutions, and depthwise-separable convolutions in language model\npre-training, considering multiple injection points for convolutions in\nself-attention layers.",
    "authors": [
      "Tyler A. Chang",
      "Yifan Xu",
      "Weijian Xu",
      "Zhuowen Tu"
    ],
    "updated": "2021-06-10T05:11:35Z",
    "categories": [
      "cs.CL"
    ]
  },
  {
    "id": "2410.05762",
    "title": "Guided Self-attention: Find the Generalized Necessarily Distinct Vectors\n  for Grain Size Grading",
    "link": "http://arxiv.org/abs/2410.05762v1",
    "pdf": "http://arxiv.org/pdf/2410.05762v1",
    "summary": "With the development of steel materials, metallographic analysis has become\nincreasingly important. Unfortunately, grain size analysis is a manual process\nthat requires experts to evaluate metallographic photographs, which is\nunreliable and time-consuming. To resolve this problem, we propose a novel\nclassifi-cation method based on deep learning, namely GSNets, a family of\nhybrid models which can effectively introduce guided self-attention for\nclassifying grain size. Concretely, we build our models from three insights:(1)\nIntroducing our novel guided self-attention module can assist the model in\nfinding the generalized necessarily distinct vectors capable of retaining\nintricate rela-tional connections and rich local feature information; (2) By\nimproving the pixel-wise linear independence of the feature map, the highly\ncondensed semantic representation will be captured by the model; (3) Our novel\ntriple-stream merging module can significantly improve the generalization\ncapability and efficiency of the model. Experiments show that our GSNet yields\na classifi-cation accuracy of 90.1%, surpassing the state-of-the-art Swin\nTransformer V2 by 1.9% on the steel grain size dataset, which comprises 3,599\nimages with 14 grain size levels. Furthermore, we intuitively believe our\napproach is applicable to broader ap-plications like object detection and\nsemantic segmentation.",
    "authors": [
      "Fang Gao",
      "Xuetao Li",
      "Jiabao Wang",
      "Shengheng Ma",
      "Jun Yu"
    ],
    "updated": "2024-10-08T07:40:31Z",
    "categories": [
      "cs.CV"
    ]
  },
  {
    "id": "2503.00687",
    "title": "Transformer Meets Twicing: Harnessing Unattended Residual Information",
    "link": "http://arxiv.org/abs/2503.00687v3",
    "pdf": "http://arxiv.org/pdf/2503.00687v3",
    "summary": "Transformer-based deep learning models have achieved state-of-the-art\nperformance across numerous language and vision tasks. While the self-attention\nmechanism, a core component of transformers, has proven capable of handling\ncomplex data patterns, it has been observed that the representational capacity\nof the attention matrix degrades significantly across transformer layers,\nthereby hurting its overall performance. In this work, we leverage the\nconnection between self-attention computations and low-pass non-local means\n(NLM) smoothing filters and propose the Twicing Attention, a novel attention\nmechanism that uses kernel twicing procedure in nonparametric regression to\nalleviate the low-pass behavior of associated NLM smoothing with compelling\ntheoretical guarantees and enhanced adversarial robustness. This approach\nenables the extraction and reuse of meaningful information retained in the\nresiduals following the imperfect smoothing operation at each layer. Our\nproposed method offers two key advantages over standard self-attention: 1) a\nprovably slower decay of representational capacity and 2) improved robustness\nand accuracy across various data modalities and tasks. We empirically\ndemonstrate the performance gains of our model over baseline transformers on\nmultiple tasks and benchmarks, including image classification and language\nmodeling, on both clean and corrupted data.",
    "authors": [
      "Laziz Abdullaev",
      "Tan M. Nguyen"
    ],
    "updated": "2025-08-04T12:28:30Z",
    "categories": [
      "cs.LG"
    ]
  },
  {
    "id": "2509.15448",
    "title": "Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to\n  Multi-Scale Problems",
    "link": "http://arxiv.org/abs/2509.15448v1",
    "pdf": "http://arxiv.org/pdf/2509.15448v1",
    "summary": "Transformers and their attention mechanism have been revolutionary in the\nfield of Machine Learning. While originally proposed for the language data,\nthey quickly found their way to the image, video, graph, etc. data modalities\nwith various signal geometries. Despite this versatility, generalizing the\nattention mechanism to scenarios where data is presented at different scales\nfrom potentially different modalities is not straightforward. The attempts to\nincorporate hierarchy and multi-modality within transformers are largely based\non ad hoc heuristics, which are not seamlessly generalizable to similar\nproblems with potentially different structures. To address this problem, in\nthis paper, we take a fundamentally different approach: we first propose a\nmathematical construct to represent multi-modal, multi-scale data. We then\nmathematically derive the neural attention mechanics for the proposed construct\nfrom the first principle of entropy minimization. We show that the derived\nformulation is optimal in the sense of being the closest to the standard\nSoftmax attention while incorporating the inductive biases originating from the\nhierarchical/geometric information of the problem. We further propose an\nefficient algorithm based on dynamic programming to compute our derived\nattention mechanism. By incorporating it within transformers, we show that the\nproposed hierarchical attention mechanism not only can be employed to train\ntransformer models in hierarchical/multi-modal settings from scratch, but it\ncan also be used to inject hierarchical information into classical, pre-trained\ntransformer models post training, resulting in more efficient models in\nzero-shot manner.",
    "authors": [
      "Saeed Amizadeh",
      "Sara Abdali",
      "Yinheng Li",
      "Kazuhito Koishida"
    ],
    "updated": "2025-09-18T21:44:07Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "stat.ML"
    ]
  }
]