import sys
from pathlib import Path

# Add src to path for imports BEFORE other imports
sys.path.append(str(Path(__file__).parent.parent))

from retrieval.arxiv_searcher import run_arxiv_search, search_arxiv_papers
from retrieval.inmemory import InMemoryIndex
from pipelines.baseline import set_global_index
from pipelines.corrective import answer_with_correction
from ui.send import send_long_message
from data.cache_loader import load_precomputed_cache, cache_exists
from config import use_langgraph
import chainlit as cl

from adapters.mock_agent import run_agent
from ui.components import render_citations, render_cornell, render_quiz

# LangGraph imports (conditional)
try:
    from graphs.message_routing import process_message_with_routing
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    print("âš ï¸ LangGraph not available, using legacy implementation")

# Global index for RAG
_rag_index = None


@cl.on_chat_start
async def on_chat_start():
    """Initialize the chat session with a greeting message."""
    global _rag_index

    # Initialize RAG index with some sample papers
    await initialize_rag_index()

    await cl.Message(
        content=(
            "## Papers RAG Agent (Baseline + Corrective RAG)\n\n"
            "ã“ã‚“ã«ã¡ã¯ï¼è«–æ–‡ã«é–¢ã™ã‚‹è³ªå•ã‚’ã—ã¦ãã ã•ã„ã€‚\n"
            "Baseline RAGã¾ãŸã¯HyDEã‚’ä½¿ã£ãŸå›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚\n\n"
            "**ä½¿ã„æ–¹:**\n"
            "- é€šå¸¸ã®è³ªå•: RAGã«ã‚ˆã‚‹å›ç­”\n"
            "- `arxiv: <query>`: è«–æ–‡æ¤œç´¢\n\n"
            "**ãƒ†ã‚¹ãƒˆç”¨ã‚¯ã‚¨ãƒªä¾‹:**\n"
            "- ã€ŒTransformerã«é–¢ã™ã‚‹è«–æ–‡ã‚’æ¢ã—ã¦ã„ã¾ã™ã€\n"
            "- ã€ŒAttentionæ©Ÿæ§‹ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€\n"
            "- ã€ŒBERT ã¨ GPT ã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿã€\n\n"
            "ä½•ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ"
        )
    ).send()


async def initialize_rag_index():
    """Initialize RAG index with precomputed cache or fallback to dynamic loading."""
    global _rag_index

    try:
        # Try to load precomputed cache first
        print("ğŸš€ Initializing RAG index...")

        if cache_exists():
            print("ğŸ“– Loading precomputed cache...")
            _rag_index = load_precomputed_cache()

            if _rag_index is not None:
                set_global_index(_rag_index)
                print(f"âœ… RAG index loaded from cache with {len(_rag_index.papers_with_embeddings)} papers")
                return
            else:
                print("âš ï¸  Failed to load cache, falling back to dynamic loading...")
        else:
            print("âš ï¸  Precomputed cache not found, falling back to dynamic loading...")

        # Fallback to dynamic loading (original implementation)
        await _initialize_rag_index_dynamic()

    except Exception as e:
        print(f"âŒ Error initializing RAG index: {e}")
        import traceback
        traceback.print_exc()
        _rag_index = None


async def _initialize_rag_index_dynamic():
    """Fallback dynamic initialization (original implementation)."""
    global _rag_index

    try:
        # Search for multiple batches of relevant papers to populate the index
        all_papers = []

        # Search queries targeting different aspects of NLP/Transformer research
        search_queries = [
            'transformer attention mechanism language',
            'BERT GPT language model pre-training',
            'fine-tuning RLHF instruction following',
            'efficient transformer attention flash',
            'language model evaluation benchmark',
            'neural machine translation attention',
            'pre-trained language representation',
            'self-attention multi-head transformer'
        ]

        for i, query in enumerate(search_queries):
            try:
                # Use relevance-based search for better results
                batch_papers = search_arxiv_papers(query, max_results=8)
                all_papers.extend(batch_papers)
                print(f"  Query {i+1}/{len(search_queries)}: '{query}' -> {len(batch_papers)} papers")
            except Exception as e:
                print(f"  âŒ Query {i+1} failed: {e}")
                continue

        # Remove duplicates based on paper ID
        seen_ids = set()
        papers = []
        for paper in all_papers:
            if paper.id not in seen_ids:
                papers.append(paper)
                seen_ids.add(paper.id)

        # If no papers found, try simple fallback searches
        if not papers:
            print("âš ï¸  No papers from specialized queries, trying fallback...")
            fallback_queries = ['transformer', 'attention mechanism', 'BERT', 'GPT']
            for query in fallback_queries:
                try:
                    batch_papers = search_arxiv_papers(query, max_results=10)
                    all_papers.extend(batch_papers)
                    print(f"  Fallback '{query}' -> {len(batch_papers)} papers")
                    if len(batch_papers) > 0:
                        break
                except Exception as e:
                    print(f"  Fallback '{query}' failed: {e}")
                    continue

            # Remove duplicates again
            seen_ids = set()
            papers = []
            for paper in all_papers:
                if paper.id not in seen_ids:
                    papers.append(paper)
                    seen_ids.add(paper.id)

        if papers:
            _rag_index = InMemoryIndex()
            _rag_index.build(papers)
            set_global_index(_rag_index)
            print(f"âœ… RAG index initialized with {len(papers)} papers")
            print(f"âœ… Index contains {len(_rag_index.papers_with_embeddings)} embedded papers")
        else:
            print("âŒ Warning: No papers found even with fallback searches")
            _rag_index = None

    except Exception as e:
        print(f"âŒ Error in dynamic initialization: {e}")
        import traceback
        traceback.print_exc()
        _rag_index = None


@cl.on_message
async def on_message(message: cl.Message):
    """
    Handle incoming messages from users.

    Args:
        message: Chainlit message object containing user input
    """
    global _rag_index

    # Check if we should use LangGraph workflows
    if use_langgraph() and LANGGRAPH_AVAILABLE:
        await handle_message_with_langgraph(message)
    else:
        await handle_message_legacy(message)


async def handle_message_with_langgraph(message: cl.Message):
    """Handle messages using LangGraph workflows."""
    global _rag_index

    # Ensure RAG index is initialized for RAG questions
    if not message.content.lower().startswith("arxiv:") and _rag_index is None:
        print("âš ï¸ RAG index is None, initializing...")
        await cl.Message(content="RAGç´¢å¼•ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚").send()
        await initialize_rag_index()
        if _rag_index is None:
            print("âŒ RAG index initialization failed")
            await cl.Message(content="RAGç´¢å¼•ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚").send()
            return

    async with cl.Step(name="LangGraph Processing", type="run") as step:
        step.output = "LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§å‡¦ç†ã—ã¦ã„ã¾ã™..."

        try:
            print(f"ğŸš€ Using LangGraph workflow for: {message.content[:50]}...")

            # Process message with LangGraph routing workflow
            response_content = process_message_with_routing(
                message_content=message.content,
                rag_index=_rag_index
            )

            step.output = "LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†"

        except Exception as e:
            response_content = f"LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            step.output = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
            print(f"âŒ LangGraph workflow failed: {e}")

    # Send response using chunked sending
    await send_long_message(response_content)


async def handle_message_legacy(message: cl.Message):
    """Handle messages using legacy implementation."""
    global _rag_index

    # Handle arXiv search command
    if message.content.lower().startswith("arxiv:"):
        await handle_arxiv_command(message)
        return

    # Handle RAG questions
    if _rag_index is None:
        print("âš ï¸ RAG index is None, initializing...")
        await cl.Message(content="RAGç´¢å¼•ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚").send()
        await initialize_rag_index()
        if _rag_index is None:
            print("âŒ RAG index initialization failed")
            await cl.Message(content="RAGç´¢å¼•ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚").send()
            return
        else:
            print(f"âœ… RAG index initialized with {len(_rag_index.papers_with_embeddings)} papers")

    async with cl.Step(name="Legacy Processing", type="run") as step:
        step.output = "RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™..."

        try:
            # Debug info
            print(f"ğŸ” Processing query: {message.content}")
            print(f"ğŸ” Using index with {len(_rag_index.papers_with_embeddings)} papers")

            # Use corrective RAG pipeline
            result = answer_with_correction(message.content, index=_rag_index)

            # Debug result
            print(f"ğŸ” Result support: {result.support:.3f}")
            print(f"ğŸ” Result citations: {len(result.citations)}")

            # Format support level
            support_level = format_support_level(result.support)

            # Create response content
            response_content = f"## å›ç­”\n\n{result.text}\n\n"

            # Add citations if available
            if result.citations:
                response_content += "## Citations:\n\n"
                for i, citation in enumerate(result.citations, 1):
                    response_content += f"{i}. [{citation['title']}]({citation['link']})\n"
                response_content += "\n"

            # Add support score
            response_content += f"**Support: {support_level} (score={result.support:.2f})**\n"

            # Add debug info about attempts (optional)
            if len(result.attempts) > 1:
                response_content += f"\n*HyDEã‚’ä½¿ç”¨ã—ãŸè£œæ­£æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã—ãŸ*"

            step.output = "å›ç­”ç”Ÿæˆå®Œäº†"

        except Exception as e:
            response_content = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            step.output = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"

    # Send response using chunked sending
    await send_long_message(response_content)


async def handle_arxiv_command(message: cl.Message):
    """Handle arxiv: command for paper search."""
    q = message.content.split(":", 1)[1].strip()
    hits = run_arxiv_search(q, max_results=5)

    if hits:
        lines = [
            f"- [{h['title']}]({h['link']})  â€¢  [PDF]({h['pdf']})"
            if h.get("pdf")
            else f"- [{h['title']}]({h['link']})"
            for h in hits
        ]
        await cl.Message(content="### arXivæ¤œç´¢çµæœ\n" + "\n".join(lines)).send()
    else:
        await cl.Message(content="è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ").send()


def format_support_level(support_score: float) -> str:
    """Format support score as High/Med/Low."""
    if support_score >= 0.8:
        return "High"
    elif support_score >= 0.62:
        return "Med"
    else:
        return "Low"


if __name__ == "__main__":
    import os

    # Set environment variable for running the app
    os.environ["CHAINLIT_HOST"] = "localhost"
    os.environ["CHAINLIT_PORT"] = "8000"

    # This is mainly for development/testing
    # In production, use: chainlit run src/ui/app.py
    print("To run the app, use: uv run chainlit run src/ui/app.py -w")
