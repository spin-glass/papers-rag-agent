import sys
from pathlib import Path

# Add src to path for imports BEFORE other imports
sys.path.append(str(Path(__file__).parent.parent))

from retrieval.arxiv_searcher import run_arxiv_search, search_arxiv_papers
from retrieval.inmemory import InMemoryIndex
from pipelines.baseline import set_global_index
from pipelines.corrective import answer_with_correction
from ui.send import send_long_message
from data.cache_loader import load_precomputed_cache, cache_exists
from config import use_langgraph, enable_langsmith_tracing, get_langsmith_api_key, get_langsmith_project, get_langsmith_endpoint
import chainlit as cl


# TODO: Remove this import - it's unused here. Actual import happens at line 340 where it's needed.
# LangGraph availability check
try:
    from graphs.message_routing import process_message_with_routing  # noqa: F401
    LANGGRAPH_AVAILABLE = True
    HAS_MESSAGE_ROUTING = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    HAS_MESSAGE_ROUTING = False
    print("âš ï¸ LangGraph not available, using legacy implementation")

# Global index for RAG
_rag_index = None

# Add startup logging for Cloud Run debugging
print("ğŸš€ Papers RAG Agent starting up...")
print(f"ğŸ“ LangGraph available: {LANGGRAPH_AVAILABLE}")
print("âœ… Application module loaded successfully")


def initialize_langsmith_tracing():
    """Initialize LangSmith tracing if enabled."""
    # Simplified: Only log status, don't modify environment variables at runtime
    if enable_langsmith_tracing():
        api_key = get_langsmith_api_key()
        if api_key:
            print(f"âœ… LangSmith tracing enabled for project: {get_langsmith_project()}")
        else:
            print("âš ï¸ LangSmith tracing is enabled but API key not found")
    else:
        print("â„¹ï¸ LangSmith tracing is disabled")


@cl.on_chat_start
async def on_chat_start():
    """Initialize the chat session with a greeting message."""
    global _rag_index

    try:
        # Initialize LangSmith tracing if enabled
        initialize_langsmith_tracing()

        # Check OpenAI API Key before initialization
        from config import get_openai_api_key_safe
        api_key = get_openai_api_key_safe()

        if not api_key:
            await cl.Message(
                content=(
                    "## âš ï¸ è¨­å®šãŒå¿…è¦ã§ã™\n\n"
                    "Papers RAG Agentã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€OpenAI API Keyã®è¨­å®šãŒå¿…è¦ã§ã™ã€‚\n\n"
                    "**è¨­å®šæ–¹æ³•:**\n"
                    "1. [OpenAI Platform](https://platform.openai.com/api-keys) ã§API Keyã‚’å–å¾—\n"
                    "2. ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š: `export OPENAI_API_KEY=\"your_key_here\"`\n"
                    "3. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å†èµ·å‹•\n\n"
                    "è©³ç´°ã¯ `SETUP.md` ã‚’ã”ç¢ºèªãã ã•ã„ã€‚\n\n"
                    "**ç¾åœ¨åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½:**\n"
                    "- `arxiv: <query>`: è«–æ–‡æ¤œç´¢ï¼ˆAPI Keyä¸è¦ï¼‰\n\n"
                    "API Keyè¨­å®šå¾Œã«å…¨æ©Ÿèƒ½ã‚’ãŠä½¿ã„ã„ãŸã ã‘ã¾ã™ã€‚"
                )
            ).send()
            return

        # Initialize RAG index with some sample papers
        await initialize_rag_index()

    except Exception as e:
        error_msg = str(e)
        print(f"âŒ Critical error during chat initialization: {error_msg}")
        import traceback
        traceback.print_exc()

        await cl.Message(
            content=(
                "## âŒ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼\n\n"
                f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n\n"
                f"**ã‚¨ãƒ©ãƒ¼è©³ç´°:** {error_msg}\n\n"
                "**åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½:**\n"
                "- `arxiv: <query>`: è«–æ–‡æ¤œç´¢\n\n"
                "ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"
            )
        ).send()
        return

    await cl.Message(
        content=(
            "## Papers RAG Agent (Baseline + Corrective RAG)\n\n"
            "ã“ã‚“ã«ã¡ã¯ï¼è«–æ–‡ã«é–¢ã™ã‚‹è³ªå•ã‚’ã—ã¦ãã ã•ã„ã€‚\n"
            "Baseline RAGã¾ãŸã¯HyDEã‚’ä½¿ã£ãŸå›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚\n\n"
            "**ä½¿ã„æ–¹:**\n"
            "- é€šå¸¸ã®è³ªå•: RAGã«ã‚ˆã‚‹å›ç­”\n"
            "- `arxiv: <query>`: è«–æ–‡æ¤œç´¢\n\n"
            "**ãƒ†ã‚¹ãƒˆç”¨ã‚¯ã‚¨ãƒªä¾‹:**\n"
            "- ã€Œæœ€è¿‘ã®Transformerã«é–¢ã™ã‚‹è«–æ–‡ã‚’æ¢ã—ã¦ã„ã¾ã™ã€\n"
            "- ã€ŒAttentionæ©Ÿæ§‹ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€\n"
            "- ã€ŒBERT ã¨ GPT ã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿã€\n\n"
            "ä½•ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ"
        )
    ).send()


async def initialize_rag_index():
    """Initialize RAG index with precomputed cache or fallback to dynamic loading."""
    global _rag_index

    try:
        # Try to load precomputed cache first
        print("ğŸš€ Initializing RAG index...")

        if cache_exists():
            print("ğŸ“– Loading precomputed cache...")
            _rag_index = load_precomputed_cache()

            if _rag_index is not None:
                set_global_index(_rag_index)
                print(f"âœ… RAG index loaded from cache with {len(_rag_index.papers_with_embeddings)} papers")
                return
            else:
                print("âš ï¸  Failed to load cache, falling back to dynamic loading...")
        else:
            print("âš ï¸  Precomputed cache not found, falling back to dynamic loading...")

        # Fallback to dynamic loading (original implementation)
        await _initialize_rag_index_dynamic()

    except Exception as e:
        print(f"âŒ Error initializing RAG index: {e}")
        import traceback
        traceback.print_exc()
        _rag_index = None


async def _initialize_rag_index_dynamic():
    """Fallback dynamic initialization (original implementation)."""
    global _rag_index

    try:
        # Search for multiple batches of relevant papers to populate the index
        all_papers = []

        # Search queries targeting different aspects of NLP/Transformer research
        search_queries = [
            'transformer attention mechanism language',
            'BERT GPT language model pre-training',
            'fine-tuning RLHF instruction following',
            'efficient transformer attention flash',
            'language model evaluation benchmark',
            'neural machine translation attention',
            'pre-trained language representation',
            'self-attention multi-head transformer'
        ]

        for i, query in enumerate(search_queries):
            try:
                # Use relevance-based search for better results
                batch_papers = search_arxiv_papers(query, max_results=8)
                all_papers.extend(batch_papers)
                print(f"  Query {i+1}/{len(search_queries)}: '{query}' -> {len(batch_papers)} papers")
            except Exception as e:
                print(f"  âŒ Query {i+1} failed: {e}")
                continue

        # Remove duplicates based on paper ID
        seen_ids = set()
        papers = []
        for paper in all_papers:
            if paper.id not in seen_ids:
                papers.append(paper)
                seen_ids.add(paper.id)

        # If no papers found, try simple fallback searches
        if not papers:
            print("âš ï¸  No papers from specialized queries, trying fallback...")
            fallback_queries = ['transformer', 'attention mechanism', 'BERT', 'GPT']
            for query in fallback_queries:
                try:
                    batch_papers = search_arxiv_papers(query, max_results=10)
                    all_papers.extend(batch_papers)
                    print(f"  Fallback '{query}' -> {len(batch_papers)} papers")
                    if len(batch_papers) > 0:
                        break
                except Exception as e:
                    print(f"  Fallback '{query}' failed: {e}")
                    continue

            # Remove duplicates again
            seen_ids = set()
            papers = []
            for paper in all_papers:
                if paper.id not in seen_ids:
                    papers.append(paper)
                    seen_ids.add(paper.id)

        if papers:
            _rag_index = InMemoryIndex()
            _rag_index.build(papers)
            set_global_index(_rag_index)
            print(f"âœ… RAG index initialized with {len(papers)} papers")
            print(f"âœ… Index contains {len(_rag_index.papers_with_embeddings)} embedded papers")
        else:
            print("âŒ Warning: No papers found even with fallback searches")
            _rag_index = None

    except Exception as e:
        print(f"âŒ Error in dynamic initialization: {e}")
        import traceback
        traceback.print_exc()
        _rag_index = None


@cl.on_message
async def on_message(message: cl.Message):
    """
    Handle incoming messages from users.

    Args:
        message: Chainlit message object containing user input
    """
    global _rag_index

    # Temporarily use legacy implementation for stability
    # TODO: Re-enable LangGraph after Cloud Run deployment is stable
    # if use_langgraph() and LANGGRAPH_AVAILABLE:
    #     await handle_message_with_langgraph(message)
    # else:
    await handle_message_legacy(message)


async def handle_message_with_langgraph(message: cl.Message):
    """Handle messages using LangGraph workflows with streaming support."""
    global _rag_index

    # Ensure RAG index is initialized for RAG questions
    if not message.content.lower().startswith("arxiv:") and _rag_index is None:
        print("âš ï¸ RAG index is None, initializing...")
        await cl.Message(content="RAGç´¢å¼•ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚").send()
        await initialize_rag_index()
        if _rag_index is None:
            print("âŒ RAG index initialization failed")
            await cl.Message(content="RAGç´¢å¼•ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚").send()
            return

    try:
        print(f"ğŸš€ Using LangGraph workflow with streaming for: {message.content[:50]}...")

        # Process message with streaming LangGraph workflow
        response_content = await process_message_with_routing_streaming(
            message_content=message.content,
            rag_index=_rag_index
        )

        print("âœ… LangGraph streaming workflow completed")

    except Exception as e:
        error_msg = str(e)
        print(f"âŒ LangGraph workflow failed: {e}")

        # Check for specific API key related errors
        if "OPENAI_API_KEY" in error_msg:
            response_content = (
                "## âš ï¸ API Keyè¨­å®šã‚¨ãƒ©ãƒ¼\n\n"
                "OpenAI API KeyãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€è³ªå•ã«å›ç­”ã§ãã¾ã›ã‚“ã€‚\n\n"
                "**è§£æ±ºæ–¹æ³•:**\n"
                "1. ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š: `export OPENAI_API_KEY=\"your_key_here\"`\n"
                "2. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å†èµ·å‹•\n\n"
                "è©³ç´°ã¯ `SETUP.md` ã‚’ã”ç¢ºèªãã ã•ã„ã€‚\n\n"
                "**åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½:**\n"
                "- `arxiv: <query>`: è«–æ–‡æ¤œç´¢ï¼ˆAPI Keyä¸è¦ï¼‰"
            )
        else:
            response_content = f"LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}"

        # Send error response
        await send_long_message(response_content)


async def process_message_with_routing_streaming(message_content: str, rag_index=None) -> str:
    """
    LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å®Ÿè¡Œã—ã€é€”ä¸­çµæœã‚’Chainlitã‚¹ãƒ†ãƒƒãƒ—ã§è¡¨ç¤º
    """
    from graphs.message_routing import create_message_routing_graph, MessageState
    from config import get_graph_recursion_limit
    from langchain_core.runnables import RunnableConfig

    try:
        # ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
        routing_graph = create_message_routing_graph()

        # åˆæœŸçŠ¶æ…‹ã‚’æº–å‚™
        initial_state = MessageState(
            message_content=message_content,
            message_type="",
            rag_index=rag_index,
            arxiv_results=None,
            rag_result=None,
            final_response=None,
            error=None
        )

        # æœ€çµ‚å›ç­”ç”¨ã®ç©ºãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æº–å‚™
        agent_message = cl.Message(content="")
        await agent_message.send()

        config = RunnableConfig(recursion_limit=get_graph_recursion_limit())
        response_chunks = []

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ
        async for output in routing_graph.astream_log(initial_state, config=config, include_types=["llm"]):
            for op in output.ops:
                # ãƒãƒ¼ãƒ‰å®Ÿè¡Œçµæœã®å‡¦ç†
                if op["path"] == "/streamed_output/-":
                    await handle_node_output(op)

                # æœ€çµ‚å›ç­”ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼ˆLLMã‹ã‚‰ã®å‡ºåŠ›ï¼‰
                elif (op["path"].startswith("/logs/") and
                      op["path"].endswith("/streamed_output_str/-")):
                    chunk = op["value"]
                    response_chunks.append(chunk)
                    await agent_message.stream_token(chunk)

        # æœ€çµ‚çš„ãªå›ç­”ã‚’çµåˆ
        final_response = "".join(response_chunks)

        # å›ç­”ãŒç©ºã®å ´åˆã¯ã€æœ€çµ‚çŠ¶æ…‹ã‹ã‚‰å–å¾—
        if not final_response.strip():
            print("âš ï¸ No streaming response received, falling back to invoke...")
            final_state = routing_graph.invoke(initial_state, config=config)
            final_response = final_state.get("final_response", "å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            # ç©ºã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«æœ€çµ‚å›ç­”ã‚’è¨­å®š
            await agent_message.stream_token(final_response)

        return final_response

    except Exception as e:
        print(f"âŒ Streaming workflow failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šé€šå¸¸ã®å‡¦ç†
        from graphs.message_routing import process_message_with_routing
        return process_message_with_routing(message_content, rag_index)


async def handle_node_output(op):
    """
    å„ãƒãƒ¼ãƒ‰ã®å®Ÿè¡Œçµæœã‚’Chainlitã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦è¡¨ç¤º
    """
    try:
        # ãƒãƒ¼ãƒ‰åã‚’å–å¾—
        node_name = list(op["value"].keys())[0]
        node_state = op["value"][node_name]

        # ãƒãƒ¼ãƒ‰ã«å¿œã˜ã¦ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¡¨ç¤º
        if node_name == "classify":
            await display_classification_step(node_state)
        elif node_name == "rag_pipeline":
            await display_rag_pipeline_step(node_state)
        elif node_name == "arxiv_search":
            await display_arxiv_search_step(node_state)
        elif node_name == "baseline":
            await display_baseline_step(node_state)
        elif node_name == "hyde_rewrite":
            await display_hyde_step(node_state)
        elif node_name == "enhanced_retrieval":
            await display_enhanced_retrieval_step(node_state)
        elif node_name == "format_rag":
            await display_format_step(node_state)
        elif node_name == "format_arxiv":
            await display_arxiv_format_step(node_state)

    except Exception as e:
        print(f"âš ï¸ Error displaying node output: {e}")


# å„ãƒãƒ¼ãƒ‰ç”¨ã®ã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤ºé–¢æ•°
async def display_classification_step(state):
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ†é¡çµæœã‚’è¡¨ç¤º"""
    message_type = state.get("message_type", "unknown")

    async with cl.Step(name="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ†é¡", type="tool") as step:
        if message_type == "rag":
            step.output = "ğŸ“ RAGè³ªå•ã¨ã—ã¦åˆ†é¡ã•ã‚Œã¾ã—ãŸ"
        elif message_type == "arxiv":
            step.output = "ğŸ“š ArXivæ¤œç´¢ã¨ã—ã¦åˆ†é¡ã•ã‚Œã¾ã—ãŸ"
        else:
            step.output = f"â“ åˆ†é¡çµæœ: {message_type}"


async def display_rag_pipeline_step(state):
    """RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œçµæœã‚’è¡¨ç¤º"""
    rag_result = state.get("rag_result")
    error = state.get("error")

    async with cl.Step(name="RAGå‡¦ç†", type="run") as step:
        if error:
            step.output = f"âŒ ã‚¨ãƒ©ãƒ¼: {error}"
        elif rag_result:
            support = rag_result.support
            citations_count = len(rag_result.citations)
            step.output = f"âœ… RAGå‡¦ç†å®Œäº†\n- Supportå€¤: {support:.3f}\n- å¼•ç”¨æ–‡çŒ®æ•°: {citations_count}"
        else:
            step.output = "ğŸ”„ RAGå‡¦ç†ä¸­..."


async def display_arxiv_search_step(state):
    """ArXivæ¤œç´¢çµæœã‚’è¡¨ç¤º"""
    results = state.get("arxiv_results", [])
    error = state.get("error")

    async with cl.Step(name="ArXivæ¤œç´¢", type="tool") as step:
        if error:
            step.output = f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {error}"
        else:
            step.output = f"ğŸ” ArXivæ¤œç´¢å®Œäº†: {len(results)}ä»¶ã®è«–æ–‡ã‚’ç™ºè¦‹"


async def display_baseline_step(state):
    """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¤œç´¢çµæœã‚’è¡¨ç¤º"""
    answer = state.get("answer")

    async with cl.Step(name="ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¤œç´¢", type="tool") as step:
        if answer:
            support = answer.support
            citations = len(answer.citations)
            step.output = f"ğŸ” ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¤œç´¢å®Œäº†\n- Supportå€¤: {support:.3f}\n- æ¤œç´¢ã•ã‚ŒãŸæ–‡çŒ®: {citations}ä»¶"
        else:
            step.output = "ğŸ”„ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¤œç´¢ä¸­..."


async def display_hyde_step(state):
    """HyDEå‡¦ç†çµæœã‚’è¡¨ç¤º"""
    hyde_query = state.get("hyde_query")

    async with cl.Step(name="HyDEæ‹¡å¼µ", type="tool") as step:
        if hyde_query:
            # ã‚¯ã‚¨ãƒªãŒé•·ã„å ´åˆã¯çœç•¥
            display_query = hyde_query[:100] + "..." if len(hyde_query) > 100 else hyde_query
            step.output = f"ğŸ”„ HyDEæ‹¡å¼µã‚¯ã‚¨ãƒªç”Ÿæˆå®Œäº†\n```\n{display_query}\n```"
        else:
            step.output = "âŒ HyDEæ‹¡å¼µã«å¤±æ•—"


async def display_enhanced_retrieval_step(state):
    """æ‹¡å¼µæ¤œç´¢çµæœã‚’è¡¨ç¤º"""
    answer = state.get("answer")
    baseline_support = state.get("baseline_support")

    async with cl.Step(name="æ‹¡å¼µæ¤œç´¢", type="tool") as step:
        if answer and baseline_support is not None:
            improvement = answer.support - baseline_support
            step.output = f"âœ… HyDEæ‹¡å¼µæ¤œç´¢å®Œäº†\n- æ–°ã—ã„Supportå€¤: {answer.support:.3f}\n- æ”¹å–„åº¦: {improvement:+.3f}"
        else:
            step.output = "ğŸ”„ HyDEæ‹¡å¼µæ¤œç´¢ä¸­..."


async def display_format_step(state):
    """æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†ã‚’è¡¨ç¤º"""
    final_response = state.get("final_response")

    async with cl.Step(name="å›ç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ", type="tool") as step:
        if final_response:
            response_length = len(final_response)
            step.output = f"ğŸ“ å›ç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå®Œäº† ({response_length}æ–‡å­—)"
        else:
            step.output = "ğŸ”„ å›ç­”ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¸­..."


async def display_arxiv_format_step(state):
    """ArXivæ¤œç´¢çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†ã‚’è¡¨ç¤º"""
    final_response = state.get("final_response")
    results = state.get("arxiv_results", [])

    async with cl.Step(name="æ¤œç´¢çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ", type="tool") as step:
        if final_response:
            step.output = f"ğŸ“ ArXivæ¤œç´¢çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå®Œäº† ({len(results)}ä»¶)"
        else:
            step.output = "ğŸ”„ æ¤œç´¢çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¸­..."


async def handle_message_legacy(message: cl.Message):
    """Handle messages using legacy implementation."""
    global _rag_index

    # Handle arXiv search command
    if message.content.lower().startswith("arxiv:"):
        await handle_arxiv_command(message)
        return

    # Handle RAG questions
    if _rag_index is None:
        print("âš ï¸ RAG index is None, initializing...")
        await cl.Message(content="RAGç´¢å¼•ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚").send()
        await initialize_rag_index()
        if _rag_index is None:
            print("âŒ RAG index initialization failed")
            await cl.Message(content="RAGç´¢å¼•ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚").send()
            return
        else:
            print(f"âœ… RAG index initialized with {len(_rag_index.papers_with_embeddings)} papers")

    async with cl.Step(name="Legacy Processing", type="run") as step:
        step.output = "RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™..."

        try:
            # Debug info
            print(f"ğŸ” Processing query: {message.content}")
            print(f"ğŸ” Using index with {len(_rag_index.papers_with_embeddings)} papers")

            # Use corrective RAG pipeline
            result = answer_with_correction(message.content, index=_rag_index)

            # Debug result
            print(f"ğŸ” Result support: {result.support:.3f}")
            print(f"ğŸ” Result citations: {len(result.citations)}")

            # Format support level
            support_level = format_support_level(result.support)

            # Create response content
            response_content = f"## å›ç­”\n\n{result.text}\n\n"

            # Add citations if available
            if result.citations:
                response_content += "## Citations:\n\n"
                for i, citation in enumerate(result.citations, 1):
                    response_content += f"{i}. [{citation['title']}]({citation['link']})\n"
                response_content += "\n"

            # Add support score
            response_content += f"**Support: {support_level} (score={result.support:.2f})**\n"

            # Add debug info about attempts (optional)
            if len(result.attempts) > 1:
                response_content += "\n*HyDEã‚’ä½¿ç”¨ã—ãŸè£œæ­£æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã—ãŸ*"

            step.output = "å›ç­”ç”Ÿæˆå®Œäº†"

        except Exception as e:
            response_content = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            step.output = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"

    # Send response using chunked sending
    await send_long_message(response_content)


async def handle_arxiv_command(message: cl.Message):
    """Handle arxiv: command for paper search."""
    q = message.content.split(":", 1)[1].strip()
    hits = run_arxiv_search(q, max_results=5)

    if hits:
        lines = [
            f"- [{h['title']}]({h['link']})  â€¢  [PDF]({h['pdf']})"
            if h.get("pdf")
            else f"- [{h['title']}]({h['link']})"
            for h in hits
        ]
        await cl.Message(content="### arXivæ¤œç´¢çµæœ\n" + "\n".join(lines)).send()
    else:
        await cl.Message(content="è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ").send()


def format_support_level(support_score: float) -> str:
    """Format support score as High/Med/Low."""
    if support_score >= 0.8:
        return "High"
    elif support_score >= 0.62:
        return "Med"
    else:
        return "Low"


if __name__ == "__main__":
    import os

    # Set environment variable for running the app
    os.environ["CHAINLIT_HOST"] = "localhost"
    os.environ["CHAINLIT_PORT"] = "8000"

    # This is mainly for development/testing
    # In production, use: chainlit run src/ui/app.py
    print("To run the app, use: uv run chainlit run src/ui/app.py -w")
