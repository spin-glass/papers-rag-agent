# src/ui/app.py
from __future__ import annotations
import os
import json
import asyncio
import httpx
import chainlit as cl

API_BASE = os.getenv("PAPERS_API_BASE", "http://localhost:9000")


# -------- helpers --------
async def get_health() -> dict:
    async with httpx.AsyncClient(timeout=15.0) as client:
        r = await client.get(f"{API_BASE}/health")
        r.raise_for_status()
        return r.json()


async def call_arxiv_search(query: str, max_results: int = 10) -> list[dict]:
    payload = {"query": query, "max_results": max_results}
    async with httpx.AsyncClient(timeout=60.0) as client:
        r = await client.post(f"{API_BASE}/arxiv/search", json=payload)
        r.raise_for_status()
        return r.json().get("items", [])


async def call_digest(cat: str = "cs.LG", days: int = 1, limit: int = 10) -> list[dict]:
    params = {"cat": cat, "days": days, "limit": limit}
    async with httpx.AsyncClient(timeout=60.0) as client:
        r = await client.get(f"{API_BASE}/digest", params=params)
        r.raise_for_status()
        return r.json()


async def call_digest_details(paper_id: str) -> dict:
    """è«–æ–‡ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
    try:
        async with httpx.AsyncClient(timeout=360.0) as client:
            r = await client.get(f"{API_BASE}/digest/{paper_id}/details")
            r.raise_for_status()
            return r.json()
    except httpx.HTTPStatusError as e:
        error_msg = f"HTTP {e.response.status_code}: {e.response.text}"
        print(f"âŒ API Error: {error_msg}")
        raise Exception(f"API Error: {error_msg}")
    except httpx.TimeoutException:
        error_msg = "Request timeout (360s)"
        print(f"âŒ Timeout: {error_msg}")
        raise Exception(f"Timeout: {error_msg}")
    except Exception as e:
        error_msg = f"Request failed: {str(e)}"
        print(f"âŒ Request failed: {error_msg}")
        raise Exception(error_msg)


async def call_fulltext(
    paper_id: str, fmt: str = "plain", max_bytes: int = 100000
) -> str:
    """è«–æ–‡ã®å…¨æ–‡ã‚’å–å¾—"""
    async with httpx.AsyncClient(timeout=60.0) as client:
        response = await client.get(
            f"{API_BASE}/digest/{paper_id}/fulltext",
            params={"format": fmt, "max_bytes": max_bytes},
        )
        response.raise_for_status()
        return response.text


async def sse_rag_stream(query: str):
    try:
        async with httpx.AsyncClient(timeout=None) as client:
            async with client.stream(
                "POST", f"{API_BASE}/rag/stream", json={"query": query}
            ) as resp:
                resp.raise_for_status()
                try:
                    async for line in resp.aiter_lines():
                        if not line or not line.startswith("data: "):
                            continue
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            obj = json.loads(data)
                        except Exception:
                            obj = {}

                        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å‡¦ç†
                        if obj.get("type") == "status":
                            yield {
                                "type": "status",
                                "text": obj.get("text", ""),
                                "done": False,
                            }
                        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å‡¦ç†
                        elif obj.get("type") == "content":
                            yield {
                                "type": "content",
                                "text": obj.get("text", ""),
                                "done": obj.get("done", False),
                            }
                        # ã‚¨ãƒ©ãƒ¼ã®å‡¦ç†
                        elif obj.get("type") == "error":
                            yield {
                                "type": "error",
                                "text": obj.get("text", ""),
                                "done": True,
                            }
                        # å¾“æ¥ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨ã®äº’æ›æ€§
                        else:
                            text = (
                                obj.get("delta")
                                or obj.get("text")
                                or obj.get("content")
                                or ""
                            )
                            if text:
                                yield {"type": "content", "text": text, "done": False}
                except asyncio.CancelledError:
                    # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚ŒãŸå ´åˆã¯é©åˆ‡ã«å‡¦ç†
                    raise
                except Exception as e:
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã®ã‚¨ãƒ©ãƒ¼ã‚’é©åˆ‡ã«å‡¦ç†
                    yield {
                        "type": "error",
                        "text": f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {str(e)}",
                        "done": True,
                    }
    except asyncio.CancelledError:
        # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚ŒãŸå ´åˆã¯å†ç™ºç”Ÿ
        raise
    except Exception as e:
        # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã‚’é©åˆ‡ã«å‡¦ç†
        yield {"type": "error", "text": f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}", "done": True}


# -------- chainlit hooks --------
@cl.on_chat_start
async def on_chat_start():
    try:
        h = await get_health()
        await cl.Message(
            content=(
                "## Papers RAG Agent (Baseline + Corrective RAG)\n\n"
                "ã“ã‚“ã«ã¡ã¯ï¼è«–æ–‡ã«é–¢ã™ã‚‹è³ªå•ã‚’ã—ã¦ãã ã•ã„ã€‚\n"
                "Baseline RAGã‚’å®Ÿè¡Œã—ã€Supportå€¤ãŒé–¾å€¤ã‚’è¶…ãˆãªã„å ´åˆã«HyDEã‚’ä½¿ã£ãŸè£œæ­£æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n\n"
                "**ä½¿ã„æ–¹:**\n"
                "- é€šå¸¸ã®è³ªå•: RAGã«ã‚ˆã‚‹å›ç­”\n"
                "- `arxiv: <query>`: è«–æ–‡æ¤œç´¢\n\n"
                "**ãƒ†ã‚¹ãƒˆç”¨ã‚¯ã‚¨ãƒªä¾‹:**\n"
                "- ã€Œæœ€è¿‘ã®Transformerã«é–¢ã™ã‚‹è«–æ–‡ã‚’æ¢ã—ã¦ã„ã¾ã™ã€\n"
                "- ã€ŒAttentionæ©Ÿæ§‹ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€\n"
                "- ã€ŒBERT ã¨ GPT ã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿã€\n\n"
                f"**ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹:**\n"
                f"- RAG Ready: {h.get('rag_ready')}\n"
                f"- Index Size: {h.get('size', 0)}ä»¶ã®è«–æ–‡\n\n"
                "ä½•ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼Ÿ"
            )
        ).send()
        await cl.Message(
            content="ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã§æ—¥æ¬¡ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’å–å¾—ã§ãã¾ã™ã€‚",
            actions=[
                cl.Action(
                    name="daily_digest",
                    payload={"cat": "cs.LG", "days": 2, "limit": 10},
                    label="ğŸ“° ãƒ‡ã‚¤ãƒªãƒ¼ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆï¼ˆcs.LG, ç›´è¿‘2æ—¥ï¼‰",
                )
            ],
        ).send()
    except Exception as e:
        await cl.Message(
            content=(
                "## âš ï¸ APIæ¥ç¶šã‚¨ãƒ©ãƒ¼\n\n"
                f"FastAPIã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“: `{e}`\n\n"
                f"**ç¢ºèªäº‹é …:**\n"
                f"- `PAPERS_API_BASE={API_BASE}` ãŒæ­£ã—ã„ã‹\n"
                f"- FastAPIã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹\n\n"
                "**åˆ©ç”¨å¯èƒ½ãªæ©Ÿèƒ½:**\n"
                "- `arxiv: <query>`: è«–æ–‡æ¤œç´¢ï¼ˆAPI Keyä¸è¦ï¼‰\n\n"
                "ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"
            )
        ).send()


@cl.action_callback("daily_digest")
async def on_daily_digest(action: cl.Action):
    try:
        cfg = {}
        try:
            if isinstance(action.payload, dict):
                cfg = action.payload
            elif isinstance(action.payload, str):
                cfg = json.loads(action.payload)
        except Exception:
            cfg = {}
        cat = cfg.get("cat", "cs.LG")
        days = int(cfg.get("days", 2))
        limit = int(cfg.get("limit", 10))

        items = await call_digest(cat=cat, days=days, limit=limit)
        if not items:
            await cl.Message(
                content="è©²å½“ã™ã‚‹ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆé …ç›®ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            ).send()
            return

        # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
        await cl.Message(
            content=f"# ğŸ“° ãƒ‡ã‚¤ãƒªãƒ¼ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ\n\n**ã‚«ãƒ†ã‚´ãƒª:** {cat} | **æœŸé–“:** éå»{days}æ—¥ | **ä»¶æ•°:** {limit}ä»¶"
        ).send()

        # å„è«–æ–‡ã‚’å€‹åˆ¥ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦è¡¨ç¤ºï¼ˆãƒœã‚¿ãƒ³ã‚’è¿‘ãã«é…ç½®ï¼‰
        for i, it in enumerate(items, 1):
            title = (it.get("title") or "").strip() or "ï¼ˆç„¡é¡Œï¼‰"
            url = it.get("url") or it.get("link") or ""
            pdf = it.get("pdf") or ""
            summary = it.get("summary_short") or it.get("summary") or ""
            paper_id = it.get("id", "")

            # è«–æ–‡æƒ…å ±ã‚’æ§‹ç¯‰
            paper_content = f"## {i}. {title}\n"

            # ãƒªãƒ³ã‚¯æƒ…å ±
            links = []
            if url:
                links.append(f"[arXiv]({url})")
            if pdf:
                links.append(f"[PDF]({pdf})")
            if links:
                paper_content += f"ğŸ”— {' | '.join(links)}\n"

            # è¦ç´„
            if summary:
                s = summary.strip()
                if len(s) > 200:
                    s = s[:200] + "..."
                paper_content += f"ğŸ“ {s}"

            # è©³ç´°è¡¨ç¤ºãƒœã‚¿ãƒ³ã‚’æº–å‚™
            actions = []
            if paper_id:
                actions.append(
                    cl.Action(
                        name="show_digest_details",
                        payload={"paper_id": paper_id, "paper_title": title},
                        label="ğŸ“– è©³ç´°ã‚’è¦‹ã‚‹",
                    )
                )

            # å„è«–æ–‡ã‚’å€‹åˆ¥ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦é€ä¿¡
            await cl.Message(content=paper_content, actions=actions).send()
    except httpx.HTTPStatusError as he:
        await cl.Message(
            content=f"âŒ /digest ã‚¨ãƒ©ãƒ¼ {he.response.status_code}: {he.response.text}"
        ).send()
    except Exception as e:
        await cl.Message(
            content=f"âŒ ãƒ‡ã‚¤ãƒªãƒ¼ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: `{e}`"
        ).send()


@cl.action_callback("show_digest_details")
async def on_show_digest_details(action: cl.Action):
    """è«–æ–‡ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º"""
    try:
        paper_id = action.payload.get("paper_id")
        paper_title = action.payload.get("paper_title", "è«–æ–‡")

        if not paper_id:
            await cl.Message(content="âŒ è«–æ–‡IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“").send()
            return

        # ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¡¨ç¤º
        loading_msg = await cl.Message(
            content=f"ğŸ“– {paper_title} ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ä¸­..."
        ).send()

        try:
            details = await call_digest_details(paper_id)

            # è«–æ–‡ã®ç« ç«‹ã¦ã«åŸºã¥ãæ§‹é€ åŒ–ã•ã‚ŒãŸè©³ç´°æƒ…å ±ã‚’æ§‹ç¯‰
            content_parts = [f"# ğŸ“„ {details['title']}"]

            # è«–æ–‡ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            content_parts.append("## ğŸ“‹ è«–æ–‡æƒ…å ±")

            if details.get("authors"):
                authors_str = ", ".join(details["authors"][:3])  # æœ€åˆã®3åã®ã¿è¡¨ç¤º
                if len(details["authors"]) > 3:
                    authors_str += f" ä»–{len(details['authors']) - 3}å"
                content_parts.append(f"**ğŸ‘¥ è‘—è€…:** {authors_str}")

            if details.get("categories"):
                content_parts.append(
                    f"**ğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒª:** {', '.join(details['categories'])}"
                )

            # ãƒªãƒ³ã‚¯æƒ…å ±
            links = []
            if details.get("url"):
                links.append(f"[arXiv]({details['url']})")
            if details.get("pdf"):
                links.append(f"[PDF]({details['pdf']})")
            if links:
                content_parts.append(f"**ğŸ”— ãƒªãƒ³ã‚¯:** {' | '.join(links)}")

            # è«–æ–‡ã®æ¦‚è¦
            if details.get("full_summary"):
                content_parts.append("\n## ğŸ“ è«–æ–‡æ¦‚è¦")
                content_parts.append(f"{details['full_summary']}")

            # è«–æ–‡æ§‹é€ ã®è¡¨ç¤º
            if details.get("paper_structure"):
                structure = details["paper_structure"]
                content_parts.append("\n## ğŸ“– è«–æ–‡æ§‹é€ ")

                # ç« ç«‹ã¦ã®è¡¨ç¤º
                if structure.get("sections"):
                    content_parts.append("**ğŸ“‹ ç« ç«‹ã¦:**")
                    for i, section in enumerate(structure["sections"], 1):
                        content_parts.append(f"{i}. {section}")

                # åˆ†æå“è³ªã®è¡¨ç¤º
                quality = structure.get("content_quality", "unknown")
                completeness = structure.get("analysis_completeness", "unknown")

                if quality == "high" and completeness == "complete":
                    content_parts.append(
                        "\nâœ… **åˆ†æå“è³ª:** é«˜å“è³ªãªè©³ç´°åˆ†æãŒå®Œäº†ã—ã¦ã„ã¾ã™"
                    )
                elif quality == "low" or completeness == "incomplete":
                    content_parts.append(
                        "\nâš ï¸ **åˆ†æå“è³ª:** åˆ†æãŒä¸å®Œå…¨ã§ã™ã€‚ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã‚’å–å¾—ä¸­..."
                    )

                # RAGçµæœã«åŸºã¥ãè©³ç´°åˆ†æ
                if (
                    details.get("cornell_note")
                    or details.get("quiz_items")
                    or details.get("citations")
                ):
                    content_parts.append("\n## ğŸ”¬ ç ”ç©¶è©³ç´°")

                    # Cornell Noteï¼ˆç ”ç©¶ãƒãƒ¼ãƒˆï¼‰ã‹ã‚‰å®Ÿéš›ã®å†…å®¹ã‚’æŠ½å‡º
                    if details.get("cornell_note"):
                        cornell = details["cornell_note"]
                        content_parts.append("\n### ğŸ“š ç ”ç©¶ãƒãƒ¼ãƒˆ")
                        content_parts.append(
                            f"**ğŸ’¡ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ:** {cornell.get('cue', '')}"
                        )
                        content_parts.append(
                            f"**ğŸ“ è©³ç´°åˆ†æ:**\n{cornell.get('notes', '')}"
                        )
                        content_parts.append(
                            f"**ğŸ“‹ ç ”ç©¶è¦ç´„:** {cornell.get('summary', '')}"
                        )

                    # é–¢é€£ç ”ç©¶
                    if details.get("citations"):
                        content_parts.append("\n### ğŸ“š é–¢é€£ç ”ç©¶")
                        for i, cite in enumerate(details["citations"], 1):
                            content_parts.append(
                                f"{i}. [{cite.get('title', '')}]({cite.get('url', '')})"
                            )

                    # ç†è§£åº¦ãƒã‚§ãƒƒã‚¯
                    if details.get("quiz_items"):
                        content_parts.append("\n### ğŸ§  ç†è§£åº¦ãƒã‚§ãƒƒã‚¯")
                        for i, quiz in enumerate(details["quiz_items"], 1):
                            content_parts.append(
                                f"\n**å•é¡Œ {i}:** {quiz.get('question', '')}"
                            )
                            for option in quiz.get("options", []):
                                marker = (
                                    "âœ… "
                                    if option.get("id") == quiz.get("correct_answer")
                                    else "âšª "
                                )
                                content_parts.append(
                                    f"  {marker}{option.get('id', '').upper()}: {option.get('text', '')}"
                                )
                            content_parts.append("")
            else:
                # è«–æ–‡æ§‹é€ ãŒå–å¾—ã§ããªã„å ´åˆ
                content_parts.append("\n## ğŸ“– è«–æ–‡å†…å®¹")
                content_parts.append(
                    "âš ï¸ è«–æ–‡æ§‹é€ ã®åˆ†æä¸­ã§ã™ã€‚RAGå‡¦ç†ã«ã‚ˆã‚Šè©³ç´°ãªç ”ç©¶å†…å®¹ã‚’å–å¾—ã—ã¦ã„ã¾ã™..."
                )

                # åŸºæœ¬çš„ãªRAGçµæœãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
                if (
                    details.get("cornell_note")
                    or details.get("quiz_items")
                    or details.get("citations")
                ):
                    content_parts.append("\n## ğŸ”¬ ç ”ç©¶è©³ç´°")

                    if details.get("cornell_note"):
                        cornell = details["cornell_note"]
                        content_parts.append("\n### ğŸ“š ç ”ç©¶ãƒãƒ¼ãƒˆ")
                        content_parts.append(
                            f"**ğŸ’¡ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ:** {cornell.get('cue', '')}"
                        )
                        content_parts.append(
                            f"**ğŸ“ è©³ç´°åˆ†æ:**\n{cornell.get('notes', '')}"
                        )
                        content_parts.append(
                            f"**ğŸ“‹ ç ”ç©¶è¦ç´„:** {cornell.get('summary', '')}"
                        )

                    if details.get("citations"):
                        content_parts.append("\n### ğŸ“š é–¢é€£ç ”ç©¶")
                        for i, cite in enumerate(details["citations"], 1):
                            content_parts.append(
                                f"{i}. [{cite.get('title', '')}]({cite.get('url', '')})"
                            )

                    if details.get("quiz_items"):
                        content_parts.append("\n### ğŸ§  ç†è§£åº¦ãƒã‚§ãƒƒã‚¯")
                        for i, quiz in enumerate(details["quiz_items"], 1):
                            content_parts.append(
                                f"\n**å•é¡Œ {i}:** {quiz.get('question', '')}"
                            )
                            for option in quiz.get("options", []):
                                marker = (
                                    "âœ… "
                                    if option.get("id") == quiz.get("correct_answer")
                                    else "âšª "
                                )
                                content_parts.append(
                                    f"  {marker}{option.get('id', '').upper()}: {option.get('text', '')}"
                                )
                            content_parts.append("")

            # ç ”ç©¶ã®æ„ç¾©
            content_parts.append("\n## ğŸ¯ ç ”ç©¶ã®æ„ç¾©")
            if details.get("categories"):
                categories = details["categories"]
                if "cs.LG" in categories:
                    content_parts.append("- **æ©Ÿæ¢°å­¦ç¿’åˆ†é‡**ã§ã®æ–°ãŸãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆ")
                if "cs.AI" in categories:
                    content_parts.append("- **äººå·¥çŸ¥èƒ½**ã®ç™ºå±•ã«å¯„ä¸ã™ã‚‹é‡è¦ãªçŸ¥è¦‹")
                if "cs.RO" in categories:
                    content_parts.append("- **ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹**åˆ†é‡ã§ã®å®Ÿç”¨çš„ãªå¿œç”¨å¯èƒ½æ€§")
                if "cs.CV" in categories:
                    content_parts.append(
                        "- **ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³**ã«ãŠã‘ã‚‹é©æ–°çš„ãªæ‰‹æ³•"
                    )
                if "cs.CL" in categories:
                    content_parts.append("- **è‡ªç„¶è¨€èªå‡¦ç†**ã«ãŠã‘ã‚‹æ–°ãŸãªæ‰‹æ³•")
                if "cs.RO" in categories:
                    content_parts.append("- **ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹**åˆ†é‡ã§ã®å®Ÿç”¨çš„ãªå¿œç”¨å¯èƒ½æ€§")

            if details.get("sections"):
                content_parts.append("\n## ğŸ“‘ æ¤œå‡ºã•ã‚ŒãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³")
                toc = details.get("toc_flat", [])
                if toc:
                    for heading in toc[:10]:
                        content_parts.append(f"- {heading}")
                    if len(toc) > 10:
                        content_parts.append(f"... (ä»– {len(toc) - 10} ã‚»ã‚¯ã‚·ãƒ§ãƒ³)")

                if details.get("has_full_text"):
                    content_parts.append(
                        f"\n**ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚µã‚¤ã‚º**: {details.get('content_length', 0):,} bytes"
                    )

            # ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ›´æ–°
            loading_msg.content = "\n".join(content_parts)

            actions = []
            if details.get("has_full_text"):
                actions.append(
                    cl.Action(
                        name="show_fulltext",
                        payload={"paper_id": paper_id},
                        label="ğŸ“„ å…¨æ–‡ã‚’è¡¨ç¤º",
                    )
                )

            if actions:
                loading_msg.actions = actions

            await loading_msg.update()

        except httpx.HTTPStatusError as he:
            error_content = (
                f"âŒ è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ {he.response.status_code}: {he.response.text}"
            )
            print(f"âŒ HTTP Error: {error_content}")
            loading_msg.content = error_content
            await loading_msg.update()
        except Exception as e:
            error_content = f"âŒ è©³ç´°å–å¾—å¤±æ•—: {str(e)}"
            print(f"âŒ General Error: {error_content}")
            loading_msg.content = error_content
            await loading_msg.update()

    except Exception as e:
        await cl.Message(content=f"âŒ è©³ç´°è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: `{e}`").send()


@cl.action_callback("show_fulltext")
async def on_show_fulltext(action: cl.Action):
    """å…¨æ–‡è¡¨ç¤ºã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    try:
        paper_id = action.payload.get("paper_id")

        if not paper_id:
            await cl.Message(content="âŒ è«–æ–‡IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“").send()
            return

        loading_msg = await cl.Message(content="ğŸ“„ å…¨æ–‡ã‚’å–å¾—ä¸­...").send()

        try:
            fulltext = await call_fulltext(paper_id, fmt="plain", max_bytes=100000)

            if len(fulltext) >= 100000:
                warning = "\n\nâš ï¸ **æ³¨æ„**: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯100,000ãƒã‚¤ãƒˆã«åˆ¶é™ã•ã‚Œã¦ã„ã¾ã™ã€‚å®Œå…¨ãªå†…å®¹ã‚’è¦‹ã‚‹ã«ã¯PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\n\n"
            else:
                warning = ""

            content = f"""### ğŸ“„ å…¨æ–‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„

{warning}```
{fulltext}
```
"""
            await loading_msg.update(content=content)

        except Exception as e:
            error_msg = f"âŒ å…¨æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"
            await loading_msg.update(content=error_msg)

    except Exception as e:
        await cl.Message(content=f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}").send()


@cl.on_message
async def on_message(msg: cl.Message):
    text = (msg.content or "").strip()
    if text.lower().startswith("digest"):
        try:
            parts = text.split()
            cat = "cs.LG"
            days = 2
            limit = 10
            for token in parts[1:]:
                t = token.strip()
                if t.lower().startswith("cs."):
                    cat = t
                elif t.lower().startswith("days="):
                    days = int(t.split("=", 1)[1])
                elif t.lower().startswith("limit="):
                    limit = int(t.split("=", 1)[1])

            items = await call_digest(cat=cat, days=days, limit=limit)
            if not items:
                await cl.Message(
                    content="è©²å½“ã™ã‚‹ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆé …ç›®ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                ).send()
                return

            # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
            await cl.Message(content="# ğŸ“° ãƒ‡ã‚¤ãƒªãƒ¼ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ").send()

            # å„è«–æ–‡ã‚’å€‹åˆ¥ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦è¡¨ç¤ºï¼ˆãƒœã‚¿ãƒ³ã‚’è¿‘ãã«é…ç½®ï¼‰
            for i, it in enumerate(items, 1):
                title = (it.get("title") or "").strip() or "ï¼ˆç„¡é¡Œï¼‰"
                url = it.get("url") or it.get("link") or ""
                pdf = it.get("pdf") or ""
                summary = it.get("summary_short") or it.get("summary") or ""
                paper_id = it.get("id", "")

                # è«–æ–‡æƒ…å ±ã‚’æ§‹ç¯‰
                paper_content = f"## {i}. {title}\n"

                # ãƒªãƒ³ã‚¯æƒ…å ±
                links = []
                if url:
                    links.append(f"[arXiv]({url})")
                if pdf:
                    links.append(f"[PDF]({pdf})")
                if links:
                    paper_content += f"ğŸ”— {' | '.join(links)}\n"

                # è¦ç´„
                if summary:
                    s = summary.strip()
                    if len(s) > 200:
                        s = s[:200] + "..."
                    paper_content += f"ğŸ“ {s}"

                # è©³ç´°è¡¨ç¤ºãƒœã‚¿ãƒ³ã‚’æº–å‚™
                actions = []
                if paper_id:
                    actions.append(
                        cl.Action(
                            name="show_digest_details",
                            payload={"paper_id": paper_id, "paper_title": title},
                            label="ğŸ“– è©³ç´°ã‚’è¦‹ã‚‹",
                        )
                    )

                # å„è«–æ–‡ã‚’å€‹åˆ¥ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦é€ä¿¡
                await cl.Message(content=paper_content, actions=actions).send()
        except httpx.HTTPStatusError as he:
            await cl.Message(
                content=f"âŒ /digest {he.response.status_code}: {he.response.text}"
            ).send()
        except Exception as e:
            await cl.Message(content=f"âŒ /digest å¤±æ•—: `{e}`").send()
        return

    # arxiv: ã‚¯ã‚¨ãƒª
    if text.lower().startswith("arxiv:"):
        query = text.split(":", 1)[1].strip()
        try:
            items = await call_arxiv_search(query, max_results=10)
            if not items:
                await cl.Message(content="è©²å½“çµæœãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚").send()
                return
            lines = []
            for it in items:
                title = it.get("title", "").strip() or "ï¼ˆç„¡é¡Œï¼‰"
                url = it.get("url") or ""
                summary = it.get("summary") or ""
                if url:
                    lines.append(f"- **[{title}]({url})**")
                else:
                    lines.append(f"- **{title}**")
                if summary:
                    # é•·ã™ãã‚‹ã¨é¬±é™¶ã—ã„ã®ã§è»½ãæŠ‘åˆ¶
                    s = summary.strip()
                    if len(s) > 280:
                        s = s[:280] + "..."
                    lines.append(f"  \n  {s}")
            await cl.Message(content="### arXiv æ¤œç´¢çµæœ\n" + "\n".join(lines)).send()
        except httpx.HTTPStatusError as he:
            await cl.Message(
                content=f"âŒ /arxiv/search {he.response.status_code}: {he.response.text}"
            ).send()
        except Exception as e:
            await cl.Message(content=f"âŒ /arxiv/search å¤±æ•—: `{e}`").send()
        return

    # ãã‚Œä»¥å¤–ã¯ /rag/stream ã‚’SSEã§å—ä¿¡ã—ãªãŒã‚‰è¡¨ç¤º
    # æœ€åˆã« readiness ã‚’è»½ãç¢ºèª
    try:
        h = await get_health()
        if not h.get("rag_ready"):
            await cl.Message(
                content="â³ RAG index æº–å‚™ä¸­ã§ã™ã€‚ã—ã°ã‚‰ãã—ã¦ã‹ã‚‰ãŠè©¦ã—ãã ã•ã„ã€‚"
            ).send()
            return
    except Exception:
        # healthå¤±æ•—æ™‚ã‚‚ä¸€å¿œæœ¬ç•ªã‚’å©ã„ã¦ã¿ã‚‹
        pass

    # å‡¦ç†é–‹å§‹ã®ã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤º
    async with cl.Step(name="RAGå‡¦ç†é–‹å§‹", type="run") as step:
        step.output = "ğŸ” è³ªå•ã‚’å‡¦ç†ä¸­ã§ã™..."

    out = cl.Message(content="")
    await out.send()

    try:
        async for chunk in sse_rag_stream(text):
            if chunk.get("type") == "status":
                # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦è¡¨ç¤º
                async with cl.Step(name="å‡¦ç†çŠ¶æ³", type="tool") as status_step:
                    status_step.output = chunk.get("text", "")
            elif chunk.get("type") == "content":
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
                text = chunk.get("text", "")
                if text:
                    await out.stream_token(text)
            elif chunk.get("type") == "error":
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                await out.update(content=f"âŒ ã‚¨ãƒ©ãƒ¼: {chunk.get('text', '')}")
                return

        await out.update()

        # å‡¦ç†å®Œäº†ã®ã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤º
        async with cl.Step(name="RAGå‡¦ç†å®Œäº†", type="run") as step:
            step.output = "âœ… å›ç­”ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ"

    except asyncio.CancelledError:
        # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚ŒãŸå ´åˆã¯é©åˆ‡ã«å‡¦ç†
        await out.update(content="â¹ï¸ å‡¦ç†ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
    except httpx.HTTPStatusError as he:
        if he.response.status_code == 503:
            await out.update(content="âš ï¸ RAG index ãŒæœªæº–å‚™ã§ã™ã€‚ï¼ˆ503ï¼‰")
        else:
            await out.update(
                content=f"âŒ /rag/stream {he.response.status_code}: {he.response.text}"
            )
    except Exception as e:
        await out.update(content=f"âŒ /rag/stream å¤±æ•—: `{e}`")
